自动化 学报 ACTAAUTOMATICASINICA 年 第卷 第期 VolNo 基于 Q 学习 算法 和 BP 神经网络 的 倒立 摆 控制 蒋国飞 　 　 吴沧浦 摘 　 要 　 Q 学习 是 Watkins ［ ］ 提出 的 求解 信息 不 完全 马尔可夫 决策问题 的 一种 强化 学习 方法 将 Q 学习 算法 和 BP 神经网络 有效 结合 ， 实现 了 状态 未 离散 化 的 倒立 摆 的 无 模型 学习 控制 仿真 表明 ： 该 方法 不仅 能 成功 解决 确定 和 随机 倒立 摆 模型 的 平衡 控制 ， 而且 和 Anderson ［ ］ 的 AHCAdaptiveHeuristicCritic 等 方法 相比 ， 具有 更好 的 学习效果 关键词 　 Q 学习 ， BP 网络 ， 学习 控制 ， 倒立 摆 系统 ， 高斯 噪声 LEARNINGTOCONTROLANINVERTEDPENDULUMUSINGQLEARNINGANDNEURALNETWORKSJIANGGUOFEI 　 WUCANGPUDepartmentofAutomaticControlBeijingInstituteofTechnologyBeijingAbstract 　 QlearningisareinforcementlearningmethodtosolveMarkoviandecisionproblemswithincompleteinformationThispaperpresentsanovelmethodtocontrolaninvertedpendulumwithunquantizedstatesbyusingQlearningandneuralnetworksSimulationresultsareincludedtoshowthatthenewmethodcannotonlybalancethedeterminedorstochasticinvertedpendulumssuccessfullybutalsoleadtoabettereffectoflearningwhencomparedwithAndersonsAHCmethodKeywords 　 QLearningBPneuralnetworklearningcontrolinvertedpendulumGaussiannoise 　 引言 　 　 在 各种 非线性 系统 中 ， 倒立 摆是 一个 十分 典型 的 例子 用 强化 学习 方法 来 实现 倒立 摆 的 平衡 控制 ， 迄今 已经 取得 了 不少 成果 年 Barto 等 人 ［ ］ 设计 了 两个 单层 神经网络 ， 采用 AHCAdaptiveHeuristicCritic 学习 算法 实现 了 状态 离散 化 的 倒立 摆 控制 年 ， Anderson ［ ］ 进一步 用 两个 双层 神经网络 和 AHC 方法 实现 了 状态 未 离散 化 的 倒立 摆 的 平衡 控制 最近 ， Peng ［ ］ 通过 将 状态 离散 化 成为 个 区域 ， 用 Lookup 表 表示 Q 值 的 方法 实现 了 基于 Q 学习 算法 的 倒立 摆 的 平衡 控制 然而 在 那些 有 连续 状态 的 问题 中 ， 如果 采用 离散 化 这些 连续 量 再 用 Lookup 表来 表示 的 方法 ， 则 Q 学习 算法 和 常规 动态 规划 方法 一样 ， 存在 状态变量 的 空间 复杂性 问题 ， 即 所谓 的 维数灾 问题 解决 方法 之一 是 用 参数 化 的 结构 来 表示 Q 值 ， 如 低阶 多项式 、 决策树 等 本文 通过 训练 BP 网络 来 逼近 Q 值 函数 并 利用 BP 网络 的 泛化 能力 ， 实现 了 基于 Q 学习 算法 的 状态 未 离散 化 的 确定 和 随机 倒立 摆 的 无 模型 学习 控制 本文 的 目的 在于 用 倒立 摆 控制 问题 来 证实 ： 用 Q 学习 和 神经网络 结合 的 方法 去 实现 某些 状态 连续 控制系统 的 无 模型 控制 的 可行性 　 Q 学习 　 　 在 介绍 Q 学习 方法 前 ， 先 简述 有限 马氏 决策问题 的 模型 ； 在 每个 时间 步 k … 控制器 观察 马氏 过程 的 状态 为 xk ， 选择 决策 ak ， 收到 即时 报酬 rk ， 并 使 系统 转移 到 下 一个 状态 yk ， 转移 概率 为 Pxkykak 控制 的 目的 是 寻求 一个 最优控制 策略 ， 使 未来 每个 时间 步所获 报酬 的 折扣 和 的 期望 最大 ， 即 极大 化 ， 其中 γ 为 折扣 因子 　 　 给定 一个 策略 π ， 定义 Q 值为 　 其中 　 　 换言之 ， Q 值 即 在 状态 x 执行 控制 a 及 后续 策略 π 的 报酬 折扣 和 的 期望 Q 学习 的 目的 就是 在 转移 概率 和 所 获 报酬 未知 的 情况 下来 估计 最优 策略 的 Q 值为 方便 起 见 ， 定义 Qxa ≡ Q π xaxa 其中 π 表示 最优 策略 　 　 在线 Q 学习 方法 实现 如下 ： 在 每个 时间 步 k ， 观察 当前 状态 xk ， 选择 和 执行 控制 ak ， 再 观察 后继 状态 yk 及 接受 即时 报酬 rk ， 然后 根据 下 式 调整 Qk 值 ： 　 　 其中 β k 为 学习 因子 ， 　 　 Watkins ［ ］ 证明 了 学习 因子 序列 ｛ β k ｝ 在 满足 一定 的 条件 下 ， 如果 任 一个 xa 二元 组能 用 等式 进行 无穷 多次 迭代 ， 则 当 k → ∞ 时 ， Qkxa 以 概率 收敛 于 Qxa 　 倒立 摆 系统 　 　 倒立 摆 系统 是 指图 所示 的 非线性 系统 小车 可以 自由 地 在 限定 的 轨道 上 左右 移动 小车 上 的 倒立 摆 一端 被 铰链 在 小车 顶部 另一端 可以 在 小车 轨道 所在 的 垂直 平面 上 自由 转动 控制 的 目的 在于 通过 推动 小车 向 左 或 向 右 移动 ， 使 倒立 摆 平衡 并 保持 小车 不 和 轨道 两端 相撞 图 　 倒立 摆 系统 　 　 一般 情况 下 ， 倒立 摆 系统 有 四个 状态变量 　 　 x ： 小车 在 轨道 上 的 位置 ； 　 　 　 　 　 θ ： 倒立 摆 偏离 垂直 方向 的 角度 ； 　 　 ： 小车 的 运动 速度 ； 　 　 　 　 　 　 　 ： 倒立 摆 的 角速度 倒立 摆 系统 可以 用 以下 运动 方程 来 描述 　 　 　 　 其中 gms ， 重力 加速度 ； mckg ， 小车 质量 ； mkg ， 倒立 摆 质量 ； lm ， 倒立 摆 的 一半 长度 ； μ c ， 小车 和 轨道 的 摩擦系数 ； μ p ， 倒立 摆 和 小车 的 摩擦系数 ； Ft ± N ， 在 时刻 t 作用 于 小车 质心 的 力 小车 轨道 长度 为 米 通过 Euler 方法 数值 近似 ， 可用 以下 差分 方程 来 仿真 倒立 摆 系统 xtxt τ t 　 　 　 tt τ t 　 　 θ t θ t τ t 　 　 tt τ t 　 　 时间 步 τ 一般 设为 秒 显然 以上 给出 的 倒立 摆 系统 是 一个 确定性 系统 本文 为了 说明 基于 Q 学习 和 神经网络 的 方法 同样 适用 于 连续 随机 系统 的 天 模型 控制 ， 在 以上 确定性 倒立 摆 模型 中 引入 一个 噪声 信号 来 构成 一个 随机 倒立 摆 模型 ， 即 在 仿真 中 用 以下 方程 来 代替 方程 tt τ t ξ μ σ 　 　 其中 ξ μ σ 为 高斯 噪声 　 基于 Q 学习 和 BP 网络 的 倒立 摆 控制 　 　 和 其他 实现 倒立 摆 控制 的 方法 不同 ， 在 强化 学习 方法 中 ， 控制器 唯一 能 从 环境 得到 的 反馈 是 当 倒立 摆 偏离 垂直 方向 的 角度 超出 ± ° 或 小车 在 ± 米处 和 轨道 两端 相撞 时 环境 给出 的 一个 失败 信号 因此 本文 定义 即时 报酬 rt 为 由于 控制器 是 在 执行 了 一系列 决策 后 才 得到 这个 延迟 的 失败 信号 ， 则 控制器 必须 解决 奖励 或 惩罚 随 时间 分配 的 问题 ， 即 确定 在 这 过程 中 哪些 决策 应该 对 最后 的 失败 负责 实际上 Q 学习 算法 是 在 各 时间 步 Q 值 的 更新 迭代 中将 这 失败 信号 进行 反传 并 根据 Q 值来 确定 相应 决策 的 优劣 本文 由于 设 倒立 摆 平衡 失败 时 的 即时 报酬 为 负 rt ， 因此 对应 Q 值较 小 的 决策 就 更 有 可能 导致 倒立 摆 系统 的 平衡 失败 同时 在 实现 状态 未 离散 化 的 倒立 摆 控制 时 ， 控制器 还 涉及 状态 空间 很大 时 Q 值 函数 的 泛化 问题 也 叫 奖励 或 惩罚 随 结构 分配 问题 　 　 本文 提出 的 基于 Q 学习 和 BP 网络 的 状态 未 离散 化 倒立 摆 控制系统 的 结构 如图所示 为 方便 起 见 ， 在 图 中 定义 状态 Xx θ TBP 网络 的 输入 为 状态 X 和 决策 a ， 输出 为 QXaW ， 其中 W 为 网络 权重 整个 控制系统 工作 如下 ： 在 每个 时间 步 k ， 观测 倒立 摆 的 当前 状态 为 Xk ， 根据 BP 网 的 实际 输出 QXkaWk 值 并 按照 某种 探索 策略 来 选择 当前 决策 ak 然后 观测 倒立 摆 的 后继 状态 Yk 并 检测 是否 有 失败 信号 确定 即时 报酬 rk 系统 再 根据 式 更新 二元 组 Xkak 的 Q 值 ， 然后 利用 误差 信号 eQXkakQXkakWk 更新 BP 网 的 权重 Wk 为 Wk ， 使 BP 网 实际 输出 逼近 更新 后 的 理想 输出 QXkak ， 然后 再 转到 状态 Yk 继续 以上 的 过程 由于 未 对 状态 空间 离散 化 ， 在 系统 中 利用 了 BP 网 的 泛化 能力 来 求解 未曾 训练 过 的 状态 决策 二元 组 的 Q 值 另外 ， 在 BP 网络 的 权重 学习 中 ， 对 任一 状态 和 决策 所 对应 的 Q 值 进行 逼近 都 可能 会 影响 该 状态 和 另 一 控制 所 对应 的 Q 值 ， 所以 图中 所示 的 BP 网 实际上 可以 用 两个 BP 网来 代替 每个 控制 一个 ， 这样 可以 期望 得到 更好 的 学习效果 　 　 图 　 基于 Q 学习 和 BP 网络 的 状态 未 离散 化 倒立 摆 控制系统 的 结构图 　 仿真 及 结果 　 　 如上节 所述 ， 在 实际 仿真 中 ， 采用 了 两个 BP 网络 每个 网络 分 三层 ， 输入 层 和 隐层 各有 五个 结点 ， 输出 层有 一个 结点 对 BP 网络 的 实际 输入 进行 了 标准化 ， 使 其 分布 在 ［ ， ］ 之间 Q 学习 算法 的 学习 因子 β ， 折扣 因子 γ 在 随机 倒立 摆 模型 的 仿真 中 ， 高斯 噪声 ξ μ σ 的 均值 μ ， 标准差 σ 一般 t 的 值 在 ， 之间 在 倒立 摆 控制系统 中 可行 控制 只有 两个 左 推 或 右 推 ， 因此 本文 直接 选择 对应 Q 值 较大 的 控制 为 当前 控制 将 随机 发生器 的 种子 和 BP 网 初始 权重 设 为 不同 值 ， 对 确定性 和 随机 倒立 摆 模型 各 做 十次 试验 ， 每次 试验 当 倒立 摆 的 试探 次数 失败 次数 超过 次 或 一次 试探 的 平衡 步数 超过 步时 ， 中止 倒立 摆 的 学习 并 重新 开始 另 一次 试验 在 仿真 中 ， 如果 倒立 摆在 一次 试探 中 能 保持 步 不到 ， 就 认为 本次 试验 已经 能 成功 控制 倒立 摆 平衡 了 在 状态 离散 化 的 倒立 摆 控制 中 ， 每次 平衡 失败 后 ， 倒立 摆 的 初始状态 一般 设在 X 的 位置 在 仿真 中 ， 为了 保证 倒立 摆 得到 在 各种 场合 的 控制 经验 ， 在 每次 平衡 失败 后 ， 将 初始状态 复位 为 一定 范围 内 的 随机 值 平均 十次 试验 的 结果 ， 得到 基于 Q 学习 和 BP 网络 的 状态 未 离散 化 倒立 摆 控制 的 结果 如图所示 图 　 各 方法 实现 状态 未 离散 化 倒立 摆 控制 的 学习曲线 　 　 在 同样 条件 下 ， 将 基于 Q 学习 方法 、 AHC 方法 和 随机 控制 无 学习 方法 实行 状态 未 离散 化 的 确定性 倒立 摆 模型 控制 的 结果 进行 比较 ， 发现 基于 Q 学习 和 BP 网络 的 方法 学习效果 最好 ， 每次 试验 在 平均 次 失败 后 就 可以 成功 控制 倒立 摆 平衡 而用 Anderson ［ ］ 的 两层 网络 和 AHC 方法 则 大约 要次 随机 控制 方法 不能 控制 倒立 摆 平衡 ， 每次 试探 最 多 只能 运行 几百步 基于 Q 学习 和 BP 网络 的 方法 同样 可以 实现 随机 倒立 摆 模型 的 平衡 控制 ， 但 由于 随机噪声 的 引入 增加 了 学习 难度 ， 每次 试验 平均 要 在 次 失败 后 才能 控制 倒立 摆 平衡 　 　 需要 说明 的 是 ， 尽管 基于 Q 学习 和 BP 网络 的 方法 取得 较 好 控制 效果 ， 但 作者 更 关注 的 是 验证 了 这种 方法 在 实现 某些 状态 连续 控制系统 的 无 模型 控制 的 可行性 实际上 ， 倒立 摆 控制 问题 只是 以上 问题 的 一个 例子 国家自然科学基金 重点 资助 项目 作者 单位 ： 北京理工大学 自动控制 系 　 北京 　 参考文献 　 　 WatkinsCJCHLearningfromdelayedrewards ［ PhDDissertation ］ UKKingsCollege 　 　 AndersonCWLearningtocontrolaninvertedpendulumusingnerualnetworksIEEEControlSystemMagazine — 　 　 BartoAGSuttonRSAndersonCWNeuronlikeadaptiveelementsthatcansolvedifficultlearningcontrolproblemsIEEETransonSMC — 　 　 PengJEfficientdynamicprogrammingbasedlearningforcontrol ［ PhDthesis ］ USANortheasternUniversity 收稿 日期 　