计算机 研究 与 发展 JOURNALOFCOMPUTERRESEARCHANDDEVELOPMENT 年 第卷 第期 VolNo 可 扩展 单一 映象 文件系统 的 设计 、 实现 及 评价 王建勇 　 祝明发 　 徐志伟 　 朱 宁宁 　 张弛 摘 　 要 　 曙光 超级服务器 是 典型 的 机群系统 ， COSMOS 是 为 其 研制开发 的 可 扩展 的 单一 映象 文件系统 文中 主要 描述 了 COSMOS 原型 系统 的 设计 、 实现 及 评价 其中 重点 介绍 了 双 粒度 合作 式 缓存 、 分布式 元 数据管理 及 网络 磁盘 存储 分组 等 关键技术 ， 并 利用 IO 基准 程序 对 原型 文件系统 进行 了 性能 评价 ， 测试 结果表明 了 该 原型 系统 在 保证系统 单一 映象 的 基础 上 ， 具备 良好 的 可扩展性 关键词 　 单一 映象 文件系统 可扩展性 双 粒度 合作 式 缓存 网络 磁盘 分组 分布式 元 数据管理 中图法 分类号 　 TPDESIGNIMPLEMENTATIONANDEVALUATIONOFASCALABLESINGLEIMAGEFILESYSTEMWANGJianYongZHUMingFaXUZhiWeiZHUNingNingandZHANGChiDepartmentofComputerScienceandTechnologyPekingUniversityBeijingNationalResearchCenterforIntelligentComputingSystemsBeijingInstituteofComputingTechnologyChineseAcademyofSciencesBeijingAbstract 　 TheDawningsuperserverisatypicalclustersystemforwhichaCOSMOSfilesystemhasbeendevelopedMainlypresentedinthispaperarethedesignimplementationandevaluationoftheprototypeCOSMOSfilesystemHerehighlitaresomekeytechnologiesincludingdualgranularitycooperativecachingdistributedmetadatamanagementandnetworkdiskstrippingTheperformanceoftheprototypefilesystemisalsoevaluatedusingthestandardIObenchmarksTheinitialperformancemeasurementsshowthatourapproachisapromisingoneinprovidingscalablesingleimagefilesystemKeywords 　 singleimagefilesystemscalabilitydualgranularitycooperativecachingnetworkdiskstrippingdistributedmetadatamanagement 　 引 　 　 言 　 　 虽然 现有 的 分布式文件系统 如 NFS ［ ］ 等 已 被 人们 广泛 接受 ， 并 作为 局域网 环境 下 方便 有效 的 共享 数据 的 手段 ， 但 远远 不能 适应 目前 计算环境 发展 的 要求 尤其 最近 几年 工作站 机群系统 的 迅猛发展 ， 使 计算能力 有 了 较大 提高 ， 应用 的 IO 需求 日益 变 大 ， 用户 对系统 的 单一 映象 、 可扩展性 和 可用性 等 功能 提出 了 更 高 的 要求 ， 同时 机群系统 也 提供 了 更 多 可用 的 资源 （ 包括 高性能 的 网络 、 分布 于 各个 节点 上 的 更 多 可用 的 磁盘 及 内存 、 速度 更快 的 处理器 等 ） ， 为 实现 高性能 的 文件系统 提供 了 可能 曙光 超级服务器 是 典型 的 机群系统 ， 它 的 主要 设计 目标 除了 要 提供 高性能 （ 即 高带宽 、 低 时延 ） 的 机 间通信 机制 外 ， 很 重要 的 一点 是 要 提供 单一 映象 功能 ， 诸如 单一 入口 点 、 单一 控制点 、 单一 文件系统 等等 我们 为 曙光 超级服务器 研制开发 了 文件系统 — — COSMOS ， 并 基于 其 前期 原型 系统 以下 简称 为 SFS 即 ascalablesingleimagefilesystem ， 对 单一 映象 文件系统 的 可扩展性 技术 进行 了 研究 ， 文中 主要 描述 了 SFS 原型 系统 的 设计 、 实现 及 评价 　 系统 概况 　 　 虽然 SFS 的 设计 环境 是 曙光 超级服务器 ， 但 其 所 采用 的 技术 却 具有 普遍性 ， 适用 于 通常 意义 上 的 机群文件 系统 的 设计 严格 的 单一 系统 映象 、 良好 的 IO 性能 和 可扩展性 及 保证 应用程序 的 完全 二进制 兼容性 是 SFS 的 主要 设计 目标 为此 ， SFS 选择 了 共享 文件系统 来 作为 实现 单一 系统 映象 的 方法 ， 采用 合作 式 缓存 及 并行 分布式 的 存储 和 控制技术 来 提高 文件系统 的 IO 性能 及 可扩展性 ， 并 利用 虚拟 文件系统 机制 来 保证 与 UNIX 文件系统 的 完全 二进制 兼容 　 　 图 给出 了 SFS 的 系统结构 它 由 核心 相关 层 和 用户 层 两 部分 组成 SFS 的 核心层 是 在 虚拟 文件系统 一级 中 实现 的 ， 它 主要 是 接收 来自 逻辑 文件系统 的 IO 请求 ， 并 以 一定 的 格式 转发给 用户 层 SFS 的 用户 层 是 其 主体 部分 ， 由类 用户 级 Daemon 进程 构成 ， 被 分别 称为 客户 client 、 管理 服务器 manager 和 存储 服务器 storager ， 它们 协调 工作 ， 共同完成 核心层 转发 过来 的 IO 请求 其中 存储 服务器 基于 AIX 的 JFS 实现 了 具体 的 数据 存储 ， 客户 完成 数据 及元 数据 的 缓存 ， 而 管理 服务器 负责 缓存 一致性 的 维护 及元 数据 的 存储 管理应用程序 发出 与 SFS 相关 的 文件系统 调用 ， 经由 逻辑 文件系统 和 虚拟 文件系统 后 ， IO 请求 被 传给 SFS 的 本地 客户 ， 若 IO 请求 不能 由 本地 客户 的 缓存 得到 满足 ， 则 转发给 管理 服务器 ， 由 管理 服务器 通知 相应 的 客户 或 存储 服务器 来 完成 具体 的 文件 操作 ， 并 把 结果 转发给 本地 客户 ， 然后 经过 虚拟 文件系统 和 逻辑 文件系统 把 结果 返回 给 应用程序 图 　 SFS 系统结构 　 关键技术 　 　 我们 在 满足 单一 系统 映象 和 二进制 兼容性 等 边界条件 下 ， 为了 提高 SFS 文件系统 的 IO 性能 及 可扩展性 ， 设计 了 基于 目录 的 无效 使能 协议 ， 提出 了 双 粒度 缓存 一致性 协议 ［ ］ 为了 避免 单一 服务器 瓶颈 问题 ， 采用 了 数据 存储 与 元 数据管理 分开 的 策略 ， 并 实现 了 分布式 的 数据 存储 和 元 数据管理 虽然 美国加州大学伯克利分校 的 xFS 系统 已经 采用 了 这种 serverless 的 方案 ， 但 我们 根据 曙光 超级服务器 的 特点 选择 了 不同 的 实现 机制 下面 我们 对 SFS 的 某些 关键技术 特点 加以 介绍 　 双 粒度 合作 式 缓存 　 　 随着 越来越 多 的 分布式系统 利用 高性能 的 网络 将 各 计算 结点 连接起来 ， 远程 内存 正 日益 成为 一种 新型 的 内存 结构 层次 ， 因为 通过 高带宽 、 低 时延 的 网络 对 远程 内存 的 访问 要 比 本地 磁盘 快得多 这种 远程 缓存 结构 remotecachingarchitecture ［ ］ 强调 了 各 存储 层次 （ 即 本地 内存 、 远程 内存 和 磁盘 ） 之间 性能 上 的 差异 由于 它 允许 系统 中 所有 结点 互相 存取 各自 的 本地 内存 以 充分利用 远程 内存 ， 这种 对称 式 结构 显然 不同于 传统 的 客户服务器 模型 ， 因为 如果 系统 中任 一个 客户 的 本地 内存 中 包含 了 被 请求 的 数据项 ， 它 都 可以 服务 该 请求 ， 因而 这是 一种 结点 之间 地位 均等 的 peertopeer 结构 ， 有些 文献 ［ ］ 称之为 合作 式 缓存 cooperativecaching 　 　 在 使用 缓存 模型 的 文件系统 中 ， 一个 重要 的 设计 问题 是 决定 缓存 粒度 ， 它 通常 也 作为 文件 读写操作 的 数据传输 单位 缓存 粒度 要 根据 文件共享 语义 、 网络通信 带宽 及时 延 、 应用 背景 等 因素 来 选择 ， 例如 若 网络通信 带宽 较 高 而 时延 较大 时 ， 缓存 粒度 不 应 太小 ， 否则 通信 开销 会太大 ； 相反 ， 若 文件共享 语义 较 严格 、 且 应用 呈现出 较 多 写 共享 的 特点 时 ， 缓存 粒度 不应 太大 ， 否则 会 引起 频繁 的 “ 假 共享 ” 事件 的 出现 　 　 NFS 和 Sprite 等 分布式文件系统 在 打开 文件 时 对 已 缓存数据 进行 有效性 验证 ， 而 xFS 提供 了 严格 的 合作 式 缓存 一致性 ， 它们 在 块 一级 通过 令牌 机制 来 维护 缓存 一致性 NFS 和 Sprite 等 系统 使用 的 粗粒度 缓存 管理 容易 引起 ‘ 假 共享 ’ 问题 ， 在 频繁 的 并发 写 共享 concurrentwritesharing 环境 中 它们 的 性能 将会 急剧下降 为了 解决 该 问题 ， Sprite 采用 的 方法 是 ： 当 发生 共享 写时 关掉 缓存 （ cachedisabling ） ， 而 NFS 干脆 就 没有 提供 严格 的 一致性 保证 然而 相对 于 粗粒度 方案 ， 像 文件系统 xFS 在 细粒度 一级 维护 缓存 一致性 将会 引起 大量 的 服务器 负载 和 某些 不必要 的 开销 　 　 我们 为 SFS 引入 双 粒度 缓存 管理 的 最初 动机 是 为了 克服 粗粒度 和 细粒度 方案 中 各自 的 缺点 当 客户 打开 一个 文件 时若 没有 与 其它 客户 发生冲突 （ 即 没有 并发 的 写 共享 ） ， 服务器 就 授予 该 客户 一个 文件 回调 ， 当 其它 客户 以 冲突 方式 打开 该 文件 时 ， 服务器 通知 客户 并 回收 该 文件 的 回调 如果 客户 失掉 了 一个 文件 的 回调 ， 那么 它 将 使用 令牌 机制 并 在 块 一级 来 管理文件 缓存 块级 的 缓存 状态 转换 如图所示 ， 这样 由于 写 共享 而 引起 ‘ 假 共享 ’ 发生 的 可能性 将会 变小 缓存 cache 状态 转换 条件 ： ： 读 不 命中 请求 ； 、 、 、 ： 无效 请求 ， 转发 后 无效 请求 ， 缓存 替换 请求 ； 、 、 ： 写 不 命中 请求 ； 、 ： 令牌 降级 请求 ； 、 ： 写 命中 请求 ； 、 ： 文件 flush 请求 ， 系统 sync 请求 ； 、 ： 读 命中 请求 ， 转发 数据 请求 图 　 客户端 缓存 有限 状态图 　 　 我们 设计 的 双 粒度 协议 是 在 文件 和 数据 块 两个 粒度 上 维护 缓存 一致性 的 ， SFS 总是 试图 在 文件 一级 进行 缓存 管理 ， 当 发生 并发 的 写 共享 时才 使用 块 一级 的 、 基于 目录 的 无效 使能 协议 这里 我们 给出 ‘ 并发 写 共享 ’ 的 定义 ： 当 一个 文件 同时 在 多个 结点 上 被 打开 ， 并且 至少 在 其中 一个 结点 上 是 以 写 方式 打开 的 ， 那么 该 文件 就 称为 正 处于 ‘ 并发 写 共享 ’ 状态 由于 客户 很少 以 共享 方式 写 数据 ， 在 细粒度 （ 诸如 数据 块级 ） 一级 来 维护 缓存 一致性 会 产生 大量 的 工作 负载 ， 基于 这些 考虑 ， SFS 实现 了 双 粒度 缓存 一致性 协议 当 一个 客户 打开 一个 文件 且 不 处于 ‘ 并发 写 共享 ’ 状态 时 ， 管理器 就 授予 该 客户 一个 文件 回调 callback ， 并 承诺 当 其它 客户 以 冲突 方式 打开 该 文件 时 通知 该 客户 ， 让 该 客户 放弃 文件 回调 当 客户 收到 一条 ‘ 回收 文件 回调 ’ 请求 时 ， 它 可 判断 出 此时 某些 其它 客户 正以 ‘ 并发 共享 写 ’ 方式 打开 该 文件 ， 从此 它 将 在 块 级 粒度 上 使用 基于 令牌 的 方案 来 维护 缓存 一致性 　 　 SFS 对 只读 回调 和 读写 回调 （ 以下 简称 写 回调 ） 作 了 区别 当 一个 客户 创建 一个 新 文件 或者 为 修改 一个 文件 而 打开 一个 文件 （ 例如 ， 当 文件 存取 模式 是 OWRONLY 或 ORDWR ） ， 且 没有 ‘ 并发 写 共享 ’ 发生 时 ， 管理器 将 给予 该 客户 一个 写 回调 ， 否则 ， 如果 一个 客户 以 ORDONLY 的 存取 模式 打开 一个 文件 且 没有 ‘ 并发 写 共享 ’ 发生 ， 该 客户 就 得到 一个 读 回调 图 刻画 了 客户端 文件 的 个 回调 状态 以及 它们 的 转换 条件 若 一个 客户 打开 了 一个 文件 却 没有 得到 任何 回调 ， 则 意味着 并发 的 共享 写 正在 进行 之中 若 一个 客户 拥有 写 回调 ， 则 它 可 推断出 目前 它 是 唯一 打开 了 该 文件 的 客户 状态 转换 条件 这里 的 ‘ 写 ’ 意味着 文件 打开 的 存取 模式 是 OWRONLY 或 ORDWR ， 而 ‘ 读 ’ 则 意味着 文件 的 存取 模式 是 ORDONLY ： ： 文件创建 ， 写 打开 ； 、 ： 读 打开 ； 、 、 、 ： 写 关闭 ； 、 ： 读 关闭 ； 、 、 、 ： 写 打开 ； 、 ： 其它 客户 写 关闭 相同 的 文件 ； ： 其它 客户 写 或 读 打开 同一个 文件 ； ： 其它 客户 写 打开 同一个 文件 ； 、 ： 读 打开 ， 写 打开 ， 读 关闭 ， 写 关闭 ； 、 ： 读 打开 ， 读 关闭 图 　 SFS 客户端 的 文件 级 缓存 有限 状态 转换 图 　 　 在 实现 新 协议 的 过程 中 我们 发现 双 粒度 协议 相对 于单 粒度 协议 有 以下 几个 好处 ： ① 它 能够 减少 由于 维护 缓存 一致性 而 带来 的 服务器 负载 ； ② 在 有些 情况 下 ， 它 能够 降低 客户 与 服务器之间 的 通信量 ； ③ 更为重要 的 是 ， 它 能够 提供 某些 有 帮助 的 提示 （ hint ） 信息 ， SFS 可以 利用 这些 提示信息 进一步 缓解 服务器端 的 负载 以及 客户服务器 间 的 网络 开销 　 网络 磁盘 分组 存储 　 　 传统 的 分布式文件系统 采用 单一 的 文件 服务器 来 响应 来自 所有 客户 的 IO 请求 ， 包括 数据 及元 数据 的 存储 和 管理 、 缓存 一致性 的 维护 等 功能 这种 单一 服务器 的 体系结构 所 具有 的 一个 好处 是 其 设计 和 实现 的 简单 ， 然而 它 严重 制约 了 系统 的 可扩展性 和 IO 性能 ， 而且 单一 服务器 容易 形成 单一 出错 点 为了 克服 传统 文件系统 单一 服务器 的 缺点 ， 我们 在 设计 SFS 时 ， 采用 了 无 集中式 服务器 方案 ， 虽然 系统 中 存在 多个 服务器 ， 由于 SFS 实现 了 文件 存储 的 位置 透明性 ， 在 用户 看来 系统 中 只有 一个 虚拟 的 单一 文件 服务器 类似 于 xFS 的 做法 ， 我们 将 数据 的 存储 和 元 数据 的 管理 分开 来 实现 ， 由 专门 的 被 称为 存储器 的 模块 实现 分布式 的 磁盘 存储 同 xFS 不同 的 是 ， SFS 的 存储器 只 负责 存储 文件 （ 包括 目录 文件 ） 数据 ， 而元 数据 如 文件系统 的 超级 块 和 索引 节点 等 内容 由 管理器 负责 存储 另外 ， SFS 是 基于 AIX 操作系统 实现 的 ， 由于 AIX 本身 的 文件系统 JFS 已 对 其元 数据 进行 了 日志 式 管理 ， 并 提供 了 异步 IO 功能 ， 且 SFS 采用 RAID 存储 模型 ， 不 存在 大量 小块 写 的 问题 ， 因而 它 没有 实现 日志 式 存储 功能 ， 这 一点 也 与 xFS 不同 　 　 为了 提高 存储 子系统 的 性能 ， 就 必须 允许 一个 或 多个 客户 以高 的 磁盘 带宽 存取 单个 文件 或 多个 文件 ， 同时 能够 有效 地 处理 小 文件 的 写 请求 因而 系统 应当 将 数据分布 于 多个 结点 的 多个 磁盘 上以 提高 并行度 为了 实现 系统 的 可扩展性 目标 ， 系统 应当 能够 有效 地 支持 成百上千 个 磁盘 ， 因而 系统 应当 能够 灵活 地 控制 磁盘 分组 （ diskstripping ） 的 并行度 磁盘 分组 是 由 PChen 等 人为 实现 大规模 的 磁盘阵列 提出 来 的 ［ ］ ， 后来 被 xFS 文件系统 所 采用 ， 并 被 扩展 为 网络 存储器 分组 ［ ］ ， 我们 也 为 SFS 系统 实现 了 存储 分组 功能 ， 并 称之为 网络 磁盘 分组 ， 它 比 xFS 的 网络 存储器 分组 更为 复杂 ， 因为 xFS 系统 的 存储 服务器 逻辑 上 对应 于个 磁盘 ， 而 SFS 系统 的 存储器 可 根据 系统配置 同时 挂 多个 磁盘 ， 我们 进行 存储 分组 的 基本 单位 是 磁盘 而 不是 存储器 ， 这 也 是 我们 称之为 网络 磁盘 分组 的 原因 　 　 实现 存储 分组 的 关键 数据结构 是 网络 磁盘 分组 映射 表 ， 如图 中 a 所示 ， 它 完成 了 从 存储 分组 标识符 到 组内 一系列 网络 磁盘 的 转换 图中 b 给出 了 SFS 系统 中和图 中 a 对应 的 网络 磁盘 的 逻辑关系 虽然 我们 为 SFS 的 存储 模型 选择 了 带 镜像 功能 的 RAID 结构 ， 从图 可以 看出 ， 目前 的 版本 还 没有 实现 镜像 功能 ， 但 在 图 所示 结构 的 基础 上 扩充 该 功能 并 不 困难 　 　 当 某个 客户 收到 文件创建 请求 后 ， 它 根据 某种 算法 （ 如 轮转 法 ） 选择 一个 管理器 ， 由该 管理器 负责 创建 和 管理 该 文件 收到 创建 文件 请求 后 ， 管理器 根据 轮转 法 （ roundrobin ） 为 被 创建 文件 选择 一个 存储 分组 号 和 一个 逻辑 起始 磁盘 号 ， 存储 分组 号 决定 了 该 文件 对应 的 存储 分组 所 包含 的 网络 磁盘 （ 即 存储器 和 磁盘 对 ） ， 而 逻辑 起始 磁盘 号 决定 了 文件 的 数据 从 哪 一个 网络 磁盘 开始 存放 然后 管理器 向 相应 的 存储器 发送 创建 子 文件 请求 ， 由 存储器 创建 具体 的 子 文件 （ 注 ： SFS 系统 的 文件 数据 在 存储器 端 是 以 JFS 子 文件 的 形式 存在 的 因而 SFS 系统 中 数据 在 磁盘 上 的 位置 是 由 存储 分组 号 和 逻辑 起始 磁盘 号 来 决定 的 当 客户 读写 文件 时 ， 客户 （ 或 管理器 ） 首先 计算 出 被 读写 数据 的 逻辑 块 号 ， 根据 该 文件 的 存储 分组 号 和 逻辑 起始 磁盘 号 来 计算 出 被 读数据 所在 的 逻辑 磁盘 号 ， 然后 向 逻辑 磁盘 号 所在 的 存储器 发送 读写 数据 请求 ， 存储器 计算 出 被 读数据 在 哪个 磁盘 上 以及 在子 文件 上 的 偏移量 ， 进而 执行 JFS 系统 调用 完成 此次 读写操作 图 　 网络 磁盘 分组 映射 关系 　 分布式 的 元 数据管理 　 　 为了 实现 单一 系统 映象 ， SFS 的 一个 关键 设计 原则 是 保证 位置 透明性 ： 用户 不用 关心 数据 在 合作 式 缓存 和 网络 磁盘 分组 中 的 位置 ， 因而 如何 确定 系统 数据 的 位置 是 非常 重要 的 在 SFS 系统 中 ， 该 任务 是 由 管理器 完成 的 ， 它 通过 分布式 的 元 数据管理 以 提供 可 扩展 的 、 分布式 的 位置服务 另外 ， 管理器 的 另 一个 任务 是 维护 合作 式 缓存 一致性 　 　 SFS 系统 中 的 每个 管理器 只 负责 维护 整个 系统 的 一个 文件 子集 的 位置 信息管理 器 使用 这些 位置 信息 能够 把 客户 的 IO 请求 转发 到 正确 的 缓存 或 存储 位置 ， 并 协调 多个 客户 对 相同 数据 的 存取 系统 通过 使用 管理器 映射 表见 图来 实现 分布式 的 元 数据管理 ， 管理器 映射 表是 一组 结点 标识符 ， 用来 指示 哪 一个 结点 管理 哪 一部分 文件系统 系统 首先 把 文件 唯一 的 索引 节点 号 作为 输入 ， 通过 某个 散列 函数 得到 一个 逻辑 管理器 号 ， 以该 逻辑 管理器 号 作为 下标 可以 从 管理器 映射 表 找出 该 文件 对应 的 管理器 ， 因而 管理器 映射 表 提供 了 一种 抽象 功能 ， 便于 实现 管理 服务 的 位置 独立性 图 　 管理器 映射 表 　 　 Zebra 是 较 早 地 把 数据 和 控制 路径 分开 实现 的 一个 文件系统 ， 但 整个 系统 只有 一个 管理器 ［ ］ xFS 为了 避免 Zebra 系统 中 单一 管理 服务器 的 瓶颈 问题 ， 它 将 系统 的 元 数据分布 在 了 多个 管理器 结点 上 ， 以 提高 系统 的 可扩展性 ［ ］ 事实上 在 xFS 系统 之前 ， 许多 大规模 并行 分布式 共享内存 系统 已经 采用 了 这种 分布式 策略 ： 将 有关 缓存 一致性 的 元 数据分布 在 多个 结点 上 SFS 的 元 数据管理 采用 了 与 xFS 非常 类似 的 技术 ， 但 二者 的 具体 实现 仍 存在 不少 差别 由于 SFS 的 存储 子系统 是 基于 AIX 的 物理 文件系统 JFS ， 而且 存储 分组 的 单位 为 网络 磁盘 而 不是 存储 服务器 ， 因而 SFS 在 元 数据 的 内容 和 组织 方面 和 xFS 有 较大 差别 ， 其 inode 不用 记录 文件 中 每块 数据 的 磁盘 位置 ， 而是 用 〈 存储 分组 标识符 ， 起始 逻辑 磁盘 号 〉 来 代替 另外 ， 由于 SFS 与 xFS 的 缓存 管理 （ 包括 一致性 协议 和 替换算法 ） 不同 ， 其 有关 缓存 的 元 数据 内容 及其 管理 亦 存在 较大 差别 SFS 的 管理器 与 xFS 的 管理器 还有 一点 是 不同 的 ： 即 前者 除了 对元 数据 进行 管理 外 ， 还 负责 元 数据 的 存储 　 系统 评价 　 　 我们 将 SFS 与 NFS 在 相同 的 实验 环境 中 进行 了 性能 比较 ， 以 检验 SFS 是否 达到 了 我们 的 设计 目标 我们 的 实验 环境 由台 PowerPC 工作站 组成 ， 其上 分别 运行 AIX 或 AIX ， 结点 之间 用 Mbits 的 以太网 互连 这个 结点 中 ， 其中 台 的 主频 为 MHz ， 其余 均 为 MHz 我们 把 其中 台 主频 为 MHz 的 机器 作为 NFS 的 服务器 ， NFS 的 客户 结点 也 选用 了 剩余 的 主频 为 MHz 的 机器 SFS 系统 的 管理器 和 存储器 分布 于 这个 结点 上 　 　 下面 我们 通过 标准 的 基准 程序 测试 了 SFS 原型 系统 的 读写 文件 带宽 和 可扩展性 对 可扩展性 的 测试 我们 是 基于 Andrew 基准 程序 的 ， 它 是 由 美国 卡内基 梅隆 大学 开发 的 专门 用于 从 应用程序 一级 测试 文件系统 性能 的 标准 测试程序 它共分 个 阶段 ， 分别 对应 于 创建 目录 、 拷贝 文件 、 列 目录 状态 信息 、 扫描 文件 及 编译 文件 对 文件 读写 带宽 的 测试 ， 我们 采用 了 Bonnie 基准 程序 Bonnie 是 由 TimBray 编写 的 、 专门 用于 测量 文件系统 的 顺序 读写 带宽 的 程序 ， 它 完成 一系列 的 测试 ： 大量 的 单个 字符 写 带宽 、 大量 整块 的 写 带宽 、 重写 （ 即先 整块 读出 ， 然后 再 整块 写回 ） 带宽 、 大量 的 单个 字符 读 带宽 以及 大量 整块 的 读 带宽 Bonnie 运行 结束 时会 打印 出 各个 阶段 平均 每秒 能够 处理 的 字节数 以及 CPU 的 利用率 　 读写 带宽 　 　 我们 首先 测试 了 SFS 系统 的 读写 带宽 ， 并 与 NFS 进行 了 对比 在 我们 的 实验 中 ， 被 读写 文件 的 大小 为 M 字节 ， 每次 读写 的 数据 块 大小 为 K 字节 我们 选择 其中 一台 MHz 结点 作为 NFS 的 服务器 ， 它 也 被 用来 运行 SFS 原型 系统 的 管理器 和 存储器 同时 我们 选择 另一台 MHz 的 结点 作为 NFS 和 SFS 的 客户端 ， 并 运行 Bonnie 基准 程序运行 结果 见图 和 图图 　 基于 Bonnie 的 写 带宽 测试 图 　 基于 Bonnie 的 读 带宽 测试 　 　 从图 可以 看出 ， SFS 的 写 带宽 （ 尤其 是 大块 数据 的 写 带宽 ） 胜过 NFS 很多 而图 说明 ， SFS 读 单个 字符 的 性能 优于 NFS ， 而 整块 （ K 字节 ） 读 的 性能 不如 NFS ， 但 在 同一个 数量级 ， 造成 这种 局面 的 主要 原因 是 ： ① 我们 的 测试环境 基于 bps 的 以太网 ， 不能 充分发挥 SFS 系统 的 合作 式 缓存 、 分布式 存储 等 技术 优势 ； ② SFS 原型 系统 尚未 进行 代码优化 我们 相信 在 对 SFS 进行 代码优化 并 采用 高性能 的 网络 后 ， SFS 系统 读 大块 数据 的 性能 能够 达到 或 超过 NFS 　 IO 可扩展性 　 　 在 本 实验 中 ， 我们 分别 启动 了 个 存储器 和 个 管理器 ， 客户 数目 的 变化 范围 是 、 、 、 图示 出 了 在 SFS 和 NFS 系统 中 ， 当 客户 由个 增加 到 个 时 ， 平均 每组 Andrew 程序 的 运行 时间 可以 看出 ， 相对 于 NFS ， SFS 系统 中 Andrew 程序运行 时间 的 增长 趋势 较为 平缓 虽然 NFS 单个 客户 的 性能 要 远远 优于 SFS （ 这 主要 是 由于 SFS 核心层 与 客户 Daemon 之间 的 通信 开销 造成 的 ） ， 但 当 客户 增加 到 个 时 ， SFS 的 性能 已 超过 NFS 这 说明 SFS 系统 的 可扩展性 要 好 于 NFS ， 同时 也 证明 SFS 系统 的 无 集中式 服务器 方案 能够 有效 地 克服 传统 文件系统 的 单一 服务器 瓶颈 问题 图 　 基于 Andrew 的 可扩展性 测试 　 结 　 　 论 　 　 传统 的 分布式文件系统 由于 采用 了 集中式 的 服务器 ， 容易 成为 性能 瓶颈 ， 而且 没有 充分考虑 计算技术 的 发展趋势 ， 分布式系统 的 IO 瓶颈 问题 变得 日益严重 本文 采用 合作 式 缓存 、 并行 分布式 的 数据 存储 和 元 数据管理 等 措施 ， 在 保证系统 单一 映象 和 二进制 兼容性 的 基础 上 ， 对 适合 于 机群 的 分布式文件系统 的 可扩展性 进行 了 研究 应用 对 IO 的 需求 是 永无止境 的 ， 且 其 IO 存取 特征 也 在 不断 发生变化 ， 计算技术 不断 呈现 出新 的 发展趋势 ， 这 一切 都 为 我们 未来 研制 新型 的 分布式文件系统 提出 了 更 大 的 挑战 本 课题 得到 国家 “ 八 六三 ” 计划 基金项目 项目编号 ZD 的 资助 作者简介 ： 王建勇 男 ， 年月生 ， 博士 主要 研究 领域 为 网络 与 分布式系统 　 　 　 　 　 祝明发 男 ， 年月生 ， 研究员 博士生 导师 主要 研究 领域 为 计算机 体系结构 及 　 　 　 　 　 人工智能 　 　 　 　 　 徐志伟 男 ， 年月生 ， 研究员 博士生 导师 主要 研究 领域 为 高性能 计算机系 　 　 　 　 　 统 　 　 　 　 　 张弛 ， 男 ， 年月生 ， 硕士 ， 主要 研究 领域 为 分布式系统 作者 单位 ： 王建勇 （ 北京大学 计算机科学 与 技术 系 　 北京 ） ； 　 　 　 　 　 祝明发 　 徐志伟 　 朱 宁宁 　 张弛 （ 国家 智能 计算机 研究 与 开发 中心 　 北京 ） 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 （ 中国科学院计算技术研究所 　 北京 ） 参考文献 　 　 SandbergRTheSunnetworkfilesystemDesignimplementationandexperienceInProceedingsofUSENIXSummerConferenceCaliforniaUniversityofCaliforniaPress ～ 　 　 WangJianyongZhuMingfaXuZhiweiCooperativecachemanagementinSFSInProcofthethInternationalConferenceonParallelandDistributedProcessingTechniquesandApplicationsLasVegas 　 　 LeffAWolfJLYuPSReplicationalgorithmsinaremotecachingarchitectureIEEETransactionsonParallelandDistributedSystems ～ 　 　 AndersonTEetalServerlessnetworkfilesystemsACMTransactionsonComputerSystems ～ 　 　 ChenPLeeEGibsonGKatzRetalRAIDHighperformancereliablesecondarystorageACMComputingSurveys ～ 　 　 HartmanJHOusterhoutJKThezebrastripednetworkfilesystemACMTransactionsonComputerSystems ～ 原稿 收到 日期 ： ； 修改稿 收到 日期 ：