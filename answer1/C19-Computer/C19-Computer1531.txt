自动化 学报 ACTAAUTOMATICASINICA 年 第卷 第期 VolNo 一种 回归 神经网络 的 快速 在线 学习 算法 韦 　 巍摘 　 要 　 针对 回归 神经网络 BP 学习 算法 收敛 慢 的 缺陷 ， 提出 了 一种 新 的 快速 在线 递推 学习 算法 本 算法 在 目标 函数 中 引入 了 遗忘 因子 ， 并 借助于 非线性 系统 的 最大 似然 估计 原理 成功 地 解决 了 动态 非线性 系统 回归 神经网络 模型 权 系数 学习 的 实时性 和 快速性 问题 仿真 结果表明 ， 该 算法 比 传统 的 回归 BP 学习 算法 具有 更快 的 收敛 速度 关键词 　 回归 神经网络 ， 在线 学习 ， 快速 算法 ANEWONLINERECURSIVELEARNINGALGORITHMFORRECURRENTNEURALNETWORKWEIWEIDepartmentofElectricalEngineeringZhejiangUniversityHangzhouAbstract 　 InthispaperanewonlinerecursivelearningalgorithmforrecurrentneuralnetworkisproposedItovercomesthedisadvantageoftheslowconvergenceoftherecurrentBPalgorithmTherealtimelearningabilityandthefastconvergenceoftherecurrentnetworkmodelofnonlineardynamicalsystemhavebeenobtainedbyintroducingtheforgettingfactorintheobjectivefunctionandthemaximumlikelihoodestimationprincipleSimulationresultsshowthattheproposedalgorithmperformsbetterthanthetraditionalrecurrentBPalgorithmKeywords 　 Recurrentneuralnetworkonlinelearningfastalgorithm 　 引言 　 　 传统 的 前 向 传播 神经网络 在 动态 时序 信号处理 、 非线性 动态 系统控制 等 带有 强 时序 行为 系统 的 应用 中 存在 相当 大 的 困难 目前 虽然 有 通过 在 网络 中 引入 时滞 环节 来 描述 系统 的 动态 性能 ［ ］ ， 但是 能够 更 直接 更 生动 地 反映 系统 动态 特性 的 网络 应该 是 动态 神经网络 ， 它 包含 网络 内部 状态 的 反馈 回归 神经网络 就是 利用网络 的 内部 状态 反馈 来 描述 系统 的 非线性 动力学 行为 构成 回归 神经网络 模型 的 方法 有 很多 ， 但 总 的 思想 都 是 通过 对 前馈 神经网络 中 加入 一些 附加 的 、 内部 的 反馈 通道 来 增加 网络 本身 处理 动态 信息 的 能力 例如 根据 状态 信息 的 反馈 途径 不同 可 构成 两种 不同 的 回归 神经网络 结构 模型 ： Jordan 型 如图 a 和 Elman 型 如图 baJordan 网络结构 　 　 　 　 　 　 　 bElman 网络 结构图 　 回归 神经网络 结构 模型 　 　 由于 在 回归 神经网络 中 存在 内部 反馈 ， 因此 传统 前馈 网络 的 学习 算法 — — BP 法已 不能 直接 应用 于 回归 神经网络 的 学习 年 Pineda 首先 提出 了 回归 神经网络 的 BP 学习 算法 ［ ］ 遗憾 的 是 Pineda 提出 的 方法 仍然 是 依据 梯度 法来 进行 的 ， 因此 不可避免 地会 产生 局部收敛 和 收敛 速度慢 等 缺陷 如何 寻求 一种 快速 的 寻优 算法 来 解决 回归 神经网络 的 学习 问题 已 引起 许多 专家学者 的 关注 本文 将 非线性 系统 的 最小 二乘 递推 算法 引 到 回归 神经网络 权阵 的 学习 中来 ， 利用 此 算法 的 实时性 和 快速性 来 解决 神经网络 的 学习 问题 依据 回归 神经网络 BP 学习 算法 的 原理 和 方法 来 推导 出 回归 神经网络 的 快速 在线 学习 算法 仿真 实验 进一步 证明 了 此 学习 算法 的 有效性 和 快速性 　 回归 神经网络 模型 及其 BP 学习 算法 　 　 对于 一个 动态 的 目标 序列 tktktk … 传统 的 神经元网络 动态 系统 描述 法是 通过 引入 系统 的 先前 输出 ykyk … ykn 等 作为 前向 传播 神经网络 模型 的 输入 信号 ， 从而 来 描述 系统 的 动力学 特性 的 最 典型 的 此类 模型 结构 就是 NarendraKS 等 人 提出 的 带 时滞 环节 的 MLP 网络结构 ［ ］ 显然 ， 引入 过少 的 时滞 环节 即 n 太小 则 不能 描述 具有 高阶 滞后 系统 的 动态 时序 特性 另一方面 ， 引入 过大 的 时滞 环节 即 n 太大 ， 则 需要 系统 更 多 的 记忆 单元 来 保持 所有 的 先验 信息 ， 因此 也 是 不 可取 的 回归 神经网络 就是 针对 这一 缺陷 而 提出 的 它 能 利用 很少 的 记忆 单元 来 描述 任何 系统 的 动力学 特性 　 　 设 Y ， H ， U ， R 分别 表示 m 维 输出 矢量 、 h 维 隐含 单元 的 状态 矢量 、 n 维 输入 矢量 和 r 维 内部 反馈 状态 矢量 ， 则 回归 神经网络 模型 可以 用 如下 方程 来 描述 Yk Φ b β ′ Hk 　 Hk Ψ c γ ′ Uk δ ′ Rk 　 Rk Λ UkRkW 　 　 其中 k 表示 第 k 时刻 ， Φ ， Ψ ， Λ 分别 为 输出 神经元 、 隐含 神经元 和 回馈 神经 单元 的 矢量 函数 ， W 为 s 维 的 神经网络 连接 权 系数 阵 ， 包括 b β c γ δ 当取 RkYkHk ， 时 ， 则 此 网络 分别 为 Jordan 、 Elman 、 MLP 网络结构 　 　 由于 引入 了 回归 单元 函数 ， 可以 实现 对 所有 先验 输入 数据 的 纪录 ； 换言之 ， 回归 神经网络 的 回归 变量 R 能够 依据 网络 输出 Y 或 隐含 单元 的 状态 H 的 信息 用 紧凑 的 形式 来 保留 系统 所有 以前 的 信息 由式 递推 可得 Rk Λ UkRkW Λ Uk Λ UkRkWW … Γ UkW 　 　 其中 Uk ｛ UU … Uk ｝ 为 所有 以前 的 输入 数据 信息 　 　 从式 可以 看出 ， 回归 神经网络 模型 是 最 节约 的 网络 模型 ， 它 无需 存储 所有 的 输入 信息 但 又 能 在 网络 中 反映 出 系统 所有 的 历史 信号 对 当前 系统 响应 的 影响 正是 由于 回归 神经网络 的 这 一 特性 使得 其 在 动态 领域 的 应用 具有 很大 的 吸引力 　 　 由 ， 式 可得 Yk Φ b β ′ Ψ c γ ′ Uk δ ′ RkWFUkRkWW 　 　 将式 代入 式 可以 看出 ， 网络 的 输出 Yk 也 是 系统 过去 所有 输入 Uk 的 函数 要 使 这样 的 回归 网络 真正 能 在 实际 中 得到 应用 ， 首先 需要 解决 此类 网络 的 学习 问题 与 传统 的 前 向 传播 网络 一样 ， 回归 神经网络 的 学习 指标 函数 仍然 选用 误差 平方和 函数 ， 即 给定 的 样本 输出 空间 Zkk … 　 　 对于 动态 序列 Zk 的 网络 权 系数 阵 W 的 最优 求解 可 归结为 求 　 　 其中 E ［ ］ 表示 数学 期望 假设 式 的 极限 存在 ， 则 由 最大 似然 估计 法 可知 权 系数 阵 的 更新 公式 为 WkWk η kPk 　 eTkek 　 　 其中 ekZkFUkRkWk ， 　 　 ek 为 ek 对 参数 阵 W 的 梯度 矢量 　 　 当 PkI 时 ， 即 为 梯度 法 ； 当 PkETkETk 时 ， 即 为 牛顿 法 其中 Ek 　 　 必须 注意 的 是 ， 由于 方程 描述 的 误差 矢量 既 是 W 的 显 函数 又 是 Rk 的 函数 即 W 的 隐 函数 因此 在 求 梯度 矢量 ek 时 不但 要 考虑 ek 对 W 的 显式 求导 而且 要 考虑 它 的 隐式 求导 考虑 了 这些 因素 以后 ， 不难 得出 回归 神经网络 模型 的 BP 学习 算法 见 文献 ［ ］ 　 回归 网络 的 在线 学习 算法 　 　 由于 考虑 的 是 动态 系统 ， 因此 在 处理 样本 数据 时应 尽量 加重 最新 的 输入输出 数据 在 网络 建模 中 的 作用 与 回归 神经网络 的 传统 BP 学习 算法 指标 函数 不同 的 是 ， 本文 提出 的 改进 算法 采用 遗忘 因子 对 Jk 中 的 各项 拟合 残差 平方 进行 加权 办法 来 提高 动态 模型 的 实时性 即取 目标 函数 　 　 　 　 如果 Wk 是 满足 目标 函数 的 极小解 ， 则 有 　 　 假设 最大 似然 估计 k 具备 良好 的 渐近 性质 ， 则 当 k 比较 大时 k 可望 与 真值 W 很 接近 因此 在 k 点 的 附近 el 可 在 k 点 进行 泰勒 展开 ， 并取 一次 项得 　 　 　 　 若 再 取得 一组 新 的 观察 数据 Zk ， 则 可 根据 lk 时刻 以前 的 全部 观察 ， 得出 使 目标 函数 　 　 的 极小解 k 此时 还 应有 　 　 　 由于 k 很大 时 ， k ≈ k ， 故 按式 有 　 　 　 　 　 　 　 利用 式 ， ， ， 对 目标 函数 式 进行 求导 得 　 　 　 将式 — 代入 式 中 可得 　 　 由式 可得 　 　 　 　 引入 变量 　 　 　 则 回归 神经网络 的 联结 系数 矩阵 的 更新 公式 为 kkPk 　 ekkekk ， 　 　 其中 　 　 　 　 利用 矩阵 反演 公式 可将式 进一步 简化 ， 从而 可 得出 整个 回归 神经网络 的 快速 在线 学习 算法 　 　 随机 选取 一个 初始 估计 系数 阵 表示 在 第一步 迭代 时权 系数 阵 按 梯度方向 进行 更新 的 步长 　 　 k ← 　 　 取 当前 最新 的 样本 数据 UkZk ， 按 下列 各式 更新 权 系数 阵 Rk Λ UkRkWk 　 　 ekkZkFUkRkk 　 　 k Λ WUkRkWkk Λ RUkRkWk 　 　 　 ekkFWUkRkWkkFRUkRkWk 　 γ k ［ α Im × m 　 eTkkPk 　 ekk ］ 　 　 Pk ［ α PkPkekk γ keTkkPk ］ α 　 　 kkPkekkekk 　 　 　 　 k ← k 转 　 仿真 研究 　 　 通过 一个 例子 的 仿真 结果 来 评价 本文 提出 的 快速 在线 学习 算法 的 有效性 仿真 模型 选用 文献 ［ ］ 中 给出 的 双线性 DGP 模型 ， 即 zkzkzkukuk 　 　 k … kmax 　 　 其中 uk 选择 独立 的 随机 序列 N ， 　 　 不难看出 ， 此 系统 的 输出 依赖于 系统 过去 的 所有 信息 在 例中 选用 Elman 回归 神经网络 结构 ， 并取 隐含 单元 为 隐含 神经元 的 激励函数 为 Sigmoid 函数 、 输出 单元 为 线性 函数 在 例子 的 仿真 研究 中取 η α ρ W ， R ， 为 — 之间 的 随机数 输入 单元 为 zkuk 当选 用 Elman 回归 神经网络 结构 时其 连接 系数 总共 有个 利用 此 神经网络 结构 和 给定 的 动态 模型 进行 仿真 研究 ， 其 仿真 结果 如图 ， 图 所示 图 　 两种 学习 算法 的 平方 误差 曲线图 　 两种 学习 算法 的 辨识 结果 　 　 图中 的 横座标 表示 迭代 的 次数 ， 纵座标 表示 最近 个点 的 平方 误差 均值 其中 实线 表示 利用 本 算法 进行 学习 的 平方 误差 收敛 曲线 、 虚线 表示 用 传统 回归 网络 BP 学习 算法 训练 的 平方 误差 收敛 曲线 ； 图 表示 最后 一次 迭代 学习 结束 后 的 最近 个点 的 系统 响应 曲线 其中 横座标 表示 时间 相对值 、 纵座标 为 系统 的 输出 即 实线 表示 动态 模型 的 真实 输出 值 、 点 划线 表示 回归 网络 在线 学习 算法 得出 的 辨识 结果 、 点 线 表示 传统 回归 网络 BP 学习 算法 得出 的 辨识 结果 从 仿真 结果 可以 看出 ， 回归 网络 在线 学习 算法 具备 比 传统 回归 BP 算法 更快 的 收敛性 和 更 高 的 辨识 精度 回归 网络 在线 学习 算法 在 学习 过程 中 由于 P 的 正 定性 难以 时时刻刻 得以 满足 ， 因此 不可避免 地 出现 一些 波动 在 本 算法 中 采取 了 一些 措施 来 抑制 算法 可能 出现 的 发散 ， 如当 相邻 两次 的 目标 函数 上升幅度 过大时权 系数 的 更新 将 沿 梯度方向 进行 矩阵 P 将 重置 为 对角 阵 从而 保证 了 回归 神经网络 在线 学习 算法 能够 正常 运行 下去 此外 ， 在 初始 学习 阶段 ， 由于 Wk 远离 真值 W ， 因此 式 的 近似 度 受到 严重 影响 在 仿真 研究 中 本文 采取 在 初始 的 几步 中 选择 负 梯度方向 进行 搜索 ， 而后 转入 在线 递推 算法 的 训练 和 学习 　 国家教委 留学 回国 人员 科研 启动 基金 资助 作者简介 ： 韦巍 　 年 出生 年 在 浙江大学 获 硕士学位 ， 年 获 博士学位 现为 浙江大学 电机系 副教授 近年来 发表 学术论文 余篇 目前 主要 研究 方向 为 智能 控制 理论 和 应用 、 机器人 控制 作者 单位 ： 浙江大学 电机系 　 杭州 　 参考文献 　 　 NarendraKSParthasarathyKIdentificationandcontrolofdynamicalsystemsusingneuralnetworksIEEETransonNN — 　 　 PinedaFJGeneralizationofbackpropagationtorecurrentneuralnetworksPhysicalRevLett — 　 　 KuanCMHornikKWhiteHAconversgenceresultsforlearninginrecurrentneuralnetworksNeuralComputation — 收稿 日期 　