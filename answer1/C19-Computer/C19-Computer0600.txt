自动化 学报 ACTAAUTOMATICASINICA 年 　 第卷 　 第期 　 Vol 　 No 　 一种 前向 神经网络 快速 学习 算法 及其 在 系统 辨识 中 的 应用 王正欧 　 林晨 　 　 摘 　 要 　 提出 一种 基于 最小 二乘 的 前 向 神经网络 快速 学习 算法 与 现有 同类 算法 相比 ， 该 算法 无需 任何 矩阵 求逆 ， 计算 量 小 ， 较 适于 需 快速 学习 的 系统 辨识 和 其他 应用文 中 推导 了 算法 ， 并 给出 一种 更为 简便 的 局部 化 算法 系统 辨识 的 仿真 实例 表明 了 算法 的 优良 性能 　 　 关键词 　 前向 神经网络 ， 快速 学习 ， 系统 辨识 AFASTLEARNINGALGORITHMOFFEEDFORWARDNEURALNETWORKSANDITSAPPLICATIONTOSYSTEMINDENTIFICATIONWANGZHENGOUInstituteofSystemsEngineeringTianjinUniversityTianjin 　 LINCHENFujianAsiaBankLimitedFuzhou 　 Abstract 　 InthispaperweproposeafastlearningalgorithmoffeedforwardnetworksbasedontheleastsquaresComparedwithexistingsimilaralgorithmsthepresentalgorithmdoesnotrequireanymatrixinversionthereforeithasalesscomputationalcostandcanbebettersuitedforsystemindentificationandotherareaswherefastlearningisrequiredWederivethealgorithmandalsogiveanevensimplerandmoreconvenientlocalizedalgorithmSimulationresultsforsystemidentificationshowtheeffectivenessofthealgorithmKeywords 　 Feedforwardneuralnetworks 　 fastlearning 　 systemidentification 　 引言 　 　 自前 向 神经网络 BP 算法 ［ ］ 提出 以来 ， 己 在 控制 界 和 其他 领域 得到 广泛应用 ［ ， ］ 由于 BP 算法 存在 收敛 慢 和 局部 极小 点 等 问题 ， 迄今已 提出 大量 改进 算法 ， 其中 采用 增广 卡尔曼滤波 的 学习 算法 ［ ］ 引起 了 广泛 地 重视 这 类 算法 无需 对 学习 速率 和 势态 项 系数 进行 猜测 ， 且 收敛 速度 快 、 精度高 其 基本 思想 是 把 网络 权值 作为 一个 相应 动态 系统 的 状态 ， 运用 增广 卡尔曼滤波 估计 ， 即可 得到 好 的 效果 然而 这 类 算法 由于 维数 过 高且 需 矩阵 求逆 ， 限制 了 网络 的 规模 多种 局部 化 算法 ［ ， ］ 的 提出 ， 虽 降低 了 维数 ， 但 仍 免不了 矩阵 求逆 本文 提出 一种 基于 最小 二乘 的 快速 学习 算法 ， 无需 任何 矩阵 求逆 ， 其 相应 的 局部 化 算法 更 极大 地 减少 了 计算 量 ， 提高 了 计算速度 ， 较 适于 系统 辨识 等 实际 应用 场合 　 网络 和 相应 的 动态 系统 　 　 考虑 一个 任意 的 具有 一个 输入 层 、 一个 输出 层 和 若干 隐层 的 前 向 网络 ， 设此 网络 包含 一个 由 全部 连接 权 构成 的 权 向量 θ ∈ RM ， 并 认为 一个 单元 的 阈值 是 由 单位 强度 输入 连接 到 该 单元 的 连接 权 假定 网络 中 单元 的 激活 函数 除 输入 层外 是 S 形 函数 φ xex ， 其中 x 为 该 单元 的 纯 输入 ， 它 等于 输入 到 该 单元 信号 的 加权 和 现在 的 问题 是 要 确定 θ ， 使得 对于 一组 给定 的 输入 向量 ik ∈ Rnk … p ， 网络 产生 的 实际 输出 向量 ok ∈ Rmk … p 应 尽可能 接近 期望 的 输出 向量 dk ∈ Rmk … p 这里 考虑 在线 学习 算法 ， 即 输入 模式 依次 输入 至 输入 层 ， 权值 则 递推 地 改进 将 θ 视为 如下 非线性 动态 系统 的 状态 θ k θ k θ 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 dkf θ ikek 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 这里 f θ ikok θ 表示 网络 的 输入 、 权值 和 输出 关系 的 非线性 函数 ， ek 为 模型 误差 f θ ik 可 在 当前 估计值 k 附近 展开 ， 故 ， 式 可 改写 为 θ k θ k θ 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 dkfkikGTk θ k ρ kek 　 　 　 　 　 　 　 　 这里 Gk 是 M × m 维 梯度 矩阵 　 　 　 　 　 　 　 　 　 　 　 ρ k 是 f 的 展开式 的 高阶 余项 若设 ξ kfkikGTkk ρ k 　 　 　 　 　 　 　 　 　 　 则 式 可 简写 为 dkGTk θ ξ kek 　 　 　 　 　 　 　 　 　 　 　 　 　 如 将 网络 中 单元 由 至 N 排列 ， 并取 权值 向量 为 θ θ T 　 θ T … θ TNT ， 则 式 可 写 为 　 　 　 　 　 这里 θ i 为 连接 到 第 i 个 单元 的 权 向量 ， ck ξ kekGk 中 的 偏 导数 可 通过 网络 反传 ok 求得 　 学习 算法 　 全局 递推 最小 二乘 算法 GRLS 　 　 基于 模型 给定 一个 输入输出 序列 对 ijdjj … k ， 选择 如下 的 目标 函数 　 　 　 　 　 　 　 　 　 这里 ‖ ‖ 表示 向量 的 欧几里得 范数 ， eij 是 式 中 第 i 个 建模 误差 ， λ 为 遗忘 因子 ＜ λ ≤ ， 估计值 k 是 由 使 ε k 达到 极小 而 得 的 此解 可 从 下列 正则方程 推导 　 　 　 　 　 其中 × M 维 向量 gTij 是 矩阵 GTj 的 第 i 行由式 可得 kSktk 　 　 　 　 　 　 　 　 　 　 　 　 　 　 其中 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 由式 可 推得 GRLS 算法 如下 ： 　 　 　 　 　 　 　 　 算法 初值 可取 prI ， r 为任 一小 正数 ， I 为 单位 阵 ， 可为 非 零 的 随机 向量 ， 式 的 推导 见 附录 　 　 注意 ， 上述 算法 要求 存储 和 改进 一个 M × M 维 矩阵 ， 但 不 要求 任何 矩阵 求逆 　 局部 化 递推 最小 二乘 算法 LRLS 　 　 在 GRLS 算法 中 P 阵 的 维数 相当 高 ， 仍难 实用 一种 有效 的 简化 方法 是 将 系统 分解成 若干 较 小 的 子系统 ， 例如 分解 到层 一级 、 神经元 一级 乃至 到 单个 连接 权 一级 等 这里 拟 在 神经元 一级 推导 LRLS 算法 　 　 考虑 网络 中 第 j 个 单元 ， 假设 它 具有 Mj 维 连接 权 向量 θ j 和 Mj 维 的 输入 向量 ， θ j 中 包含 了 该 单元 的 阈值 该 单元 对 总 输出 Ok 的 影响 可用 梯度 矩阵 GjTk 描述 ， 它 是 GTk 的 第 j 个子 块 　 　 　 　 　 　 　 　 　 　 　 　 　 　 此 矩阵 的 第 i 行是 ， 用 gjTik 表示 如 相对 于 第 j 个 神经元 的 权值 线性化 f ， 则 类似 于式 有 dkfkikGjTk θ jjk ρ jkekGjTk θ j ξ jkek 　 　 其中 　 　 　 　 　 ξ jkfkikGjTkjk ρ jk 　 　 　 　 　 　 　 　 　 　 　 　 这里 台劳 展开 仅 对 第 j 个 单元 的 权值 进行 ， 其余 权值 采用 k 时刻 以前 的 估计值 此 非线性 函数 的 线性化 可 对 每个 单元 平行 地 进行 　 　 对于 每个 单元 改进 权值 的 局部 化 算法 类似 于 ， 式 可得 　 　 　 　 　 　 　 　 　 　 　 注意 到 LRLS 仅 要求 存储 和 改进 一个 Mj × Mj 维 矩阵 随 单元 不同 Mj 可 不同 ， 此 矩阵 维数 大大 小于 GRLS 中 相应 矩阵 的 维数 ， 故 LRLS 的 存储 和 计算 量 远 小于 GRLS 　 LRLS 在 系统 辨识 中 的 应用 　 　 系统 辨识 通常 和 自 适应控制 相 联系 ， 要求 快速 及时 地 辨识 以利于 控制 ， 但 通常 的 学习 算法 受 收敛 速度限制 ， 效果 难以 理想 LRLS 算法 可较 好 地 弥补 这一 缺陷 　 　 为 研究 该 算法 在 系统 辨识 中 的 性能 ， 这里 将 它 同 原始 BP 算法 作 了 仿真 比较 为 实用 计 ， 只 研究 LRLS 的 性能 由于 前向 多层 网络 可 对 任意 线性 和 非线性 函数 映射 不失 一般性 ， 这里 仅 研究 线性系统 辨识 例子 为 简单 计 ， 在 仿真 中 略去 了 系统 模型 误差 ， 以 突出 说明 算法 的 收敛性 和 精度 由 经验 选取 λ ， 以利于 算法 收敛 以下 用个 例子 说明 新 算法 的 优越性 　 　 例 考虑 简单 单 变量 系统 ykykuk 这里 输入 uk 是 在 ， 区间 中 均匀分布 的 随机变量 ， 网络结构 为 两个 输入 uk 和 yk 、 一个 单元 隐层 及 一个 输出 yk 初始 权值 为 与 间 均匀分布 的 随机 值 ， Pj 初始 阵中 r 原始 BP 算法 中 学习 速率 和 势态 项 系数 分别 取为 和 训练样本 数为 训练 停止 准则 为 ei ， 其中 e 表示 个 样本 中 输出 误差 最大值 ， 下标 i 表示 迭代 次数 图 表示 采用 LRLS 和 原始 BP 算法 在 不同 初始条件 下次 训练 的 平均误差 随 迭代 次数 的 变化 曲线图 　 对例 采用 LRLS 和 BP 的 平均 输出 误差 曲线 　 　 例 考虑 动态 系统 ykykykukuk 这里 uk 是 和 间 均匀分布 随机变量 ， 网络结构 为 ， 初始条件 和 停止 准则 同例 ， 训练样本 为 图 表示 LRLS 和 原始 BP 算法 收敛 速度 比较 ， 其中 BP 算法 参数 取法 同例 图 　 对例 采用 LRLS 和 BP 的 平均 输出 误差 曲线 　 　 例 考虑 单 输入 二 输出 系统 ykukykukuk 网络 取为 双隐层 结构 ， uk 取为 — 间 均匀 随机变量 Pj 初始 阵中取 r ， 网络 初始 权重 取为 — 间 均匀 随机数 ， 阈值 全部 取为 判断 收敛 的 误差 函数 取为 均方 误差 其中 样本数 n ， ei 为 第 i 个 样本 的 学习 误差 图 表示 LRLS 与 原始 BP 算法 收敛 速度 的 比较 ， 其中 BP 算法 取 学习 速率 为 ， 势态 项 系数 为 图 　 对例 采用 LRLS 和 BP 的 均 方 输出 误差 曲线 注 LRLS 迭代 至次 精度 达 ， 而 BP 到次 精度 才 达例 考虑 二 输入 二 输出 动态 系统 ykykukukykykukuk 网络 取为 双隐层 结构 ， 其中 ukuk 均 取为 — 间 均匀 随机变量 模型 初始值 取为 yy ， 网络 和 算法 的 初始值 同例 ， 判断 收敛 用 的 准则 仍为 均方 误差 图 显示 了 LRLS 与 原始 BP 算法 收敛 速度 的 比较 ， 其中 BP 算法 的 参数 同例 图 　 对例 采用 LRLS 和 BP 的 均 方 输出 误差 曲线 注 LRLS 迭代 至次 精度 达 ， 而 BP 到次 精度 才 达 　 　 从 上述 四个 例子 可见 LRLS 具有 极好 的 性能 ， 其 收敛 速度 远比 原始 BP 算法 快 　 结论 　 　 本文 推导 了 一种 前向 网络 的 快速 学习 算法 GRLS ， 给出 了 一种 更为 简便 的 局部 化 算法 LRLS 该 算法 避免 了 矩阵 求逆 ， 减少 了 计算 量 ， 其中 LRLS 算法 更 使 存储量 和 计算 量 大为 降低 仿真 结果表明 了 这种 算法 具有 收敛 快速 、 精度高 的 特点 ， 是 一种 实用 的 前 向 网络 快速 学习 的 有效 算法 ， 适用 于 系统 辨识 等 实际 应用 场合 　 天津市 自然科学 基金 资助 课题 作者简介 ： 王正欧 　 年 毕业 于 天津大学 自动化 仪表 专业 现为 该校 教授 和 美国纽约 科学院 成员 及 IEEE 成员 ， 长期 从事 系统 辨识 、 系统 建模 理论 及 应用 的 教学科研 工作 近年来 在 国内外 发表 论文 余篇 近期 研究 方向 为 神经网络 理论 及 应用 、 人工智能 等 　 　 　 　 　 林 　 晨 　 年生 ， 年 毕业 于 天津大学 ， 年于 天津大学 系统工程 研究所 获工学 硕士学位 现 任职 于 福建 亚洲 银行 研究 兴趣 为 神经网络 、 系统 辨识 和 控制 、 经济 金融 等 领域 作者 单位 ： 王正欧 ； 天津大学 系统工程 研究所 　 天津 　 　 　 　 　 　 林 　 晨 ： 福建 亚洲 银行 　 福州 　 参考文献 ［ ］ 　 RumelhartDE ， HinonDEWilliamsRJLearninginternalrepresentationsbyerrorpropagationparalleldistributedprocessingExplorationsinthemicrostructureofcognitionCambridgeMAMITPress — 　 NarendraKSParthasarathyKIdentificationandcontrolofdynamicalsystemsusingneuralnetworksIEEETransNeuralNetworks — 　 ChenSBillingsSAGrantPMNonlinearsystemidentificationusingneuralnetworksIntJControl — 　 SinghalSWuLTrainingfeedforwardnetworkswiththeextendedKalmanalgorithmInProcoftheIEEEIntConfonAcousticsSpeechandSingalProcessingGlasgow — 　 KolliasSAmastassionDAnadaptiveleastsquaresalgorithmfortheefficienttrainingofartificialneuralnetworksIEEETransCircuitsandSystems — 　 ShahSPalmieriFDatumMOptimalfilteringalgorithmsforfastlearninginfeedforwardneuralnetworksNeuralNetworks — 收稿 日期 　 附 　 录 算法 ， 的 推导 　 　 从式 有 　 　 　 　 　 　 　 　 A 同理 可 得 　 　 　 　 　 　 　 A 假定 下面 的 等式 成立 　 　 　 　 　 　 　 　 　 　 　 A 这里 hk 有 与 gik 相同 的 维数 需要 指出 ， 严格 地说 ， A 式 不能 精确 成立 ， 此处 仅为 推导 方便 起 见 ； 后面 ， hkhTk 仍 将 用 ∑ gikgTik 替代 ， 故 不会 出现 误差 应用 矩阵 求逆 公式 ABCCT 　 ABBCICTBCCTB 并设 BSk λ 及 Chk 由 A 和 A 式 可得 Sk λ Sk λ SkhkhTK × Sk λ hTkSkhk 　 A 定义 PkSk ， 则 A 式 可 写 为 Pk λ Pk λ PkhkhTkPK × λ hTkPkhk 　 A 由 A 式 知 　 　 　 　 　 　 　 　 　 　 　 　 A 必 成立 ， 即 hkhTk 的 对角 元素 之 和 等于 所有 构成 hkhTk 的 矩阵 的 对角 元素 之 和 ， 因此 有 　 　 　 　 　 　 　 　 A 由 A ， A 及 A 式 可得 　 　 　 　 A 定义 ak λ Pkhk λ hTkPkhk 　 　 　 　 　 　 　 　 A 由 A 式 得 Pk λ Pk λ akhTkPk 　 　 　 　 　 　 　 　 　 　 　 A 则 A 式 可 写 为 ak λ akhTkPkhk λ Pkhk 　 　 　 　 　 　 　 　 A 或 　 　 　 　 　 　 ak λ Pk λ akhTkPkhkPkhk 　 　 　 　 　 　 　 　 A 另外 ， 式 中 的 参数估计 k 可写 为 　 　 A 把 A 式 代入 A 式 得 　 　 　 　 　 A 由 ， A 和 A 式 可得 　 A 略去 高阶 项 ρ iki … m 即得 　 　 　 　 A 从 上面 的 推导 中 已 得到 了 和 式