信息 与 控制 INFORMATIONANDCONTROL 年 第卷 第期 VolNo 自 学习 神经元 及 自 学习 BP 网络 赖晓平 周鸿兴 　 　 摘 　 要 ： 本文 针对 现有 人工 神经元 及 BP 网络 的 缺点 ， 从 实现 角度 提出 一种 新型 神经元 及 新型 BP 网络 — — 自 学习 神经元 及 自 学习 BP 网络 ． 自 学习 神经元 的 突出 特点 之一 是 它 的 内部 有 正向 通道 、 反向 通道 及 学习 器 ， 因而 能够 独立 完成 信息 的 正向 传播 、 误差 的 反向 传播 及 神经元 参数 的 修正 ． 由 自 学习 神经元 组成 的 自 学习 BP 网络 可以 真正 做到 正向 传播 信息 、 反向 传播 误差 及 学习 的 并行 化 ． 本文 还 考虑 了 自 学习 BP 网络 的 学习 问题 ， 提出 一种 新 的 学习策略 ． 我们 的 仿真 结果表明 这种 学习策略 有 很 好 的 学习效果 ． 　 　 关键词 ： 自 学习 神经元 ， 自 学习 BP 网络 ， 学习策略 ， 面向 神经元 　 　 中 图 分类号 ： TP 　 　 　 　 　 　 文献 标识码 ： ASELFEARNINGNEURONSANDSELFLEARNINGBPNETWORKSLAIXiaopingZHOUHongxingDepartmentofControlEngineeringShandongUniversityatWeihaiWeihaiDepartmentofMathenaticsShandongUniversityJinanAbstractAnewkingofneuronsandnewkindifBPnetworksselflearningneuronsandselflearningBPnetworksarepresentedinthispaperwithawiewtotheirimplementationAselflearningBPnetworkiscomposedofselflearningneuronsOneoftheprominentcharacteristicsoftheselflearningneuronisthatithasaforwardchannelabackwardchannelandaleamerThismakeseachneuroninaselflearningBPnetworkaccomplishtheforwardpropagationofmessagebackwardpropagationoferrorsandmodificationofitsparametersindependentlyandhencemakesthenetworkimplementparallelcomputingeasityAnewtrainingpolicyforBPnetworksisalsopresentedinthispaperSimulationresultsdemonstratetheeffectivenessofthepolicy 　 　 KeywordsselflearningneuronsselflearningBPnetworkstrainingpolicyneuronorientedalgorithm 　 引言 　 　 多层 前向 网络 是 研究 和 应用 得 最 广泛 也 是 最 成功 的 人工 神经元网络 模型 之一 ． 多层 前向 网络 是 一种 映射 型 网络 ， 理论 上 ， 隐层 采用 Sigmoid 激活 函数 的 三层 前向 网络 能以 任意 精度 逼近 任一 连续 非线性 函数 ． 神经元网络 可以 根据 与 环境 的 相互作用 对 自身 进行 调节 即 学习 ． 一个 BP 网络 即 是 一个 多层 前向 网络 加上 误差 反向 传播 学习 算法 ， 因此 一个 BP 网络 应有 三项 基本功能 ： （ ） 信息 由 输入 单元 传到 隐 单元 ， 最后 传到 输出 单元 的 信息 正向 传播 ； （ ） 实际 输出 与 期望 输出 之间 的 误差 由 输出 单元 传到 隐 单元 ， 最后 传到 输入 单元 的 误差 反向 传播 ； （ ） 利用 正向 传播 的 信息 和 反向 传播 的 误差 对 网络 权 系数 进行 修正 的 学习 过程 ． 目前 ， 多层 前向 网络 的 权 系数 学习 算法 大都 采用 BP 算法 及 基于 BP 算法 的 改进 算法 ， 如 带动 量项 的 BP 算法 ， 同时 修正 权 系数 及 激活 函数 的 BP 算法 及 自 适应 调整 学习 步长 的 BP 算法 等 ． 在 这些 算法 中 ， 同时 修正 权 系数 及 激活 函数 的 BP 算法 及 自 适应 调整 学习 步长 的 BP 算法 具有 较 好 的 收敛 特性 ． BP 网络 在 非线性 系统 的 辨识 、 预测 及 控制 等 问题 中有 很 好 的 应用 ． 　 　 但是 ， 当 我们 仔细分析 现有 BP 网络结构 、 权 系数 学习 算法 及其 实现 时 就 会 发现 以下 的 局限性 ： （ ） 现有 的 BP 网络结构 只能 完成 信息 的 正向 传播 任务 ， 误差 的 反向 传播 及权 系数 的 学习 都 是 人为 地 附加 在 网络 上 的 ； （ ） 从 表面 上 看 ， 现有 的 BP 网络 具有 并行 结构 ， 但 实际上 由于 学习 算法 都 是 面向 整个 网络 的 ， 因而 不能 做到 计算 的 并行 化 ； （ ） 现有 的 BP 网络 的 神经元 之间 的 连接 是 带权 连接 ， 从 实现 角度 考虑 ， 当由 神经元 组成 网络 时 ， 不太 方便 ， 从 概念 上 讲 ， 当 网络 由 单层 （ 或 单个 ） 神经元 组成 时 ， 不再 有 神经元 间 的 连接 ， 因而 不再 有 连接 权 系数 ， 这是 矛盾 的 ； （ ） 当 网络 中 某个 神经元 的 权 系数 受到冲击 而 出现 偏差 需 重新学习 时 ， 由于 算法 是 面向 网络 的 ， 不得不 对 整个 网络 的 权 系数 都 重新学习 ； （ ） 现有 的 BP 网络 学习 算法 对 所有 神经元 的 连接 权 系数 都 采用 同一 算法 进行 学习 ， 缺乏 灵活性 ． 　 　 本文 针对 现有 BP 网络 的 这些 问题 ， 提出 一种 新型 的 神经元 及 新型 的 BP 网络 ． 这种 新型 的 神经元 内有 传播 信息 的 正向 通道 、 传播 误差 的 反向 通道 及 利用 正向 传播 的 信息 及 反向 传播 的 误差 进行 学习 的 学习 器 ． 信号 权 系数 、 偏量 权 系数 及 激活 函数 的 陡度 都 认为 是 神经元 的 内部 参数 ， 学习 器 的 任务 就是 调整 这些 参数 ． 由 这种 新型 的 神经元 连接 在 一起 构成 一种 新型 的 BP 网络 ， 它 将 有 以下 特点 ： （ ） 能够 完整 地 实现 BP 网络 的 三项 基本功能 ， 即 正向 传播 信息 、 反向 传播 误差 及 参数 的 学习 ； （ ） 信息 的 正向 传播 、 误差 的 反向 传播 及 参数 的 学习 都 是 并行 的 ， 从而 真正 做到 计算 的 并行 化 ； （ ） 神经元 之间 的 连接 是 无权 的 ， 实现 非常 方便 ； （ ） 对 神经元 参数 的 学习 采用 面向 神经元 的 算法 ， 从而 当 某个 神经元 受到冲击 ， 参数 出现 偏差 时 ， 可以 单独 对 这个 神经元 进行 学习 ， 加快 学习 的 速度 ； （ ） 不同 的 神经元 可以 采用 不同 的 学习 算法 ， 从而 构成 各种 学习策略 ， 极大地提高 了 BP 网络 学习 的 灵活性 ． 　 　 本文 还 将 研究 BP 网络 的 学习 问题 ， 提出 一种 新 的 学习策略 ， 这种 学习策略 的 收敛 特性 比 通常 的 同时 修正 权 系数 及 激活 函数 的 BP 算法 及 自 适应 调整 学习 步长 的 BP 算法 都 好得多 ． 本文 给出 的 仿真 例子 也 说明 了 这 一点 ． 　 自 学习 神经元 　 　 定义 称 一个 神经元 为 自 学习 神经元 ， 如果 它 具有 正向 、 反向 及 学习 等 三种 内部 状态 ， 且 当 它 处于 这 三种 状态 时能 分别 实现 正向 传播 信息 、 反向 传播 误差 及 调节 自身 参数 等 三项 基本功能 ． 　 　 信息 的 正向 传播 当自 学习 神经元 处于 正向 状态 时 ， 神经元 进行 信息 的 正向 传播 ． 输入 信号 uuu … un τ 与 偏置 量 一起 组成 输入 向量 φ uu … un τ ， φ 与权 系数 向量 wwww … wn τ 点 乘 ， 产生 净 输入 net φ τ w ， net 与 系数 μ 相乘 得到 总 输入 z μ net ， z 通过 激活 函数 f 作用 产生 神经元 的 活性 ofz ， 再 与 全一 向量 σ … τ 相乘 得到 输出 向量 ooo … o τ ∈ Rm ． 　 　 权 系数 向量 w 中 的 w 称为 偏量 权 系数 ， ww … wn 称为 信号 权 系数 ． 激活 函数 通常 取 Sigmoid 函数 ： 非对称 的 fxexpx 或 对称 的 fxexpxexpx ， 也 可以 取 线性 函数 fxx ． 系数 μ 称为 激活 函数 的 陡度 ． 调整 μ 可以 改变 神经元 的 活性 o 关于 净 输入 net 的 变化率 ， 这 相当于 改变 神经元 的 激活 函数 ． 图 　 正向 状态图 　 反向 状态 　 　 正向 状态 时自 学习 神经元 的 外部 特性 如图所示 ， SI 代表 信号 输入 端 ， SO 代表 信号 输出 端 ． 　 　 误差 的 反向 传播 当自 学习 神经元 处于 反向 状态 时 ， 神经元 进行 误差 的 反向 传播 ． 其 外部 特性 如图所示 ， 正向 状态 时 的 SO 此时 为 误差 输入 端 EI ， 正向 状态 时 的 SI 此时 为 误差 输出 端 EO ． 输入 误差 eee … em τ 由 EI 端 输入 到 神经元 内部 ， 与 全一 向量 σ 点 乘后 再 与 f ′ zf ′ x 为 fx 的 导数 相乘 产生 总 误差 ， 经 μ w 加权 得到 输出 误差 ε ε ε … ε n τ δ μ www … wn τ 由 EO 端 输出 ． 　 　 参数 的 学习 — — 面向 神经元 的 学习 算法 自 学习 神经元 的 参数 包括 信号 权 系数 ww … wn ， 偏量 权 系数 w 及 激活 函数 的 陡度 μ ， 称 θ www … wn μ τ w τ μ τ 为 神经元 的 参数 矢量 ． 神经元 处于 学习 状态 时 ， 由 内部 的 学习 器 对 参数 矢量 θ 进行 修正 ， 利用 正向 传播 的 信息 如 φ 、 net 、 反向 传播 的 误差 δ 及 旧 的 参数 矢量 θ oldw τ old μ old τ 通过 某一 学习 算法 计算 新 的 参数 矢量 θ neww τ new μ new τ ． 这里 所 采用 的 学习 算法 是 面向 神经元 的 — — 利用 神经元 内 存储 的 信息 修正 神经元 自己 的 参数 ， 学习 算法 本身 的 设计 参数 可 由 外界 设定 ． 面向 神经元 的 学习 算法 采用 统一 的 形式 θ new θ old Δ θ new 　 　 　 　 　 　 　 　 　 其中 的 Δ θ new Δ w τ new Δ μ new τ ， 不同 的 算法 有 不同 的 计算公式 ． 下表 给出 了 一些 面向 神经元 的 学习 算法 ． 表面 向 神经元 的 学习 算法 算法 Δ wnew Δ μ new 设计 参数 算法 η δ μ old φ η ＞ 算法 α Δ wold η δ μ old φ ＜ α ＜ η ＞ 算法 γ Δ μ oldk δ net ＜ γ ＜ k ＞ 算法 α Δ wold η δ μ old φ γ Δ μ oldk δ net ＜ α ＜ η ＞ ＜ γ ＜ k ＞ 　 　 为了 实现 自 学习 神经元 的 三项 功能 ， 神经元 内部 应有 传播 信息 的 前 向 通道 、 传播 误差 的 后 向 通道 及 修正 神经元 参数 的 学习 器 ． 图是 自 学习 神经元 的 一种 实现 ． K 和 K 都 连 到 F 位置 时 构成 前向 通道 ， 自 学习 神经元 处于 前向 状态 ． K 和 K 都 连 到 B 位置 时 构成 后 向 通道 ， 自 学习 神经元 处于 后 向 状态 ． 自 学习 神经元 在 三个 状态 间 交替 变换 ， 在 正向 状态 和 反向 状态 之后 ， 学习 器 利用 正向 状态 时 传播 的 信息 及 反向 状态 时 传播 的 误差 调整 神经元 自身 的 参数 时 ， 自 学习 神经元 处于 学习 状态 ． 图自 学习 神经元 　 自 学习 BP 网络 ． 　 自 学习 BP 网络 及 与 普通 BP 网络 的 区别 　 　 定义 由 自 学习 神经元 采用 无权 连接 构成 如图所示 有 一个 输入 层 、 若干个 隐层 、 一个 输出 层 另加 一个 比较 层 的 神经元网络 ， 称为 自 学习 BP 网络 ， 用 记号 表示 为 Nnn … nMnM ． 这里 M ≥ ， ni 表示 第 i 层 神经元 个数 ， i 表示 输入 层 ， i … M 表示 隐层 ， iM 表示 输出 层 ． 图自 学习 BP 网络 　 　 自 学习 BP 网络 的 比较 层 神经元 有 正向 状态 和 反向 状态 ． 正向 状态 时 将 输出 神经元 的 输出 和 期望 输出 y 进行 比较 ． 反向 状态 时 将 正向 状态 时 的 比较 误差 （ y ） 加权 后 形成 加权 误差 并 将 其 反向 传输 ． 　 　 自 学习 BP 网络 输出 层 神经元 的 激活 函数 一般 取为 线性 函数 ： fxx ． 输入 层 神经元 的 SIEO 端 只 正向 输入 信息 ， 不 反向 传播 误差 ． 每个 输入 神经元 只 接受 一维 输入 信号 ， 信号 权 系数 等于 ， 偏量 权 系数 等于 ， 激活 函数 的 陡度 可变 ， 因而 输入 层 神经元 的 权 系数 向量 可 表示 为 ： wj τ j … n ， 其中 上标 表示 第一层 即 输入 层 ， 下标 j 表示 第 j 个 神经元 ． 输入 层 神经元 只有 激活 函数 的 陡度 需要 学习 ． 　 　 虽然 从 表面 上 看 ， 自 学习 BP 网络 和 普通 的 BP 网络 很 类似 ， 但 它们 之间 是 有 区别 的 ． 与 普通 BP 网络 相比 ， 自 学习 BP 网络 有 以下 五个 优点 ． 　 　 （ ） 由于 自 学习 神经元 内 的 权 系数 向量 可以 对 神经元 的 输入 信号 及 反传 误差 加权 ， 从而 自 学习 BP 网络 神经元 之间 的 连接 是 无权 的 ， 不论 对 软件 实现 还是 对 硬件 实现 ， 这 都 是 非常 方便 的 ． 　 　 （ ） 自 学习 BP 网络 本身 具有 正向 传播 信息 、 反向 传播 误差 及 对 神经元 参数 进行 学习 的 能力 ． 当 每个 神经元 都 处于 正向 状态 时 ， 网络 可 正向 传播 信息 ， 当 每个 神经元 都 处于 反向 状态 时 ， 网络 可 反向 传播 误差 ， 当 每个 神经元 都 处于 学习 状态 时 ， 网络 可 对 网络 参数 进行 修正 ． 　 　 （ ） 自 学习 BP 网络 的 各个 神经元 除 它们 之间 的 连接 关系 外 相对 独立 ， 从而 可以 实现 信息 正向 传播 、 误差 反向 传播 及 参数 调整 的 并行 化 ． 　 　 （ ） 自 学习 BP 网络 中 的 每个 神经元 都 有 自己 的 学习 器 ， 采用 的 学习 算法 是 面向 神经元 的 ， 因而 各个 神经元 可以 采取 不同 的 学习 算法 ． 　 　 （ ） 各个 神经元 的 学习 器是 相互 独立 的 ， 因此 当 某个 神经元 受到冲击 需要 重新学习 时 ， 可以 只 修正 这一 神经元 的 参数 ， 其它 神经元 的 参数 保持 不变 ． 这种 学习 方式 比 修改 所有 神经元 参数 的 学习 方式 有 更 快 的 收敛 速度 ． ． 自 学习 BP 网络 的 工作 原理 　 　 由于 每个 自 学习 神经元 都 有 正向 传播 信息 、 反向 传播 误差 及 学习 的 功能 ， 且 神经元 之间 的 连接 又 是 无权 的 ， 因而 BP 网络 的 大部分 工作 都 分散 到 了 各个 神经元 ． 由 自 学习 神经元 构成 的 自 学习 BP 网络 也 有 三种 状态 ： 正向 状态 、 反向 状态 及 学习 状态 ． 当 网络 中 所有 神经元 都 处于 正向 状态 时 ， 网络 为 正向 状态 ． 所有 神经元 都 处于 反向 状态 时 ， 网络 为 反向 状态 ． 所有 神经元 都 处于 学习 状态 时 ， 网络 为 学习 状态 ． 网络 状态 间 的 转换 由 网络 本身 负责 ． 另外 ， 神经元 的 学习 算法 有些 由 外部 设定 的 设计 参数 需要 由 网络 来 给定 ． 　 　 一旦 将 自 学习 神经元 连成 自 学习 BP 网络 ， 神经元 之间 的 信号 关系 就是 确定 的 ． 设 网络 的 输入 为 xxx … xn τ ， 期望 输出 为 yyy … ynM τ ， 实际 输出 为 y … nM τ ． 则 网络 在 正向 传播 信息 的 过程 中 ， 各层 神经元 的 输入输出 信号 之间 有 如下 关系 ： uljxjj … n 　 　 　 　 　 　 　 　 　 　 　 joMjj … nM 　 　 　 　 　 　 　 　 其中 uij 表示 第 i 层 第 j 个 神经元 的 输入 信号 ， i 时 是 标量 ， i 时 是 ni 维 向量 ， oij 表示 第 i 层 第 j 个 神经元 的 输出 信号 ． 网络 在 反向 传播 误差 的 过程 中 各层 神经元 的 输入输出 误差 之间 有 如下 关系 ： eMj λ jyjjj … nM 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 其中 eij 表示 第 i 层 第 j 个 神经元 的 输入 误差 ， iM 时 是 标量 ， iM 时 是 ni 维 向量 ， eijk 是 其 第 k 个 分量 ． λ j 为 比较 层 第 j 个 神经元 对 比较 误差 （ yjj ） 的 加权 因子 ． ε ikj 表示 第 i 层 第 k 个 神经元 的 输出 误差 ε ik 的 第 j 个 分量 ． 　 　 定理 设自 学习 BP 网络 为 Nnn … nMnM ， 定义 网络 对 样本 xy 的 匹配 误差 为 　 　 　 　 　 　 　 其中 是 网络 在 输入 x 下 的 实际 输出 ， Λ diag λ λ … λ nM λ j （ j … nM ） 等于 网络 比较 层 第 j 个 神经元 的 加权 因子 ， 则 ： 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 证明 先证 （ ） 式 ． 当 iM 时 ， 当 iMM … 时 ， （ ） 式 得证 ． 　 　 由 （ ） 式 ， 立即 有 ： 定理 证毕 ． 　 　 定理 表明 ， 若 样本 匹配 误差 如 （ ） 式 定义 ， 则 自 学习 BP 网络 中 神经元 的 总 误差 δ 就是 普通 BP 网络 中 的 反向 传播 误差 δ ， 从而 面向 神经元 的 学习 算法 — 学习 算法 对 参数 的 更新 都 是 沿 匹配 误差 对 参数 的 负 梯度方向 即 最速 下降 方向 进行 的 ． 　 自 学习 BP 网络 的 学习策略 　 　 由于 自 学习 BP 网络 中 的 神经元 内部 有 参数 学习 器 ， 其中 的 学习 算法 是 面向 神经元 的 ， 因而 对 网络 中 的 各个 神经元 可以 采取 不同 的 学习 算法 ， 从而 构成 各种 学习策略 ， 这 是 自 学习 BP 网络 和 普通 BP 网络 相比 的 一个 突出 优点 ． 面向 神经元 的 学习 算法 可以 是 算法 算法 中 的 任何 一种 ， 也 可以 是 其它 类型 的 算法 如 最小 二乘 算法 等 ． 　 　 学习策略 ： 输入 层 神经元 参数 固定 ， 隐层 神经元 及 输出 层 神经元 均 采用 算法 调整 参数 ， 其中 的 设计 参数 η 常数 ． 　 　 学习策略 ： 输入 层 神经元 参数 固定 ， 隐层 神经元 及 输出 层 神经元 均 采用 算法 调整 参数 ， 其中 的 设计 参数 α 及 η 均取 常数 ． 　 　 学习策略 ： 输入 层 神经元 参数 固定 ， 隐层 神经元 及 输出 层 神经元 均 采用 算法 调整 参数 ， 其中 的 设计 参数 α 、 η 、 γ 及 k 分别 取 不同 的 常 数值 ． 　 　 由 定理 的 （ ） 式 及 （ ） 式 我们 知道 ， 若 输入 层 神经元 为 线性 神经元 ， 则 学习策略 相当于 普通 多层 前向 网络 的 BP 算法 ， 学习策略 相当于 普通 多层 前向 网络 带动 量项 的 BP 算法 ， 学习策略 相当于 同时 修正 权 系数 和 激活 函数 的 BP 算法 ． 　 　 学习策略 自 适应 调整 步长 的 BP 学习策略 ： 输入 层 神经元 参数 固定 ， 隐层 神经元 及 输出 层 神经元 均 采用 算法 调整 参数 ， 其中 学习 步长 η 是 随 学习 样本 xtyt 变化 的 ， 时变 的 学习 步长 η t 由下式 给出 ： 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 其中 P 表示 网络 所有 隐层 神经元 及 输出 层 神经元 的 参数 组成 的 矢量 ， 即 　 　 　 　 　 　 J ′ PtP 表示 匹配 误差 JtPytt τ Λ ytt 关于 P 的 梯度 ， 从而 　 　 　 　 　 　 β 是 可调 参数 ， P 是 P 的 初始值 ， J 是 匹配 误差 的 初始值 ． 　 　 定理 样本 集为 xtytt … ． 若自 学习 BP 网络 Nnn … nMnM 采用 学习策略 更新 其 神经元 的 参数 ， 则 有 　 　 　 　 　 　 　 　 　 　 　 证明 由 学习策略 及 定理 有 从而 有 再 由 （ ） 式 及 β 知 结论 成立 ． 证毕 ． 　 　 学习策略 让 陡度 参数 参与 和 权 系数 类似 的 修正 ， 从而 可 避免 有些 神经元 的 输出 饱和 ， 加速 目标 函数 的 收敛 ． 学习策略 适应 学习 样本 xtyt 的 变化 ， 参数 更新 的 目的 要 使 新 参数 下 网络 与 样本 xtyt 的 匹配 误差 小于 旧 参数 下 网络 与 样本 xtyt 的 匹配 误差 （ 定理 ） ． 这 两种 学习策略 比 学习策略 及 学习策略 具有 快得多 的 收敛 速度 ， 但 也 有 各自 的 局限性 ． 学习策略 起步 时 学习 速率 很大 ， 但 当 误差 平方和 指标 小到 一定 数值 以后 ， 学习 速率 变小 ， 以至 指标 很难 收敛 到 很小 的 值 ． 学习策略 在 误差 平方和 指标 小到 一定 值 以后 收敛 很快 ， 但 学习 的 起步 阶段 较慢 ， 且 对权 系数 的 初始值 比较 敏感 ． 这些 可以 从 我们 的 仿真 结果 看出 ． 本文 将 把 这 两种 学习策略 结合 起来 ， 避开 各自 的 短处 ， 形成 一种 新 的 学习策略 — — 两 阶段 学习策略 ． 下面 先对 学习策略 进行 一点 小 的 修改 ， 得到 下面 的 学习策略 ． 　 　 学习策略 激活 函数 陡度 可调 的 BP 学习策略 ： 输入 层 神经元 采用 算法 调整 参数 ， 隐层 神经元 及 输出 层 神经元 均 采用 算法 调整 参数 ， 算法 及 算法 中 的 设计 参数 α 、 η 、 γ 及 k 均 取常 数值 ． 　 　 输入 层 神经元 的 作用 是 将 网络 的 输入 信号 变换 到 一个 合适 的 范围 后 再 传输 到 隐层 ． 神经元 的 活性 函数 若 为 对称 的 Sigmoid 函数 ， 则 可 把 输入 信号 变换 到 ， 若 为 线性 函数 ， 则 可 通过 调整 其 陡度 将 输入 信号 变换 到 所 需要 的 范围 ． 学习策略 允许 输入 层 神经元 的 激活 函数 陡度 和 隐层 神经元 及 输出 层 神经元 的 激活 函数 陡度 一起 参与 参数 的 修正 ， 这 将 进一步提高 学习 过程 的 收敛 速度 ． 　 　 学习策略 两 阶段 学习策略 ： 给定 样本 集 Ω xkykk … N ， 网络 的 整个 学习 过程 分 两个 阶段 ， 起步 时用 学习策略 进行 学习 ， 当 网络 对 样本 集 Ω 的 匹配 误差 　 　 　 　 　 　 　 　 　 　 　 小于 某一 数值 Jc 后 ， 改用 学习策略 进行 学习 ， 直到 J Ω 小于 某一 给定 值 J 为止 ． 　 　 可以 看到 ， 这里 给出 的 两 阶段 学习策略 集中 了 自 适应 调整 步长 的 BP 学习策略 和 激活 函数 陡度 可调 的 BP 学习策略 的 优点 ， 避开 了 它们 的 短处 ， 因而 将 有 比 这 两种 学习策略 更好 的 收敛 特性 ． 现设 样本 集 为 Ω xkykk … N ， 自 学习 BP 网络 为 Nnn … nMnM ， 则 采用 两 阶段 学习策略 对 网络 进行 学习 的 具体 过程 如下 ： 　 　 初始化 网络 ： 设定 各 神经元 的 活性 函数 的 类型 ， 给 各 神经元 的 参数 置 初值 ． 　 　 给定 NP （ 最大 学习 周期 ） 、 J 及 Jc ． 令 np ． 　 　 对 k … N ： 　 　 a 令 tnpNkxtxkytyk ； 　 　 b 进行 网络 的 正向 计算 ； 　 　 c 进行 网络 的 反向 计算 ； 　 　 d 用 自 适应 调整 步长 的 BP 学习策略 更新 神经元 参数 ． 　 　 计算 J Ω ． 若 J Ω Jc ， 则 转到 ； 否则 ， np ← np ， 转 到 ． 　 　 对 k … N ： a 令 tnpNkxtxkytyk ； 　 　 b 进行 网络 的 正向 计算 ； 　 　 c 进行 网络 的 反向 计算 ； 　 　 d 用 激活 函数 陡度 可调 的 BP 学习策略 更新 神经元 参数 ． 　 　 计算 J Ω ， np ← np ． 若 npNP 或 J Ω J ， 转 到 ； 否则 转 到 ． 　 　 结束 ． 　 实例 仿真 　 　 自 适应 调整 步长 的 BP 学习策略 相当于 中 给出 的 自 适应 调整 学习 步长 的 BP 算法 ， 激活 函数 陡度 可调 的 BP 学习策略 则 是 对 中 给出 的 同时 修正 权 系数 和 激活 函数 的 BP 算法 稍加 修改 而 来 ， 在 这 一节 ， 我们 分别 称 它们 为 现有 学习策略 A 及 现有 学习策略 B ． 　 　 考虑 一个 非线性 系统 的 辨识 问题 ． 设 非线性 系统 为 ykykykuky 其中 uk 是 均匀分布 于 ， 的 随机 输入 信号 ． 为了 仿真 ， 产生 组 数据 ukyk τ yk 作为 学习 样本 集 ． 采用 结构 为 N 的 自 学习 BP 网络 进行 建模 ． 　 　 网络 的 输入 层 神经元 及 输出 层 神经元 均 设为 线性 神经元 ， 隐层 神经元 设 为 对称 Sigmoid 型 ． 各 神经元 激活 函数 陡度 的 初值 均取 ， 权 系数 的 初始值 则 取 均匀分布 于 的 随机数 ． 在 同 一组 权 系数 初始值 下 分别 用 现有 学习策略 A 、 现有 学习策略 B 及 两 阶段 学习策略 对 样本 集 进行 学习 ． 现有 学习策略 A 中 β ； 现有 学习策略 B 中 所有 神经元 均取 α η γ k ； 两 阶段 学习策略 中取 Jc ． 通过 几十次 的 仿真 ， 结果表明 ： 现有 学习策略 A 起步 时 学习 速率 很大 ， 且 对权 系数 的 初始值 不 很 敏感 ， 但 当 J Ω 小到 一定 数值 以后 ， 学习 速率 变小 ， J Ω 很难 收敛 到 很小 的 值 ； 现有 学习策略 B 起步 阶段 收敛 较慢 ， 且 对权 系数 的 初始值 比较 敏感 ， 有时 甚至 不 收敛 ， 但是 J Ω 一旦 小到 一定 数值 ， 以后 就 收敛 很快 ； 两 阶段 学习策略 在 整个 学习 阶段 都 有 很快 的 收敛 速度 ， 对权 系数 的 初始值 不太 敏感 ． 两 阶段 学习策略 的 收敛 速度 比 现有 学习策略 A 及 现有 学习策略 B 的 收敛 速度 大 四倍 以上 ． 图 给出 了 一次 典型 的 仿真 结果 ， 图中 纵坐标 J Ω np 是 对数 刻度 的 ， 横坐标 np 则 主 刻度 （ n ） 为 对数 刻度 ， 次 刻度 为 均匀 刻度 ． 　 结论 　 　 本文 提出 的 自 学习 神经元 具有 正向 传播 信息 、 反向 传播 误差 及 学习 的 功能 ， 它 是 一个 独立 的 实体 ． 由 自 学习 神经元 可 方便 地 构成 自 学习 BP 网络 ， 这种 BP 网络 有 许多 普通 BP 网络 所 没有 的 优点 ． 本文 提出 的 两 阶段 学习策略 具有 收敛 速度 快 、 收敛 后 的 均 方 误差 指标 小 的 特点 ． 本文 的 结果 为 BP 网络 的 实现 及 BP 网络 的 实际 应用 打下 了 坚实 的 基础 ． 图 三种 学习策略 的 收敛 性能 比较 基金项目 国家自然科学基金 资助 项目 项目 批准 号 作者简介 　 　 赖晓平 ， 男 ， 硕士 ， 副教授 ． 研究 领域 为 系统 辨识 ， 数字 信号处理 ， 神经网络 理论 与 应用 ． 　 　 周鸿兴 ， 男 ， 教授 ， 博士生 导师 ． 研究 领域 为 分布 参数 系统 ， DEDS 理论 ， 神经网络 理论 ． 作者 单位 ： 赖晓平 山东大学 威海 分校 控制 工程系 威海 　 　 　 　 　 周鸿兴 山东大学 数学 与 系统 科学 学院 济南 参考文献 　 LVFausettFundamentalsofNeuralNetworksArchitecturesAlgorithmsandApplicationsPrenticeHallEnglewoodCliffs 　 SZhouDPopocivGSchulzEkloffAnImprovedLearningLawforBackpropagationNetworksInIEEEInternationalConferenceonNeuralNetworksSanFranciscoIEEEpress ～ 　 SZhouDPopocivGSchulzEkloffAMotivationFollowedLearningProceedingsofInternationalJointConferenceonNeuralNetworks ～ 　 李士勇 ． 模糊控制 神经 控制 和 智能 控制论 ． 哈尔滨工业大学 出版社 ， 收稿 日期