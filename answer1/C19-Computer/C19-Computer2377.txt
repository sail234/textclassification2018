计算机 研究 与 发展 JOURNALOFCOMPUTERRESEARCHANDDEVELOPMENT 年 第卷 第期 VolNo 神经网络 的 规则 提取 研究 黄源 　 萧嵘 　 张福炎 摘 　 要 　 文中 论述 了 作为 解决 神经网络 “ 黑箱 问题 ” 有效 手段 的 规则 提取 方法 ， 分析 了 基于 结构 分解 和 输入输出 映射 的 神经网络 规则 提取 的 各种 算法 ， 概括 了 它们 的 基本 思想 并 分析 了 它们 的 优劣 ， 在 相似 权值法 的 基础 上 提出 CSW 算法 ， 有效 解决 了 连续 值 输入 网络 的 规则 提取 问题 将 CSW 算法 应用 于 IRIS 分类 问题 取得 了 良好 的 效果 关键词 　 神经网络 ， 规则 提取 ， 结构 分解 法 ， 输入输出 映射 法 中图法 分类号 　 TPANAPPROACHTORULEEXTRACTIONOFNEURALNETWORKSHUANGYuan ， XIAORongandZHANGFuYanStateKeyLaboratoryforNovelSoftwareTechnologyNanjingUniversityNanjing 　 InstituteofMultimediaComputingTechnologyNanjingUniversityNanjing 　 Abstract 　 Inthispapertheruleextractionofneuralnetworksisdiscussedwhichisaneffectivemethodtoavoidtheshortcomingofbeing “ blackboxes ” TechniquesbasedondecompositionalandinputoutputmappingapproachesarestudiedandtheirfundamentalconceptsandevaluatestheirperformancesaregeneralizedBasedonsimilarweightapproachtheCSWapproachisproposedtoefficientlysolvetheruleextractionfromcontinuousinputneuralnetworksCSWisappliedinIRISFlowerClassificationProblemandexperimentresultsshowthatrulesextractedbyourmethodareaccurateandcomprehensibleKeywords 　 neuralnetworkruleextractiondecompositionalapproachinputoutputmappingapproach 　 　 神经网络 技术 近年来 在 各 领域 得到 广泛应用 ， 它 通过 对 训练 集 的 反复 学习 获取 知识 ， 具有 直观性 、 并行性 、 鲁棒性 和 抗噪性 ， 在 噪声 和 数据 不 完整 情况 下 能够 高质量 建模 和 基于 符号 的 传统 AI 技术 相比 ， 神经网络 技术 主要 缺点 是 获得 的 知识 隐含 在 网络 的 一系列 连接 和 权值 中 ， 处理过程 无法 为 用户 所 理解 ， 模型 对于 用户 来说 是 一个 黑箱 由于 缺乏 透明性 ， 在 数据挖掘 和 决策 支持 领域 ， 以及 在 安全 性能 要求 很 高 的 关键 应用 方面 ， 神经网络 技术 往往 被 认为 是 不 可靠 和 难于 理解 的 ， 应用 受到 一定 限制 因此 ， 有 必要 建立 一个 解释 机制 ， 用 规则 取代 权值 矩阵 ， 为 决策 支持 类 应用 提供 完整 的 决策 说明 ， 为 关键 应用 提供 结果 的 可信度 和 质量 检测 手段 ， 并 为 基于 符号 和 基于 连接 的 两种 AI 技术 提供 一种 有效 的 集成 方法 解释 机制 的 一种 解决方案 是 建立 神经网络 的 同时 建立 一个 简单 的 基于 规则 的 系统 ， 神经网络 做 基本 的 决策 操作 ， 将 其 输入 模式 和 最后 结论 给 基于 规则 的 系统 ， 用 反向 关联 来 构造 一个 联系 网络 输入 和 输出 的 推理 ， 这种 方法 要求 开发者 为 同一 问题 建立 两种 解决 方法 ， 开销 很大 ， 且 把 决策 过程 和 解释 过程 分裂 开来 ， 解释 的 内容 受限于 规则 库 的 结构 ， 缺乏 灵活性 ， 现在 已较 少 采用 另 一种 方法 就是 本文 所 讨论 的 规则 提取 方法 ， 即 对 训练 好 的 神经网络 及其 训练 示例 施加 一个 反向 工程 来 决定 它 做 决策 所 依据 的 特征 和 规则 ， 从而 产生 对此 网络 准确 的 符号 描述 本文 试图 对 规则 提取 的 各种 算法 作 一些 分析 和 比较 ， 并 在 此基础 上 针对 连续 值 输入 的 神经网络 给出 一个 新 的 规则 提取 算法 即 CSW 算法 ， 对 IRIS 问题 的 实验 结果 证明 这种 方法 是 简洁 而 有效 的 　 已有 的 规则 提取 方法 分析 　 　 可以 把 神经网络 规则 提取 方法 分为 两类 一类 是 基于 结构 分解 的 规则 提取 方法 ， 它 以 神经网络 隐藏 结点 和 输出 结点 为 单位 将 网络 分解 为 若干 单层 网络 的 集合 ， 对 每 一 子网 搜索 和 提取 规则 ， 最后 对 这些 规则 进行 组合 以 描述 整个 网络 的 特性 最 简单 的 一种 算法 是 Fu 提出 的 子集 法 ［ ］ ， 其 基本 思想 是 寻找 输入 连接 的 子集 使 其 权值 和 超过 输出 神经元 的 阈值 在 前馈 网络 中将 结点 的 输出 值 表示 为 outputfnet 且 net ω ixi θ ， 其中 θ 是 结点 的 阈值 如果 加权 输入 值 的 某个 组合 大于 神经元 结点 的 阈值 的话 ， 则 此 神经元 的 状态 是 激活 的 ， 否则 它 的 状态 是 未 被 激活 的 子集 法 假设 神经网络 中 每个 隐 结点 和 输出 结点 都 实现 一个 符号 规则 ， 与 结点 对应 的 概念 就是 规则 后 件 ， 而 向 该 结点 提供 输入 的 其它 结点 的 某个 子集 则 代表 规则 前 件 规则 提取 过程 就是 为 每个 后 件 寻找 使 其 成立 的 足够 条件 它 的 缺陷 是 当 网络 较 复杂 时 连接 子集 搜索 的 空间 就 会 变得 很大 ， 产生 组合 爆炸 问题 ， 其 算法 复杂度 为 On 为了 减少 搜索 开销 ， 可以 为 规则 条件 的 数量 设置 一个 上限 ， 即 限制 规则 搜索 的 深度 ， 在 单调 的 情况 下 假设 规则 搜索 的 深度 为 k ， 则 算法 复杂度 可 降低 为 这种 方法 缺点 是 可能 降低 规则 的 通用性 ， 因为 对于 某些 网络 来说 必须 搜索 到 足够 深度 才 能够 找到 合适 的 规则 子集 法 的 另 一个 缺点 是 即使 网络 规模 比较 小 也 会 抽取 出 大量 的 规则 且 表达 不够 清晰 ， 为此 Towell 提出 了 MOFN 方法 ［ ］ ， 该 方法 中 规则 的 表达形式 是 ： if 在 N 个 前提 中有 M 个 是 正确 的 then … 其 基本 思想 是 Towell 在 试验 中 发现 可以 把 规则 的 前提 分类 ， 使 一个 前提 类中 每个 前提 都 具有 同样 的 重要性 ， 并 可 与 类 中 的 其它 成员 互换 规则 还 可以 更 简洁 地 表示 成 ifNmore 　 thanPos 　 SetNeg 　 Setthen … ， 其中 Pos 　 Set 和 Neg 　 Set 是 正 条件 和 负 条件 的 集合 ， 如果 输入 满足 的 正 条件 数 减去 负 条件 数 大于 整数 N ， 则 结论 成立 和 子集 法 相比 ， MOFN 方法 产生 的 规则 集小 ～ 个 数量级 ， 算法 开销 小 ， 产生 的 规则 可读性 好 其 缺陷 是 所 针对 的 神经网络 最好 是 基于 知识 的 且 训练 过程 中 各 隐藏 结点 的 意义 基本 不 改变 ， 否则 将 降低 组合 后 规则 的 可 理解 性 由于 用 MOFN 方法 要求 权值 是 可 聚类 的 ， 对于 普通 的 神经网络 ， 需要 采用 “ 柔性 共享 权 ” ［ ］ 方法 训练 网络 ， 以 提高 网络 的 泛化 能力 ， 在 分类 误差 和 网络 复杂度 之间 做 一个 有效 的 均衡 ， 简化 网络 并 促进 权值 在 训练 过程 中 有效 聚类 　 　 基于 结构 分解 的 方法 将 网络 分成 若干个 单层 子网 ， 由于 需要 进行 搜索 和 规则 合并 ， 对于 复杂 网络 其 算法 复杂度 大大提高 且 规则 可 理解 性 很 差 在 这类 方法 中 ， 网络 剪枝 （ prune ） 十分 重要 RX 算法 ［ ］ 首先 用 权 衰减 （ weightdecay ） 方法 构造 BP 网络 （ 该 网络 中 连接 权 的 大小 反映 了 连接 的 重要 程度 ） ， 然后 对 网络 进行 修剪 ， 在 预测 精度 不变 的 情况 下 删除 次要 连接 ， 在 对 网络 进行 充分 简化 的 条件 下 ， 对 隐藏 层 结点 的 激活 值 进行 聚类 ， 根据 不同 的 隐藏 层 结点 激活 值用 穷举 搜索 的 办法 来 寻找 从 输入 层到 隐藏 层 和 从 隐藏 层到 输出 层 的 规则 我们 在 实验 中 发现 这种 算法 对于 部分 网络 的 规则 提取 效率 相当 高 ， 但 由于 采用 了 穷举 搜索 的 办法 ， 这种 算法 要求 剪枝 后 的 网络 非常 简化 ， 随着 网络 复杂度 的 增长 其 算法 复杂度 呈 几何级数 增长 　 　 另一类 方法 是 基于 输入输出 映射 的 规则 提取 算法 ， 这 类 方法 和 基于 结构 分解 的 方法 不同之处 在于 它 忽略 了 神经网络 的 隐藏 层 内部结构 ， 直接 在 输入输出 结点 之间 寻找 对应 ， 提取 相应 的 规则 较 有 代表性 的 算法 是 Sestito 等 人 提出 的 相似 权值法 ［ ］ ， 这种 方法 将 输出 节点 添加 到 输入 层去 与 输入 节点 进行 比较 ， 在下文 介绍 CSW 算法 时 还 会 对 它 进行 较 详细分析 Craven 和 Shavlik 提出 了 用 学习 的 方法 提取 规则 ［ ］ ， 它 将 规则 抽取 视为 一个 学习 任务 ， 首先 用 Oracle 调用 EXAMPLES 产生 一个 示例 ， 用 训练 好 的 神经网络 对此 示例 进行 分类 ， 判断 这一 示例 是否 已 被 规则 集 覆盖 ， 如果 没有 则 用 这个 示例 初始化 一个 规则 ， 依次 从 规则 前件 中 删除 一个 条件 ， 再用 Oracle 调用 SUBSET 来 判断 该 规则 是否 与 网络 保持一致 ， 如果 规则 仍旧 能够 覆盖 所有 的 示例 ， 则 说明 这个 条件 是 可 被 删除 的 ， 否则 说明 此 条件 是 不可 删除 的 ， 将 它 重新 添加 到 规则 前件 中 去 ， 重复 上述 过程 直到 规则 前提 达到 最 简并 将 此 规则 加入 到 规则 集中 这种 方法 完全 是从 底向上 的 ， 在 实行 中 不断 修改 规则 ， 用 规则 集 和 神经网络 分类 结果 的 比较 来 确定 最后 的 规则 集 是否 达到 要求 和 自顶向下 方法 相比 ， 如果 规则 集中 有 大量 复杂 规则 可 有效 防止 自顶向下 的 方法 搜索 过程 中 规则 前提 的 组合 爆炸 问题 ， 但 对于 规则 集中 较为 普遍 的 规则 它 的 计算 时间 有所增加 基于 连续 值 输入 的 CSW 算法 　 　 对于 一个 典型 的 三层 BP 网络 ， 假设 网络 输入 层有 m 个 神经元 xx … xm 隐藏 层有 h 个 神经元 yy … yh ， 输出 层有 n 个 神经元 zz … zn ， θ j 为 神经元 yj 的 阈值 ， ω ij 是 神经元 xi 到 神经元 yj 的 连接 权值 ， β jk 是 神经元 yj 到 神经元 zk 的 连接 权值 ， 则 网络 可以 表示 成为 Fm → nFx … ， xmz … ， znzkgkyj β jk 且 yjfjxi ω ij θ j 　 　 为了 比较 输入 和 输出 的 关系 ， 相似 权值法 把 输出 节点 添加 到 输入 层去 ， 这样 输入 层 就 有 mn 个 节点 xx … xmn ， 由于 此时 输入 节点 和 输出 节点 是 在 同 一层 上 ， 所以 就 较易 得出 某一 输出 和 哪些 输入 有关 对 改变 了 结构 的 网络 进行 重新 训练 ， 如果 原来 的 输入 节点 和 新 加入 的 输入 节点 权值 相似 ， 就 可以 认为 这 一对 输入输出 之间 存在 关联 使用 误差 平方和 SSE 来 判断 节点 之间 的 权值 相似 关系 ， 对于 输入 神经元 a 和 输出 神经元 b 现已 加入 输入 层 ） ， 可以 定义 两 神经元 的 平方 误差 为 SSEab ω bj ω aj 　 　 SSE 表示 输入 a 与 输出 b 之间 接近 程度 （ closeness ） ， SSE 值越 小 就 说明 输入 a 对 输出 b 的 贡献 越大 如果 训练 集较 小 或 输入输出 之间 的 关系 比较 分散 ， 则 仅凭 SSE 还 不能 确定 一对 输入输出 之间 的 关联 ， 还 必须 用 Hebb 规则 确定 它们 之间 的 抑制性 连接 权 ， 即 通过 对 训练 集 的 负集 （ 将 原 训练 模式 对 各 属性 值 取反 ） 的 训练 ， 确定 哪些 输入 和 输出 之间 不 可能 存在 关联 在 负集 训练 时 一般 省去 隐藏 层 ， 输入 神经元 和 输出 神经元 之间 的 连接 权值 称为 无关 权值 ， 它 可 作为 输入 与 输出 间 无关 性 （ irrelevance ） 的 度量 ， 值越 小则 说明 某 输入 与 输出 的 关系 越 密切 可以 用 两 神经元 的 无关 权值 和 误差 平方和 的 乘积 来 判断 它们 之间 的 相似 关系 ： ProductabWeightab × SSEabProductab 的 值 接近 于 ， 则 说明 ab 这 一对 输入输出 之间 存在 关联 相似 权值法 应用 于 动物 识别 和 LED 数字信号 领域 都 产生 了 清晰 、 易懂 且 一致性 强 的 规则 和 大多数 规则 提取 算法 一样 相似 权值法 只能 应用 于二值 输入 的 神经网络 ， 在 实际 应用领域 中 ， 连续 属性 不可避免 地 经常出现 ， 并且 在 很多 任务 中 都 具有 重要 作用 ， 而 神经网络 相对 于 符号 学习 机制 的 一个 重要 优势 就 在于 神经网络 可以 很 好 地 处理 连续 属性 输入 ， 从 处理 连续 属性 的 神经网络 中 抽取 规则 有 很 强 的 实践 意义 ， 对于 连续 值 输入 的 神经网络 ， Sestito 提出 对 相似 权值法 的 改进 方法 ［ ］ ， 采用 的 规则 形式 为 RiIf （ （ vmini ≤ ai ≤ vmaxi ∧ … ∧ （ vminini ≤ aini ≤ vmaxini 　 then 　 bi 　 　 其中 vminik 和 vmaxik 分别 是 训练 集中 符合 规则 Ri 前件 结构 和 结论 的 所有 示例 中 aik 的 最小值 和 最大值 可以 看出 这种 通过 训练 集来 确定 规则 前 件 范围 的 方法 抗噪 能力 较弱 ， 训练 集中 存在 若干个 异常 示例 就 会 大大降低 所 产生 规则 的 精度 　 　 对此 我们 提出 了 CSWcontinuoussimilarweight 方法 CSW 方法 首先 对 所有 的 连续 值 输入 离散 化 ， 然后 将 连续 值 输入 转换成 二值 输入 的 网络 ， 应用 相似 权值法 获得 相应 规则 ， 再 将 规则 的 前件 转换 为 输入 区间 在 连续 值 输入 离散 化 过程 中 ， 我们 采用 χ 统计 意义 的 判断 方法 对 相邻 区间 进行 冲突 分析 对于 一个 示例 集 来说 ， 设 A 是 待 离散 化 变量 ， l 是 A 的 离散 化 区间 数 ， C 是 示例 的 分类 ， k 是 分类 数 ， Aij 是 A 取值 在 第 i 个 离散 化 区间 中时 第 j 个 分类 的 示例 数 ， m ∈ ［ l ］ ， 对 区间 m 和 m 进行 冲突 分析 ， 设 RiAij 和 CjAij 分别 为 是 第 i 个 区间 和 第 j 个 分类 的 示例 总数 ， Aij 的 分布 ， 则 和 一般 的 离散 化 方法 要 获得 尽可能少 的 离散 化 区间 不同 ， 在 CSW 方法 中 为了 保证 连续 数据 的 区分 能力 和 产生 规则 的 精度 ， 往往 需要 较 多 的 离散 化 区间 以 经典 的 IRIS 植物 识别 问题 为例 ， 合并 从 一个 预先 设定 的 重要性 度量 sigLevel 开始 ， 初始化 将 变量 A 在 示例 集中 的 所有 可能 取值 作为 一个 独立 的 离散 区间 ， 合并 所有 χ 小于 sigLevel 的 相邻 区间 ， 并 逐渐 减小 sigLevel 直到 离散 数据 的 不 一致 率 大于 指定 常数 δ 为止 为了 减少 精度 损失 ， 保证 学习 的 效果 ， 我们 取 δ ， sigLevel ， χ 四个 输入 的 分类 区间 数 分别 为 ， ， ， ， 分类 区间 如表 所示 表 SepallengthSepalwidth — 　 — Petallength — Petalwidth — — — — 　 　 离散 化 的 区间 共有 个 ， 应用 相似 权值法 将 输出 节点 添加 到 输入 层 ， 可得 的 神经网络 对 改变 了 结构 的 网络 进行 重新 训练 ， 根据 权值 计算 SSE 如表 所示 ， 利用 两 神经元 的 无关 权值 和 误差 平方和 的 乘积 Productab 来 判断 输入 和 输出 之间 的 相似 关系 ： ProductabWeightab × SSEab 在 CSW 方法 中 我们 还 对 原有 的 相似 权值法 做 了 一点 改进 ， 原有 规则 的 前件 中 缺少 notA 、 notB 这样 的 前提 ， 我们 对于 Product 值 很大 输入 结点 （ 如 分类 的 Inputnode ） 取反 作为 否定 前提 加入 规则 前 件 ， 当然 如果 同一 连续 属性 的 其它 离散 区间 中 已有 结点 作为 相似 结点 加入 规则 前 件 ， 由于 同一 属性 取值 的 唯一性 ， 就 没有 必要 加入 否定 前提 另外 ， 原有 的 相似 权值法 前提 间 只 考虑 “ 与 ” 关系 ， 产生 的 规则 虽然 分类 精度 很 高 但 覆盖 的 示例 很少 ， 我们 对于 相似 结点 作为 单 前提 规则 进行 分类 精度 分析 ， 如果 精度 超过 阈值 则 以 “ 或 ” 关系 加入 规则 前 件 经过 简化 产生 的 规则 为表 SSEOutputnodeOutputnodeOutputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnodeInputnode 　 　 规则 If ≤ petallengththenirissetosa ； 　 　 规则 If ≤ petallengthAND ≤ petalwidththenirisversicolor 　 　 规则 Ifsepallength ≥ ORpetallength ≥ ORpetalwidth ≥ thenirisvirginica 　 　 产生 规则 集 的 分类 精度 与 其他 分类 算法 的 比较 如表 所示 ， CSW 从 神经网络 中 提取 的 规则 不仅 容易 理解 ， 而且 有 和 其他 分类 算法 相当 的 预测 精度 图 给出 了 存在 噪声 数据 情况 下 CSW 算法 和 Sestito 对 SW 的 改进 算法 及 ID 算法 的 分类 精度 的 比较 ， 可以 看出 CSW 算法 有 较 强 的 抗噪 能力 ， 这 是 由 CSW 离散 化 过程 的 容错性 和 神经网络 算法 本身 的 抗噪性 决定 的 表 　 IRIS 分类 问题 测试 结果 比较 算法 SetosaViginicaVersicolor 平均 预测 精度 CSWGVSIVSMNTgrowthDasarathyC 图 　 有 噪声 情况 下 IRIS 分类 问题 结果 比较 　 结束语 　 　 神经网络 的 “ 黑箱 问题 ” 影响 了 神经网络 在 数据挖掘 、 决策 支持 等 关键 领域 的 广泛应用 ， 规则 提取 方法 则 是 解决 “ 黑箱 问题 ” 的 有效 手段 对于 一般 的 网络 ， 结构 分解 法 的 算法 开销 小 一些 ， 它 每次 只 考虑 一个 单层 网络 ， 由于 输入输出 之间 的 关系 是 单调 的 ， 在 规则 前提 中 不必 考虑 否定 条件 但 由于 还 存在 一个 多级 规则 合并 的 问题 ， 算法 受 网络 复杂度 的 限制 ， 当 网络 的 隐藏 层较 多时 提取 规则 的 可读性 将会 变得 很差 ， 对于 这 类 算法 ， 规则 提取 前 的 网络 剪枝 、 删除 冗余 结点 等 预处理 工作 就 十分 重要 ； 输入输出 映射 法不受 网络 规模 的 限制 ， 但 由于 算法 开销 大 ， 必须 大大 缩小 规则 搜索 的 解 空间 ， Craven 方法 一般 适合 于 训练 集较 小 的 情况 相似 权值法 把 输入 和 输出 放在 同 一层 上 进行 比较 ， 有效 避免 了 两类 方法 的 缺陷 ， 本文 提出 的 CSW 方法 将 相似 权值法 扩展 到 连续 值 输入 网络 实验 结果表明 ， 该 方法 具有 较 好 的 分类 精度 、 抗噪性 和 可 理解 性 规则 提取 方面 还有 很多 工作 要 做 ， 如 降低 算法 的 复杂度 ， 提高 所 提取 规则 的 可 理解 性及 算法 的 适用性 ， 研究 提取 的 规则 集 的 评估 标准 和 在 训练 中 从 神经网络 动态 提取 规则 以及 时 修正 神经网络 并 提高 神经网络 性能 等 都 是 进一步 的 研究 方向 作者简介 ： 黄源 ， 男 ， 年月生 ， 博士 研究生 ， 研究 方向 为 数据挖掘 和 神经网络 萧嵘 ， 男 ， 年月生 ， 博士 研究生 ， 研究 方向 为 数据挖掘 和 神经网络 张福炎 ， 男 ， 年月生 ， 博士生 导师 ， 主要 研究 领域 为 多媒体技术 、 计算机 图形学 等 作者 单位 ： 南京大学 计算机软件 新 技术 国家 重点 实验室 　 南京 　 南京大学 多媒体计算机 研究所 　 南京 　 参考文献 　 　 FuLMRulegenerationfromneuralnetworksIEEETransonSysManandCybernetics ～ 　 　 TowellGShavlikJTheextractionofrefinedrulesfromknowledgebasedneuralnetworksMachineLearning ～ 　 　 NowlanSJHintonGESimplifyingneuralnetworksbysoftweightsharingNeuralComputation ～ 　 　 RudySetionoLiuHUnderstandingneuralnetworksviaruleextractionInProcofthethInternationalJointConferenceonArtificialIntelligenceMontreal ～ 　 　 SestitoSDillonTKnowledgeacquisitionofconjunctiverulesusingmultilayeredneuralnetworksInternationalJournalofIntellSys ～ 　 　 CravenMWShavlikJWUsingsamplingandqueriestoextractrulesfromtrainedneuralnetworksInProcofthethIntlConfonMathineLearningNewBrunswick ～ 　 　 SestitoSDillonTKnowledgeacquisitionofconjunctiveruleswithcontinuouslyvaluedattributesInProcofthIntlConfonArtificialIntelligenceAvignonFrance ～ 　 　 黄源 ， 张福炎 基于 神经网络 的 数据挖掘 工具 的 研究 清华大学 学报 ， S ～ HuangYuanZhangFuyanResearchofdataminingtoolsbasedonneuralnetworkapproachJournalofTsinghuaUniversity ～ 原稿 收到 日期 ： ； 修改稿 收到 日期 ：