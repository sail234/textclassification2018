自动化 学报 ACTAAUTOMATICASINICA 年 　 第卷 　 第期 　 VolNo 广义 LVQ 神经网络 的 性能 分析 及其 改进 张志华 　 郑 南宁 　 王天树 摘 　 要 　 首先 从 理论 上 分析 了 广义 学习 矢量 量化 GLVQ 网络 的 GLVQF 算法 的 性能 ， GLVQF 算法 在 一定 程度 上 克服 了 GLVQ 算法 存在 的 问题 然而 ， 它 对 获胜 表现型 的 学习 具有 好 的 性能 ， 对于 其它 的 表现型 ， 性能 却 十分 不 稳定 分析 了 产生 这个 问题 的 原因 ， 直接 从 表现型 的 学习 率 出发 ， 提出 了 选取 学习 率 的 准则 ， 并 给出 了 两种 改进 的 算法 最后 ， 使用 IRIS 数据 验证 了 算法 的 性能 ， 改进 算法 较之 GLVQF 算法 具有 明显 的 稳定性 和 有效性 关键词 　 亏损 因子 ， 模糊 度 因子 ， 学习 率 ， IRIS 数据 BEHAVIORALANALYSISANDIMPROVINGOFGENERALIZEDLVQNEURALNETWORKZHANGZhihua 　 ZHENGNanning 　 WANGTianshuInstituteofAIandRoboticsXianJiaotongUniversityXian 　 Abstract 　 　 InthispapertheperformanceofGLVQFalgorithmofGLVQnetworkistheoreticallyanalyzedTheGLVQFalgorithmtosomeextenthasovercometheshortcomingsthatGLVQalgorithmpossessesButtherearesomeproblemsinGLVQFalgorithmforexamplethealgorithmhasgoodperformanceonthewinningprototypeandonotherprototypesitsperformanceisveryunstableInthispaperthereasonsoftheproblemarediscussedTherulesofchoosingthelearningratesareproposedandtwomodifiedalgorithmsaredevelopedtherefromFinallytheperformanceofthemodifiedalgorithmsisverifiedwithIRISdatawhichshowsthemodifiedalgorithmsaremorestableandeffectivethanGLVQFalgorithmKeywords 　 LossfactorfuzzydegreefactorlearningrateIRISdata 　 引 　 言 　 　 近几年 ， 由于 理论 上 和 网络结构 上 表现 出 广泛 的 活力 ， Kohonen 关于 聚类 算法 ［ ］ 的 研究 工作 变得 十分 流行 Kohonen 的 工作 ［ ］ 主要 集中 在 两 方面 ： 一是 学习 矢量 量化 LVQ 算法 ， 二是 他 的 自 组织 特征 映射 SOFM 网络 但是 LVQ 存在 神经元 未 被 充分利用 以及 输入 样本 和 竞争 单元 之间 的 信息 被 浪费 等 两个 主要 问题 ［ ］ SOFM 同样 也 存在 两大 缺点 ［ ］ ： 一是 哪些 节点 应 被 考虑 ； 二是 每个 非 获胜 节点 应 怎样 发挥 影响 Huntsberger 和 Hjjimarangsee ［ ］ 首次 把 SOFM 同 LVQ 相结合 ， 为 试图 克服 两者 上述 各自 的 问题 提出 了 一种 新 的 思路 Pal 等 ［ ］ 在 这个 思想 上 提出 了 一种 广义 的 学习 矢量 量化 GLVQ 网络 Gonzalez 等 ［ ］ 对 GLVQ 进行 了 性能 分析 ， 发现 它 仍然 存在 一些 问题 Karayiannis 等 ［ ］ 修正 了 GLVQ ， 提出 了 GLVQF 算法 GLVQF 算法 在 一定 程度 上 解决 了 GLVQ 算法 存在 的 问题 但 同时 GLVQF 算法 对于 不同 的 模糊 度 因子 聚类 性能 不 稳定 本文 从 数学 上 分析 了 GLVQF 算法 的 性能 ， 并 提出 了 两种 改进 的 算法 　 广义 学习 矢量 量化 GLVQ 网络 和 GLVQF 算法 　 　 记 X ｛ xx … xp ｝ Rn 为 输入 样本 集合 ， 设 X 中 的 类数 表现型 的 个数 为 cGLVQ 网络 的 学习 规则 是从 最优化 一个 目标 函数 导出 的 该 目标 函数 定义 为 输入 样本 x 的 一个 亏损 函数 Lx 　 其中 表现型 ν ｛ vv … vc ｝ Rn 是 我们 要 寻找 的 矢量 量化 器 ， gr 是 x 相对 表现型 的 亏损 因子 ， 其 不同 的 定义 就 会 导出 不同 的 学习 算法 在 GLVQ 算法 中 ， 由下式 定义 ： 　 　 　 由于 GLVQ 算法 定义 gr 为式 ， 在 实际 应用 中 ， 使得 算法 背离 了 初衷 ［ ］ 为此 ， 文献 ［ ］ 引入 模糊 C 均值 中 的 隶属 度 公式 ［ ］ 来 定义 gr ， 即 　 其中 m ∈ （ ， 是 一个 常数 ， 表示 模糊 度 因子 此时 ， 用 梯度 下降 法 求解 目标 函数 Lx 的 最小值 ， 导出 了 GLVQF 算法 ， 对于 输入 x ， 第 t 次 的 学习 规则 是 vjtvjt α thjmxvtj … c 　 这里 把 hjm 称为 学习 率 hjmgjFjmj … c 　 　 　 　 GLVQF 算法 的 性能 分析 　 　 为了 方便 起 见 ， 令 vi 是 相对 于 输入 向量 x 的 获胜 表现型 ， 即 ， vl 是 相对 于 输入 向量 x 的 竞争 最小 的 表现型 ， 即 本文 其余部分 都 作 这样 假设 现在 分析 GLVQF 的 性能 首先 　 　 因为 δ ir ≤ δ ir ≥ r … ， c 所以 Fim ≥ 于是 有 giFim ≥ gi 又 δ lk ≥ δ lk ≤ Flm ≤ 所以 glFlm ≤ gl 　 　 可以 看出 Fkm 是 个 扰动 参数 （ k … ， c ， 对于 获胜 节点 ， 它 加大 了 学习 步长 ， 而 对 竞争 最小 节点 ， 它 减弱 了 学习 步长 进一步 比较 himhkmk … ， c ； k ≠ i 的 大小 首先 由式 经过 简单 计算 ， 有 　 　 　 定理 ） hkm ≤ himk … ck ≠ i ； Fkm ≤ Fimk … ck ≠ i 　 　 从 直观 上 看 ， 作者 希望 当 ‖ xvk ‖ ≥ ‖ xvj ‖ 时 ， 算法 也 能 满足 hkm ≤ hjm 然而 ， 遗憾 的 是 该 结论 即使 对于 竞争 最小 的 表现型 也 不能 成立 举例说明 　 　 例设 c 令 ‖ xv ‖ ‖ xv ‖ ‖ xv ‖ 经过 简单 计算 有 显然 ‖ xv ‖ ＜ ‖ xv ‖ ， 而 由此 例 还 可见 hkm 不 一定 大于 零 k ≠ i 但 当 m ≥ 时 ， 有 下述 结论 成立 ： 　 　 定理 设 m ≥ ， 如果 ‖ xvk ‖ ≥ ‖ xvj ‖ ， 则 ＜ hkm ≤ hjm 　 　 证明 　 因为 m ≥ ∴ 因而 当 ‖ xvk ‖ ≥ ‖ xvj ‖ ， 有 又 所以 hkmhjm ≤ 于是 定理 得证 　 　 从 另 一个 角度 来 分析 GLVQF 算法 ， 即 固定 δ kr ， 把 hkmgkFkm 当作 m 的 函数 进行 分析 首先 给出 一个 引理 　 　 引理 ［ ］ 　 　 这里 给出 hkmgkk … ， c 关于 m 的 导数 ： 　 　 　 　 根据上述 两 式 、 引理 和 文献 ［ ］ ， 很 容易 得到 下面 几个 定理 ： 　 　 定理 gi 为 m 在 区间 ∞ 的 递减 函数 ， 并且 　 　 定理 gl 为 m 在 区间 ∞ 的 递增 函数 ， 并且 　 　 定理 him 为 m 在 区间 ∞ 的 递减 函数 ， 并且 himhim 　 　 定理 表明 him 有 类似 定理 的 结果 ， 但 遗憾 的 是 本文 不能 证明 hlm 有 类似 定理 的 结论 成立 作者 的 目标 是 希望 找到 满足 下述 两个 条件 的 hkm ： 　 　 当 him 为 m 的 递减 函数 ， 且 himhim ； 　 　 当 k ≠ ihkm 为 m 的 递增 函数 ， 且 hkmhkm 这样 就 可以 采用 类似 于 文献 ［ ］ 中 的 方法 ， 随着 学习 次数 的 增加 ， m 由大 逐渐 减小 ， 即 开始 时 ， 所有 表现型 的 学习 步长 都 均等 ， 而 最后 ， 只有 获胜 表现型 激活 ， 其它 表现型 逐渐 抑制 从 直观 上 看 ， 这是 合理 的 　 GLVQF 的 改进 算法 　 　 作者 的 目的 是 构造 满足 上述 条件 的 hkmk … ， c 文献 ［ ］ hkm 的 构造 是从 gk 着手 的 其实 在 学习 时 ， 只 需要 hkm 因而 可 直接 从 hkm 着手 ， 而 不 考虑 gk 　 　 hkm 是 gk 加上 了 一个 扰动 项 Fkm ， 对于 获胜 表现型 它 增大 了 其 激活 程度 ； 而 对于 竞争 最小 的 表现型 它 增大 了 其 抑制 程度 ； 但 对 其它 的 表现型 ， Fkm 这个 扰动 量 有时 会 产生 很 差人意 的 效果 （ 正如 上节 的 例子 所 分析 的 ） 略去 Fkm ， 直接 令 hkmgk 　 对于 获胜 表现型 和 竞争 最小 的 表现型 来说 ， 降低 了 它们 的 激活 或 抑制 程度 ， 但 对于 其它 表现型 ， 稳定 了 其 效果 ， 故 整个 算法 相对而言 要 稳定 些 由 定理 和 定理 ， 可知 hkm 满足 上面 的 条件 ， 但 条件 只是 对 竞争 最小 的 表现型 成立 我们 把 基于 式 导出 的 算法 简称 改进 算法 　 　 下面 讨论 另 一种 改进 （ 简称 改进 算法 ） 令 　 由 上式 首先 可 得 　 　 hkm ＜ gkk … c 因为 所以 　 所以 由式 可得 ＜ hkm ＜ gk ＜ gihimk … ， c ； k ≠ i 再 根据 式 ， 显然 有 hkmhkmk … ， c ； k ≠ i 　 　 又 由于 k … ck ≠ i 　 　 　 　 所以 　 hkmk … ck ≠ i 是 m 在 区间 ∞ 上 的 递增 函数 于是 两个 条件 满足 　 数值 实验 及 结果 分析 　 　 这里 用 著名 的 IRIS 数据 ［ ］ 来 验证 上述 改进 算法 的 性能 IRIS 数据 经常 被 用来 检验 聚类 无 监督 或 分类 有 监督 模式识别 方法 的 性能 ， 它 总共 有个 数据 ， 每个 数据 又 由个 分量 组成 IRIS 数据 分为 类 ， 每类 有个 数据 对 有 监督 设计标准 的 错误 个数 为 — ， 而 无 监督 设计标准 的 个数 在 左右 这里 作者 用 GLVQF 算法 ， 改进 算法 ， 分别 对 IRIS 数据 进行 聚类 为了 更好 地 比较 这 三个 算法 的 性能 ， 分别 取 不同 的 m 值 进行 实验 ， 其 结果 见 表表 给出 了 表现型 的 初始值 ， 实验 都 是 根据 这个 初始值 进行 的 为了 有 一个 比较 准则 ， 用 NP 最近 邻 算法 直接 对 样本 进行 聚类 ， 结果 也 见 表表 　 数值 实验 中 表现型 的 初试 值及 最近 邻 算法 实验 结果 初始 质心 最后 质心 混淆 矩阵 错误率 　 　 在 表中 每 一行 上部 代表 聚类 的 混淆 Confusion 矩阵 ， 下部 表示 聚类 的 错误率 从表 可以 看到 ， GLVQF 算法 在 m ＞ 时 ， 聚类 的 误差 很大 ， 在 m 值 趋近 时 算法 溢出 ， 以至于 不能 进行 聚类 改进 算法 比 GLVQF 算法 聚类 效果 有 一定 的 提高 ， 但 相应 的 问题 同样 存在 其 原因 正如 第三节 所 分析 的 ， 是 由于 式 定义 的 学习 率 hjmj … c 随着 m 取 不同 的 值 ， 其 性能 不 稳定 ， 从而 严重破坏 了 算法 的 聚类 效果 改进 算法 完全 满足 两个 条件 ， 该 算法 在 m 以及 m 时聚类 的 错误 个数 为 ， 低于 ， 而 此时 GLVQF 算法 和 改进 算法 的 错误 数都 很 高当 m 的 值 趋近 时 ， 比如 m ， 其它 两种 算法 已经 无法 进行 聚类 ， 而 改进 算法 仍然 能 有效 地 对 数据 样本 进行 聚类 ， 这 表明 该 算法 的 聚类 性能 是 十分 稳定 ， 所以 它 是 十分 有效 的 表 　 三种 学习 算法 实验 结果 比较 mGLVQF 算法 溢 　 出 无 　 法聚 　 类 改进 算法 溢 　 出 无 　 法聚 　 类 改进 算法 　 　 该 实验 的 结果 与 前文 的 理论 分析 是 吻合 的 它 进一步 验证 了 基于 两个 条件 选取 学习 率 hkmk … ， c 而 导出 的 学习 算法 是 有效 的 　 结 　 论 　 　 GLVQ 算法 是从 最优化 一个 目标 函数 而 导出 的 ， 该 算法 构造 新颖 ， 为 克服 学习 矢量 量化 算法 存在 的 问题 提供 了 一个 新 的 思路 Karayiannis 等 通过 引入 模糊 c 均值 的 隶属 度 函数 ， 修正 了 GLVQ 算法 ， 提出 一类 新 的 学习 算法 GLVQF 本文 的 工作 为 GLVQF 算法 提供 一定 的 数学 理论 基础 完善 学习 矢量 量化 算法 的 理论 ， 以及 基于 这些 理论 设计 出 更 高效 的 聚类 算法 并 把 算法 应用 于 图象处理 中将 是 作者 进一步 研究 的 课题 国家自然科学基金 重点 资助 项目 No 作者简介 ： 张志华 　 讲师 ， 现在 西安交通大学 攻读 博士学位 主要 从事 模式识别 ， 图象处理 ， 模糊 逻辑系统 及 神经计算 智能 等 领域 的 研究 　 　 　 　 　 郑 南宁 　 博士 ， 博士生 导师 主要 从事 模式识别 ， 图象处理 及 计算机 视觉 等 领域 的 理论 与 应用 研究 ， 现已 在 国内外 重要 学术期刊 及 会议 上 发表 论文 一 百余篇 作者 单位 ： 西安交通大学 人工智能 与 机器人 研究所 　 西安 　 参考文献 　 BezdekJPatternRecognitionwithFuzzyObjectiveFunctionAlgorithmsNewYorkPelnum 　 KohonenTSelforganizationmapsBerlinSpringerVerlag 　 ChungFLLeeTFuzzycompetitivelearningNeuralNetworks 　 TsaoECKBezdekJCPalNRFuzzyKohonenclusteringnetworksPatternRecognition 　 HuntsbergerTAjjimarangseePParallelselforganizingfeaturemapsforunsupervisedpatternrecognitionIntJGeneralSystems 　 PalNRBezdekJCTsaoECKGeneralizedclusteringnetworksandKohonensselforganizingschemeIEEETransonNeuralNetworks 　 GonzalezAZGranaMAnjarADAnanalysisoftheGLVQalgorithmIEEETransonNeuralNetworks 　 KarayiannisNBBezdekJCPalNRHatharayRJRepairtoGLVQanewfamilyofcompetitivelearningschemesIEEETransonNeuralNetworks 　 BezdekJCPalNRTwosoftrelativesoflearningvectorquantizationNeuralNetworks 收稿 日期 ： 修稿 日期 ：