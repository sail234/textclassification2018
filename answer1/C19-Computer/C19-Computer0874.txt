自动化 学报 ACTAAUTOMATICASINICA 年 第卷 第期 VolNo 对 多层 前向 神经网络 研究 的 几点 看法 阎 平凡 摘 　 要 　 从 不同 的 领域 对 多层 前向 网络 的 作用 本质 作 了 分析 ， 对 泛化 能力 、 模型 选择 、 有限 样本量 等 主要 问题 做 了 定性 讨论 ； 对 当前 前向 网络 研究 中 的 一些 问题 提出 了 看法 关键词 　 神经网络 ， 多层 感知器 ， 统计 建模 SOMEVIEWSONTHERESEARCHOFMULTILAYERFEEDFORWARDNEURALNETWORKSYANPINGFANDeptofAutomationTsinghuaUniversityBeijingAbstract 　 TheessentialfunctionofmultilayeredfeedforwardnerworkwasanalysedfromthepointofviewofvariousdisciplineGeneralizationabilitymodelselectionandtheproblemoflimitedsamplesizewerediscussedqualitativelySomeproblemsintheresearchofmultilayerfeedforwardnetworkarepointedoutKeyword 　 Neuralnetworkmultilayerperceptionstatisticalmodeling 　 引言 　 　 文献 ［ ］ 对 当前 国内 人工神经网络 ANN 研究 的 现状 作 了 分析 ， 作者 对 该文 指出 的 一些 问题 也 有 同感 有关 联想 记忆 网络 ， 文 ［ ］ 已 发表 了 一些 很 好 的 看法 ， 这里 着重 讨论 关于 多层 前向 网络 简记 为 MFN 研究 中 的 问题 国内 在 MFN 方面 的 研究 工作 很多 ， 其中 不少 是 很 好 的 ， 但 也 存在 一些 问题 ， 一是 有些 理论 研究 不够 深入 ， 甚至 没有 抓住 关键问题 ； 二是 有些 应用 没有 说 清楚 为什么 ANN 是 较 合理 的 选择 ， 使人 感到 其 应用 有 一定 盲目性 为此 ， 提出 以下 应 注意 的 问题 　 　 从 多方面 深入 理解 MFN 作用 的 本质 ； 　 　 对 MFN 的 主要 要求 是 什么 ； 　 　 用 好 MFN 的 关键问题 及 主要 困难 ； 　 　 对 一些 笼统 的 提法 或 似是而非 的 问题 ， 应 加以分析 　 对 MFN 作用 的 理解 　 　 要 理解 MFN 的 作用 不能 只 局限于 对 ANN 的 研究 ， 应 搞清楚 其 与 相关 领域 的 关系 和 差异 　 　 　 函数 逼近 　 　 用 一些 简单 函数 的 复合 去 逼近 一个 未知 函数 f 工程 上 常用 的 有 两种 类型 一是 谱 方法 ， 二是 有限元法 谱 方法 把 f 分解成 不同 频率 的 谐波 成份 ， 基 函数 的 正交 性 显而易见 这种 分解 在 频域 的 分辨率 几乎 是 无限 的 单频 的 谐波 在 频域 是 一个 δ 函数 ， 有利于 频域 分析 但 各基 函数 在 时 空域 是 全局性 的 ， 时 空域 分辨率 很 低 有限元法 则 是 在 空 时域 把 f 分成 一些 小 区域 ， 每一 小区 可用 简单 的 函数 表示 ， 其空 时域 分辨率 高而 频域 分辨率 低 　 　 从 函数 逼近 角度 可 把 MFN 看作 由 许多 简单 函数 组成 的 复合 函数 ， 或者 看作 函数 的 综合 过程 文献 ［ ］ 给出 一个 广义 网络 逼近 定理 　 　 以下 各类 网络 在 由 d 维 欧氏 空间 紧集 上 的 连续函数 构成 的 空间 上 ， 在 下述 意义 下 是 稠密 的 ， 即 对 任一 上述 函数 f 都 存在 一个 收敛 于 f 的 网络函数 序列 fn 　 　 所 列 网络 包括 了 不同 结构 和 不同 节点 函数 的 网络 可以 说 任何 一种 复合 函数 都 对应 一种 MFN ， 反之 任一 MFN 也 都 可 写成 复合 函数 的 形式 ， 可 把 网络 看作 逼近 算法 的 图 Graph 示 形式 MFN 表示 的 复合 函数 一般 不能 硬性 归属于 前面 说 的 两种 极端 情况 基于 记忆 的 映射 网络 如 ： CMAC ［ ］ ， 资源 可 重置 网络 ［ ］ 和 RBF 网络 应 属于 有限元 类 ， 多数 以 指数函数 为 节点 函数 的 网络 ， 则 看作 谱 分解 更 合理 　 　 许多 工程 问题 ， 如 动态 系统 建模 ， 信号 的 滤波 及 变换 等 ， 都 是 一个 函数 到 另 一 函数 的 映射 ［ ］ ， 文 ［ ］ 对 MFN 的 逼近 能力 作 了 扩展 ， 证明 它 不仅 可 逼近 Rn 上 的 函数 ， 且 可 逼近 函数 簇泛 函 ， 文 ［ ］ 给出 了 更 一般 的 多层 网络 泛函 的 概念 ， 可 把 MFN 看作 无限 维 函数 空间 上 的 算子 　 　 　 回归 与 分类 　 　 由 随机样本 组 xi ， yii … n 寻找 y 与 x 间 关系 的 最佳 在 均 方 误差 意义 下 函数 yfx 是 统计学 中 的 一个 古老 分支 — — 回归 分析 的 研究 内容 ， 可以 说 任一 参数 或非 参数 回归 模型 都 对应 一个 MFN ， 任一 MFN 也 可 写出 其 对应 的 回归 模型 文 ［ ］ 在 从 统计 观点 研究 ANN 的 学习 时 ， 把 希望 输出 与 观测 输入 的 关系 写成 dgx ε ， gxE ［ d ｜ x ］ 这 就是 回归 模型 文 ［ ］ 从 非线性 数据分析 的 观点 讨论 了 MFN 的 作用 ， 证明 当 用作 分类器 时 ， 是 按 后验 概率 划分 的 ， 对此 ， 文 ［ ］ 有 更 详细 的 讨论 　 　 回归 分析 中有 一 类型 ， 其 形式 与 MFN 更 接近 ， 即 投影 寻踪 回归 ProjctionPurswitRegressionPPR ［ ］ ， 其 输入 x 与 输入 y 间 关系 可 表示 为 其 大致 过程 是 ， 第一步 ， 先 把 高维 的 输入 x 投影 到 低 维空间 u ， 第二步 ， 由 u 的 非线性 函数 σ 的 线性组合 逼近 y ， 要 学习 的 参数 有 三个 ： β ik 平滑 函数 σ 的 形式 和 输入 层 数值 wk ， 文 ［ ， ］ 对 PPR 与 MFN 的 学习 作 了 比较 ， 文 ［ ］ 从 PPR 的 观点 讨论 了 CascadeCorrelation 学习 ， 文 ［ ］ 介绍 了 PPR 网络 的 实现 方法 　 　 正规化 理论 RegularizationTheory 　 　 TPoggio 等 在 文 ［ ］ 中 把 监督 学习 与 逼近 联系 起来 ， 并用 正规化 方法 处理 它们 ， 说明 了 正规化 方法 可 等价 于 一个 只含 一个 隐层 的 MFN ， 称之为 正规化 网络 进一步 又 提出 广义 正规化 网络 ， 它 包含 了 很 广 一类 逼近 网络 ， 文 ［ ］ 是 对 正规化 理论 与 ANN 的 全面 总结 MFN 可 看作 某一 正规化 方法 的 实现 ， 由于 MFN 学习 本身 就是 一个 反 问题 ［ ， ］ ， 它 与 正规化 理论 的 联系 是 显而易见 的 　 对 MFN 的 基本 要求 和 主要 困难 　 　 在 有限 数据 样本 上 学习 后 有 推广 泛化 能力 ， 是 对 MFN 的 基本 要求 有关 泛化 问题 作者 已有 讨论 ［ ］ 除 此 ， 网络 的 容错性 、 学习 速度 等 有时 也 很 重要 ， 相对 其它 性能 而言 ， 泛化 能力 是 主要 的 泛化 能力 与 网络结构 规模 、 给定 样本 数量 与 问题 本身 的 复杂程度 有关 问题 的 复杂程度 无法控制 ， 实际 可 存在 两类 问题 ： 　 　 给定 网络结构 ， 为 达到 好 的 推广 能力 需要 多少 训练样本 　 　 训练样本 量 已定 ， 确定 合理 的 网络结构 以 保证 好 的 推广 能力 　 　 第二类 问题 实际上 遇到 的 最 多 ， 即 在 样本量 已 定时 ， 如何 根据 对 问题 的 先验 知识 如果 有 的话 来 设计 网络结构 ， 下 一步 是 用 BP 法 学习 参数 权 和 阈值 前者 是 结构 学习 ， 后者 是 参数 学习 ， 目前 大部分 研究 是 针对 参数 学习 的 ， 应 看到 更 有 决定 意义 的 是 结构设计 ， 目前 对此 问题 尚 没有 理想 的 方法 ， 文 ［ ］ 对此 作 了 初步 探讨 ， 下 一节 将 从 统计 建模 观点 讨论一下 基本 原则 　 统计 建模 与 最 简单 原则 　 　 建模 是 一种 对 未知 世界 的 逼近 方法 ， 对模型 的 三个 基本 要求 是 ： Flexibility 适用性 ， Dimensionality 计算 复杂性 和 Interpretability 透明性 Flexibility 是 模型 对 不同 问题 提供 准确 逼近 的 能力 ， 不 准确 模型 与 真实 规律 的 偏离 表现 为 估计 结果 的 偏置 BiasDimensionality 是 据 有限 样本 确定 模型 中 的 参数 时 产生 的 不稳定性 ， 表现 为 估计 结果 的 方差 Variance ， 所谓 “ 维数 灾难 ” 是 指为 把 方差 限制 到 要求 的 范围 内 所 需 数据量 随维数 自变量 数 的 迅速 增长 如 指数 增长 Flexibility 和 Dimensionality 是 一对 矛盾 ， 对此 可 简单 说明 如下 ［ ］ ： 　 　 给定 数据 xi ， yi 后 ， 在 均 方 误差 意义 下 y 对 x 的 回归 E ［ y ｜ x ］ 是 最好 的 模型 因为 　 　 可用 后 一项 ， 即 x 与 E ［ y ｜ x ］ 的 接近 程度 作为 x 好坏 的 度量 ， 经过 推导 可 得 　 　 　 　 　 　 　 　 　 　 　 　 偏置 　 　 　 　 　 　 　 　 　 　 　 方差 　 　 结合 MFN 看 ， 若 网络 过于 简单 ， 则 不足以 反映 未知 规律 yfx 的 复杂性 ， 因而 无法 准确 逼近 fx 偏置 大 反之 若 结构 过于 复杂 ， 则 要 由 给定 数据 学习 的 未知 参数 太 多 ， 使 学习 结果 不 稳定 方差 大 　 　 世纪 的 法国 修道士 WilliamOckham 提出 过 一个 最 简单 最 节省 原则 ： “ 与 已知 事实 符合 一致 的 理论 中 的 最 简单 者 就是 最好 的 理论 ” ， 后 人称 此 原则 是 “ OccamsRazor ” 文 ［ ］ 从 Bazes 观点 对此 作 了 一个 定量 说明 这一 思想 与 爱因斯坦 的 一句 名言 “ Anytheorymustbeassimpleaspossiblebutnotsimpler 的 精神 是 一致 的 对 建模 问题 说 就是 与 给定 数据 例子 ， 或 训练样本 一致 的 最 简单 者 是 最好 的 选择 所以 通常 的 建模 准则 总 包含 两项 ， 一是 模型 输出 与 给定 数据 一致 用 对数 似然 函数 表示 ， 当 数据 为 高斯分布 时 就是 均方 误差 ， 二是 对模型 复杂性 参数 多少 的 约束 项 此项 的 作用 可 理解 为 适当 引入 偏置 以 减少 方差 如 信息 判据 AIC ， NIC ［ ］ ， 正规化 ［ ］ ， 最小 描述 长度 MDL ［ ， ］ 都 如此 模型 有 泛化 能力 是 基于 它 表示 的 规律 具有 某种程度 的 平滑 性 简单 性 ， 正规化 理论 ， 回归 模型 也 都 是 以 不同 形式 对 fx 的 平滑 性 加以 约束 　 几点 看法 　 　 对于 为什么 采用 ANN 以及 用 好 ANN 的 关键问题 ， 有 一些 过于 笼统 甚至 似是而非 的 提法 ， 应当 分析 例如 ： 　 　 “ ANN 的 构造 和 作用 机制 更 接近 人脑 ， 所以 对于 模式识别 ， 对 环境 自 适应 等 问题 用 ANN 比 其它 方法 好 ” 。 应当 看到 现有 的 ANN 模型 只是 人脑 的 一种 表面 的 非本质 的 粗略 近似 ， 其 规模 和 机制 的 复杂程度 离 人脑 还有 很大 差距 ， 现有 ANN 仍 是 一种 工程 上 的 人造 模型 用 现有 的 ANN 模型 去 解决 上述 一类 问题 并 不 注定 比 传统 方法 好 ， 传统 方法 中 的 一些 基本 困难 如 模型 选择 、 变量 选择 、 有限 样本 问题 也 没有 消失 　 　 “ ANN 本质 上 具有 学习 和 自 适应能力 ， 只要 有 例子 ， ANN 就 能 通过 学习 解决问题 ” 并非 只有 ANN 才 有 学习 能力 ， 例如 有 较长 历史 的 模式识别 与 可 训练 机器 理论 ［ ， ］ 也 是 通过 学习 来 解决问题 的 ， 也 并非 只要 有 例子 就 一定 能 学到 其中 隐含 的 规律 ， 哪 一类 问题 是 可以 学习 的 ， 对 那些 可以 学习 的 问题 其 学习 的 复杂性 如何 ， 正是 计算 学习 理论 研究 的 问题 ［ — ］ ， 一些 研究者 不 理解 上述 问题 的 重要性 ， 以为 学习 速度 是 主要矛盾 　 　 “ ANN 可以 模拟 任何 非线性 关系 ， 且 不 需 有关 系统 的 模型 知识 ” 并非 只有 ANN 具有 逼近 任何 非线性 规律 的 能力 ， 同时 有 能力 的 方法 不 一定 就是 好 方法 ， 还要 看 它 的 计算 复杂性 是否 可以 接受 有效性 非 参数 回归 可以 模拟 任何 非线性 规律 ， 很早 对 它 的 一致性 即 随着 样本数 n 的 增加 误差 一致 下降 ， 且 n → ∞ 时 e → o 已 给出 了 理论 证明 ， 但 至今 非 参数 回归 用 的 不 多 有 一些 被 称为 非 参数 回归 的 方法 实际上 是 一种 半 参数 法 ， 这是 由于 所 需 样本数 随 问题 规模 急剧 增长 Dimensionality 不好 ， 文 ［ ］ 证明 非 参数 回归 的 收敛 速度 正比 于 nr 其中 rpmpdp 代表 未知 规律 f 的 平滑 程度 ， m 是 要 估计 的 f 的 导数 次数 ， d 为 输入 变量 数 ， 粗略 看 可以 认为 逼近 误差 按 npd 下降 　 　 对 MFN 尚 没有 明确 的 规律 ， 有人 对 逻辑 型 网络 奇偶校验 网络 作 了 试验 ， 把 逻辑 函数 看作 谓词 ， 输入 变量 维数 是 谓词 的 阶数 ， 结果 发现 学习 时间 随 输入 维数 指数 上升 ［ ］ 　 　 “ 传统 的 模式识别 和 回归 分析 要 事先 抽取 特征 ， ANN 可 自动 压缩 变量 维数 ” ， MFN 的 隐层 可 理解 为 对 输入 变量 做 了 变换 ， 但 这 代替 不了 开始 的 特征 变量 选择 ， 特别 是 当 样本量 有限 时 提高 网络 性能 的 一个 重要 措施 是 压缩 原始 变量 的 维数 ， 这一 工作 可用 ANN ［ ］ 也 可以 用 其它 方法 ， 有时 还要 依靠 专家 知识 　 　 此外 还有 以下 问题 值得注意 ： 　 　 有限 样本量 问题 　 样本量 有限 是 统计 模式识别 和 回归 分析 的 主要 困难 ， 如果 样本量 不 受限制 也 有 足够 的 计算资源 来 处理 这些 样本 ， 则 不管 模型 多 复杂 也 不会 出现 过 拟合 现象 ， 统计 建模 中 的 Dimensionality 问题 也 不 存在 了 最近 文 ［ ］ 对此 问题 作 了 分析 ， 结合 ANN 的 讨论 可见 文 ［ ， ］ 没有 其它 先验 知识 时 ， 解决问题 的 信息 全靠 从 训练样本 得到 能 达到 的 最好 效果 决定 于 样本 的 数量 和 质量 如果 认为 样本 来源 可靠 则 只 决定 于 样本 的 数量 希望 所 选 网络 尽可能 把 隐含 在 样本 中 有关 问题 的 信息 都 提取 出来 ， 这 就 要求 网络 与 样本量 “ 匹配 ” ［ ］ 　 　 变量 选择 问题 　 前面 已 指出 ， 为 保证 估计 结果 的 稳定性 ， 所 需 样本量 大致 按 问题 规模 输入 变量 维数 指数 上升 ， 可见 当 样本量 有限 时 ， 保证 推广 能力 的 最 有效 方法 是 降低 输入 向量 的 维数 ， 只 保留 那些 重要 的 变量 特征选择 的 具体方法 在 模式识别 和 回归 分析 中 都 有 ， 这里 不再 讨论 　 　 先验 知识 的 运用 　 如果 能 利用 先验 知识 ， 则 有 可能 限制 模型 的 复杂性 ， 对 解决问题 有 很大 帮助 ， 特别 是 样本量 少 而 问题 又 较 复杂 时 ， 应 设法 尽量 利用 先验 知识 对 此文 ［ ］ 作 了 讨论 ， 许多 人 主张 把 传统 人工智能 技术 与 ANN 结合 起来 ， 在 一定 程度 上 也 是 由于 传统 AI 更 易于 利用 人类 的 先验 知识 以 降低 ANN 学习 的 负担 　 讨论 　 　 提出 以上 看法 不是 想 说明 ANN 的 许多 能力 别的 方法 也 能 做到 ， 从而 限制 ANN 的 应用 与 研究 ， 也 不能 要求 事先 把 为什么 用 ANN 完全 搞清楚 才能 用 它 ， 因为 未 经过 实践 是 不能 搞清楚 的 ， 只是 为了 更 深入 有效 的 研究 ANN 及其 应用 提供 一些 看法 ， 供 大家 深入 思考 MFN 和 一些 传统 方法 的 等效 性 说明 原来 方法 的 一些 基本 问题 仍然 存在 ， 它们 的 研究 结果 对 研究 ANN 是 很 有用 的 ， 出现 上述 问题 的 一个 原因 是 在 ANN 的 发展 初期 ， 国外 一些 文章 较 多 强调 了 ANN 与 传统 方法 的 不同 ， 而 对 使用 中 的 一些 困难 ， 如 结构设计 建模 问题 、 样本量 、 变量 选择 等 很少 讨论 有 一些 应用 文章 不少 是 权威性 的 工作 对 上述 问题 的 处理 不够 合理 ， 例如 有些 应用 中 ， 模型 中 的 未知 参数 权 系数 个数 甚至 比 样本数 还 多 对 这种 情况 最近 国外 也 有所 讨论 ［ ］ ， 应 引起 足够 注意 当然 MFN 确有 自己 的 特点 ， 作者 认为 主要 是 ： 　 　 构造 上 更 接近 生物 神经系统 信息 的 表示 与 处理 是 分布 在 联接 权上 ， 尽管 离 真实 脑 模型 还有 很大 距离 ， 其 功能 也 没有 达到 所 期望 的 那样 强 ， 但 总是 朝 这 方向 前进 了 一步 　 　 有 很大 的 灵活性 ， 同一 形式 的 MFN ， 可 用作 函数 逼近 、 回归 、 正规化 等等 ， 而且 更 形象化 在 某种意义 上 也 可以 灵活 地 引入 不同 性质 的 约束 ， 也 可 方便 地 引入 信息反馈 与 上下文 信息 ， 加 一些 局部 反馈 又 可 模拟 时间 序列 或 动态 系统 等等 促进 了 上述 各 方法 的 融合 　 　 本质 上 的 并行处理 及 结构 上 的 规则性 ， 对 硬件 实现 有利 资金 项目 ： 国家 攀登 计划 认知科学 神经网络 重大 关键 项目 资助 作者 单位 ： 清华大学 自动化系 　 北京 　 参考文献 ［ ］ 张承福 对 当前 神经网络 研究 的 几点 看法 力学 进展 ， ， ： — ［ ］ 张承福 等 联想 记忆 神经网络 中 的 若干 问题 自动化 学报 ， ， ： — ［ ］ BarronARStatisticalpropertiesofartificialneuralnetworksProcthConferenceondecisionandcontrol ［ ］ AlbusAnewapproachtomanipulatorcontrolthecerebellarmodelarticulationcontrollerCMACJonDynamicalSystemsMeasureandcontrol ［ ］ PlattJAresourceallocatingnetworkforfunctioninterpolationNeuralcomputation ［ ］ BarrettJFTheuseoffunctionalintheanalysisofnonlinearphysicalsystemJElectroandcontrol ［ ］ ChenTianpingetalApproximationofcontinuousfunctionsbyNNwithapplicationtodynamicsystemsIEEETransNN ［ ］ ModhaDSetalMultilayerfunctionalsinMathamaticalApprochtoNNTaylorJGedAmsterdamElsevierSciencePublishers ［ ］ WhiteHArtificialneuralnetworksapproximationandlearningtheoryCambirdgeMABlackwell ［ ］ AsohHNonlineardataanalysisandMLPProcofIJCNN Ⅱ Ⅱ ［ ］ GishHApobabilisticapproachtotheunderstandingandtrainingofNNclassifierProcICASSP ［ ］ 成 平等 投影 寻踪 — — 一类 新 的 统计 方法 应用 概率 统计 ， ， ： — ［ ］ MaechlerMetalProjectionpursuitlearningnetworksforregressionProcToolsforAIIEEEPress ［ ］ HwangJengNangetalRegressionmodelinginbackpropagationandprojectionpursuitlearningIEEETransNN ［ ］ HwangJengNangetalThecascadecorrelationlearningAprojectionpursuitlearningperspectiveIEEETransNN ［ ］ ZhaoYingetalImplementingprojectionpursuitlearningIEEETransNN ［ ］ PoggioTetalNetworksforapproximationandlearningProcIEEE ［ ］ GirosiFetalRegularizationtheoryandneuralnetworkarchitecturesNeuralcomputation ［ ］ OgawaHNeuralnetworktheoryasinverseproblemJEICEJapan ［ ］ 阎 平凡 多层 前馈 网络 的 推广 和 学习 问题 中国 神经网络 年 学术会议 论文集 ， 年 ， 上册 ， — ［ ］ 阎 平凡 现在 神经网络 解决问题 的 困难 与 结构 自 适应 问题 中国 神经网络 年 学术 大会 报告 ， ， 月 ， 武汉 ［ ］ GemmanSNeuralnetworksandtheBiasvariancedilemmaNeuralcomputation ［ ］ GarettAJOckhamsRazorinmaximumentropyandbayesmethodGrandyWTetalEdKluwerAcademicPublisher ［ ］ NoboraetalNICDetermingthenumberofhiddenunitforANNIEEETransNN ［ ］ WulizhongetalAsmoothingregularizorforfeedfrowardandrecurrentneuralnetworkdsIEEETransNN ［ ］ 阎 平凡 最小 描述 长度 与 多层 前馈 网络 设计 中 的 一些 问题 模式识别 与 人工智能 ， ， ： ［ ］ RohwerRetalMinimumdescriptionlengthregularizationandmultimodeldataIEEETransNN ［ ］ 边肇祺 等 模式识别 北京 ： 清华大学出版社 ， ［ ］ SklanskyJ 著 ， 阎 平凡 等 译 模式 分类器 和 可 训练 机器 北京 ： 科学出版社 ， ［ ］ 张鸿宾 计算 学习 理论 与其 应用 计算机科学 ， ， ： — ［ ］ AnthonyMetalComputationallearningtheoryCambridgeEnglandCambridgeUniversityPress ［ ］ AnthonyMetalComputationallearningtheoryforANNinMathematicalapprocachtoNNJGTaylorEd ［ ］ StoneCJOptimalglobalrateofconvergencefornonparametricregressionANNStat ［ ］ TesauroGetalScalingrelationshipsinBPlearningComplexsystem ［ ］ MaoJianchangetalANNforfeatureextractionandmultivariatedataprojectionIEEETransNN ［ ］ RandySJetalSmallsamplesizeeffectsinstatisticalpatternrecognitionrecommendationforpracticeIEEETransPAMI ［ ］ 张鸿宾 训练 多层 网络 的 样本数 问题 自动化 学报 ， ， ： ［ ］ 阎 平凡 人工神经网络 的 容量 ， 学习 与 计算机 复杂性 问题 电子学 报 ， ， ： ［ ］ AbuMostafaYSHintsNeuralcomputation ［ ］ DuinRSuperlearningandneuralnetworkmagicPatternrecognitionletters 　 　 阎 平凡 　 简介 见 本刊 第卷 第期 收稿 日期 　