自动化 学报 ACTAAUTOMATICASINICA 年 　 第卷 　 第期 　 Vol 　 No 　 联想 记忆 神经网络 的 一个 有效 学习 算法 梁学斌 　 吴 立德 　 俞俊 　 　 摘 　 要 　 提出 一种 新 的 联想 记忆 网络 模型 的 有效 学习 算法 ， 它 具有 下述 特点 ： 可以 全部 存储 任意 给定 的 训练 模式 集 ， 即 对于 训练 模式 的 数目 和 它们 之间 相关性 的 强弱 没有 限制 ； 最小 的 训练 模型 吸引 域 达到 最大 ； 在 的 基础 上 ， 每个 训练 模式 具有 尽可能 大 的 吸引 域 ； 联想 记忆 神经网络 是 全局 稳定 的 大量 的 计算机 仿真 实验 结果 充分说明 所 提出 的 学习 算法 比 已有 算法 具有 更强 的 存储 能力 和 联想 容错 能力 　 　 关键词 　 联想 记忆 网络 ， 训练 模式 集 ， 全部 存储 ， 最大 吸引 域 ， 全局 稳定性 ANEFFICIENTLEARNINGALGORITHMFORASSOCIATIVEMEMORYNEURALNETWORKLIANGXUEBIN 　 　 WULIDE 　 　 YUJUNDeptofComputerScience ， FudanUniversity 　 Shanghai 　 Abstract 　 AnewandefficientlearningalgorithmofasociativememoryneuralnetworkisproposedwiththefollowingcharacteristicsitcanstoreanygiventrainingpatternsetnomatterhowmuchandwhatcorrelationamongthemmaybethesmallestdomainofattractionoftrainingpatternsismaximizedeachdomainofattractionoftrainingpatternsisguaranteedtobeaslargeaspossiblethedesignedassociativememorynetworkisgloballystableAlargenumberofcomputerexperimentalresultsconfirmthatouralgorithmpossessesmorepowerfulstorageabilityandmorefaulttolerancecapabilitythanexistingonesKeywords 　 Associativememory 　 trainingpatternset 　 totalstorageability 　 maximizeddomainofattraction 　 globalstability 　 引言 　 　 联想 记忆 神经网络 可 分为 有 自 反馈 和 无自 反馈 两种 模型 ， 它 具有 信息 记忆 和 信息 联想 的 功能 ， 能够 从 部分 信息 或 有 适当 畸变 的 信息 联想 出 相应 的 存储 在 联想 记忆 神经网络 中 的 完整 的 记忆 信息 ［ ， ］ 其 性能 主要 是 由 具体 的 学习 算法 来 决定 　 　 至今 ， 已经 提出 了 不少 关于 联想 记忆 神经网络 的 学习 算法 ， 主要 有 Hebbian 学习 算法 ［ ］ 、 投影 学习 算法 ［ ］ 、 Gardner 学习 算法 ［ ］ 、 最小 重叠 学习 算法 ［ ］ 、 HoKashyap 学习 算法 ［ ］ 、 神经元 或 训练 模式 加权 学习 算法 ［ ］ 和 优化 学习 算法 ［ ］ 等 其中 只有 优化 学习 算法 严格 考虑 了 如何 提高 联想 记忆 神经网络 的 存储 能力 和 联想 容错 能力 　 　 基于 文献 ［ ］ 的 思想 ， 本文 提出 了 设计 联想 记忆 网络 的 极大 极小 准则 ， 它 要求 设计 出 的 对称 连接 权阵 应 使得 网络 最小 的 记忆 模式 吸引 域 达到 最大 并 进一步 发展 了 综合 联想 记忆 网络 的 一个 有效 学习 算法 ， 它 具有 如下 特点 ： 可以 全部 存储 任意 给定 的 训练 模式 集 ； 最小 的 训练 模式 吸引 域 达到 最大 ； 在 的 基础 上 ， 每个 训练 模式 具有 尽可能 大 的 吸引 域 ； 网络连接 权阵 是 主 对角 元为 的 对称 阵 ， 因此 所 设计 出 的 联想 记忆 神经网络 是 全局 稳定 的 ［ ］ 　 无自 反馈 网络 模型 训练 式 集 的 基本 约束 　 　 联想 记忆 神经网络 模型 是 由 N 个 互联 神经元 组成 的 非线性 动力系统 网格 状态 vv … vNt ， 其中 vii … N 表示 第 i 个 神经元 的 状态 ， 取值 空间 是 ； 网络连接 权阵 WWijN × N 其中 Wij 表示 从 第 j 个 神经元 到 第 i 个 神经元 的 连接 权 联想 记忆 神经网络 模型 可 表述 为 ［ ］ 　 　 　 　 　 　 　 　 　 　 其中 v ′ iv ′ … v ′ Nt 表示 下 一 时刻 的 状态 向量 ， 非线性 函数 sgnx 定义 为 当 x ≥ 时为 ， 而 当 x 时为 　 　 若 网络连接 权阵 W 满足 WijWjiij … N ， 且 Wiii … N ， 即 连接 权阵 是 一个 具有 零 对角 的 实 对称 阵 ， 则 称为 Hopfield 网络 ［ ］ 　 　 设有 M 个 不同 的 训练 模式 ， 即 x ， x … ， xM ， 其中 xuxu … xuNtu … M 它们 成为 系统 的 稳定 吸引 子 等价 于 对 所有 u … M ， 都 成立 　 　 　 　 　 　 　 　 　 　 　 　 文献 ［ ］ 已 证明 ， Hopfield 网络 的 存储容量 不 超过 N ， 即 对于 任何 正整数 M ， 若 MN ， 则 一定 存在 M 个 训练 模式 ， 它们 不 可能 同时 是 Hopfield 网络 的 稳定 吸引 子 　 　 本节 的 结果 是 ， 即使 M ≤ N ， 若 M 个 不同 训练 模式 同时 成为 Hopfield 网络 的 稳定 吸引 子 ， 则 必须 满足 如下 基本 约束 FundamentalConstraintFC ： 任意 两个 不同 的 训练 模式 至少 有 两个 分量 不同 若不然 ， 则 存在 某 两个 不同 训练 模式 刚好 有且 只有 一个 分量 不同 由于 Hopfield 网络 无自 反馈 ， 故 这 两个 训练 模式 必然 是 同一个 训练 模式 ， 这 就 出现 了 矛盾 　 　 由于 有些 训练 模式 间 的 分类 仅靠 某个 模式 分量 ， 如 汉字 “ 王 ” 和 “ 玉 ” ， “ 已 ” 和 “ 己 ” 等 因此 ， 上述 基本 约束 FC 反映 了 Hopfield 网络 在 存储 能力 方面 的 局限性 　 　 以下 不妨 称 连接 权阵 是 零 对角 实 对称 阵 的 网络 为 无自 反馈 的 Hopfield 网络 ， 称 连接 权阵 是 具有 非零 对角 元 的 实 对称 阵 的 网络 为 有 自 反馈 的 Hopfield 网络 有 自 反馈 的 Hopfield 网络 可以 存储 任意 给定 的 训练 模式 集 ［ ］ ， 问题 的 实质 在于 如何 使 每个 训练 模式 具有 尽量 大 的 吸引 域 　 稳定 吸引 子 和 联想 容错性 分析 　 　 Hopfield 网络 的 连接 权阵 可 由 Hebbian 学习 准则 确定 ， 即 　 　 　 　 　 　 　 　 　 　 　 Hebbian 学习 准则 主要 是 为了 模拟 生物 神经网络 工作 原理 而 提出 的 ［ ］ ， 并 不是 从 联想 记忆 人工神经网络 的 工程设计 准则 出发 它 通常 只能 存储 相关性 较弱 的 训练 模式 集 　 　 定理 M 个 不同 训练 模式 xx … xM 成为 系统 的 稳定 吸引 子 的 充分条件 是 对 所有 u … M 都 成立 　 　 　 　 　 　 　 　 定理 设 xuu ∈ … M 是 系统 的 一个 记忆 模式 ， x 是 一个 畸变 模式 ， Hxxu 表示 x 和 xu 之间 的 Hamming 距离 若 HxxuFuiWmii ∈ … N 则 x 一步 迭代 联想 出 xui ， 其中 　 　 证明 即 要 证明 xi ‘ xui 该 等式 成立 的 一个 充分条件 是 　 　 　 　 　 　 　 　 　 　 　 　 　 注意 到 　 　 　 　 　 　 　 　 　 　 　 　 　 由于 　 　 　 　 　 　 　 　 　 　 故由式 得 　 　 　 　 　 　 　 从而 不等式 成立 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 证毕 　 　 为了 书写 和 结论 简洁 ， 且 不 影响 结论 的 实质 ， 文后 均 假定 对于 所有 i … NWijj … N 不全 为 ， 且 Wmi 这样 ， Fui 值越 大 ， 则 xu 的 第 i 个 分量 的 联想 容错性 就 越强 类似 的 定性 讨论 也 可 参考文献 ［ ， ］ 令 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 推论 设 xuu ∈ … M 是 系统 的 一个 记忆 模式 ， x 是 一个 畸变 模式 若 HxxuFu ， 则 一步 迭代 联想 出 xu 　 　 由 推论 可知 ， Fu 是 记忆 模式 xu 的 吸引 域 半径 的 一个 下界 　 极大 极小 设计 准则 和 两个 学习 算法 　 　 设 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 则 F 是 最小 的 记忆 模式 吸引 域 的 半径 的 一个 下界 　 　 从 数学 上 ， 极大 极小 设计 准则 可 表示 为 下列 有 约束 不可 微 优化 问题 即 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 其中 argmax 的 约束条件 是 WijWji 且 ｜ Wij ｜ ≤ 　 ij … N 　 　 将式 中 的 Wijij 　 ij … N 都 变化 成 Wji ， 则 这个 约束 不可 微 优化 问题 含有 NN 个 自由 变量 ， 即 Wij ， 而 argmax 的 约束条件 是 ｜ Wij ｜ ≤ 因此 ， 有 约束 不可 微 优化 问题 等价 于 下列 线性规划 问题 ： Pmaxz 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 并 带有 约束条件 CFui ≥ z 　 i … N 　 u … M 　 　 　 　 　 　 　 　 　 　 C ｜ Wij ｜ ≤ 　 i ≤ j 　 ij … N 　 　 　 　 　 　 　 　 　 　 线性规划 问题 — 可以 用 单纯形 方法 来 求解 由于 单纯形 方法 相对 较 复杂 ， 特别 是 在 N 和 M 都 比较 大时 为此 ， 本文 将 提出 更 有效 的 简单 方法 　 　 现对式 作 分析 　 　 　 其中 第二个 等 号 利用 了 Fui 只 与 Wijj … N 有关 的 事实 ， 和 分别 表示 在 约束 C 和 C 下求 最大值 ， C 定义 为 C 　 　 ｜ Wij ｜ ≤ 　 j … N 　 　 　 　 　 　 　 　 　 　 　 　 由于 minab ≥ minaminb 其中 a 和 b 是 变量 ， 故由式 得 　 　 　 　 　 　 　 易知 约束 优化 问题 　 　 　 　 　 　 　 　 　 　 　 的 解是 　 　 　 　 　 　 　 　 　 　 由 不等式 可知 ， 式 是 不可 微 优化 问题 式 的 一个 次优 解 　 　 Hopfield 联想 记忆 网络 的 连接 权 可以 由式 来 确定 ， 该 规则 就 称为 快速 学习 算法 它 具有 下述 特点 ： 　 　 可以 存储 任意 给定 的 训练 模式 集 ， 且 具有 一定 的 联想 容错 能力 ［ ］ ； 　 　 可以 用作 其它 约束 优化 迭代 算法 的 初值 ； 　 　 设计 出 的 连接 权值 为 ， 或 ， 特别 是 ， 主 对角 权 元素 全为 ， 故 网络 易于 硬件 实现 和 光学 实现 　 　 设 f 是 线性规划 问题 — 的 解 线性规划 问题 — 等价 于 在 约束条件 C ｜ Wij ｜ ≤ 　 ij 　 ij … N 　 　 　 　 　 　 　 　 　 　 下 求解 线性 不等式 组 　 　 　 　 　 Fui ≥ f 　 i … N 　 u … M 　 　 　 　 　 　 　 　 　 而 在 无 约束条件 下 求解 线性 不等式 组 可用 感知器 算法 ［ ］ 　 　 设 训练 模式 的 学习 序列 是 xx … xM … xx … xM … 即 是 一个 循环 学习 序列 感知器 算法 按此 循环 顺序 接受 训练 模式 xuu ∈ … M ， 且 按 下列 规则 来 修改 连接 权 ： 对于 i … N ， 有 　 　 　 　 　 其中 t 是 算法 迭代 次数 ， q 是 任意 正数 ， Wij 的 迭代 初值 可以 任意 选定 　 　 为 使得 每个 训练 模式 具有 一定 大小 的 吸引 域 ， 在 感知器 算法 中须 加入 约束条件 C 同时 ， 将 快速 学习 算法 的 结果 作为 连接 权阵 的 迭代 初值 ， 并且 只 对 初值 为 的 非 对角 权 元素 进行 优化 迭代 在 此基础 上 ， 再 使得 每个 训练 模式 具有 尽量 大 的 吸引 域 　 　 本文 的 约束 感知器 优化 学习 算法 可 总结 如下 ： 　 　 初始化 Wijij 　 ij … N 由 快速 学习 算法 确定 ， f 并 选取 学习 因子 q ， 迭代 次数 δ 和 δ ； 　 　 学习 设从 循环 学习 序列 xx … xM … xx … xM … 接受 到 训练 模式 xuu ∈ … M ， 则 对于 i … N ， 按下式 来 修改 Wijtij 　 j … N 　 　 　 　 其中 函数 Φ x 定义 为 当 x 时为 ， 当 x 时为 ， 否则 为 x 每 完成 一个 循环 序列 xx … xM ， 就 称为 迭代 一次 ； 　 　 判断 若 算法 收敛 ， 即 Fui ≥ fi … Nu … M 时 的 迭代 次数 小于 δ ， 则 ff ， 转 至 步骤 ； 若 算法 迭代 次数 超过 δ 后 仍 不 收敛 ， 则 Wij 返回 算法 式 迭代 之前 的 值 ， 并 转至 步骤 ； 　 　 增大 每个 训练 模式 的 吸引 域设 fufu … M 设从 循环 学习 序列 xx … xM … xx … xM ， … ， 接受 到 训练 模式 xuu ∈ … M ， 则 对于 i … N ， 按下式 来 修改 Wijtijj … N 　 　 　 每 完成 一个 循环 序列 xx … xM ， 就 称为 迭代 一次 　 　 每 迭代 一次 ， 且 迭代 次数 小于 δ 时 ， 对于 u … M ， 按 如下 规则 递增 fu ： 若 Fui ≥ fui … N ， 且 Fu ′ i ≥ fu ′ u ′ ≠ uu ′ … M 　 i … N 则 fufu ， 并且 迭代 次数 重新 从 开始 ， 返回 步骤 ； 　 　 若 算法 式 在 迭代 次数 超过 δ 后 ， 所有 fuu … M 都 不能 再作 递增 ， 则 Wij 返回 算法 式 迭代 之前 的 值 ， 并 结束 算法 　 计算机 实验 结果 　 　 实验 中取 qN ， δ δ 设 　 　 　 FmaxFmin 和 Fave 可 用作 统计 量 来 定量 客观 地 评价 Hopfield 联想 记忆 学习 算法 的 联想 容错 能力 FmaxFmin 和 Fave 越大 ， 则 说明 算法 的 联想 容错 能力 越强 　 　 第一组 实验 的 训练 模式 集是 汉字 “ 己 ” 、 “ 已 ” 和 “ 巳 ” ， 如图所示 用 快速 学习 算法 可以 记忆 该 训练 模式 集 Hebbian 学习 规则 和 文献 ［ ］ 的 优化 学习 算法 都 不能 存储 这个 训练 模式 集 这 是因为 ， “ 己 ” 和 “ 已 ” 以及 “ 己 ” 和 “ 巳 ” 的 Hamming 距离 均 为 实验 结果表明 Hebbian 学习 规则 的 联想 结果 是 个 训练 模式 都 联想 到 模式 “ 己 ” 表是 快速 学习 算法 和 本文 优化 学习 算法 的 联想 容错 能力 比较 由表 可知 ， 快速 学习 算法 和 本文 优化 学习 算法 在 此 特例 下 ， 具有 相同 的 联想 容错 能力 图 　 实验 训练 模式 集表 　 两种 算法 的 联想 容错 能力 比较 　 FmaxFminFave 快速 学习 算法 本文 优化 学习 算法 　 　 第二组 实验 的 训练 模式 集是 英文字母 “ A ” — “ H ” ， 如图所示 ， 对于 组 训练 模式 集 A ， ABABCABCDABCDEABCDEFABCDEFG 和 ABCDEFGH 分别 用 本文 优化 学习 算法 和 文献 ［ ］ 的 优化 学习 算法 进行 学习 表是 两种 优化 学习 算法 的 容错 能力 比较 ， 其中 M 表示 上述 组 训练 模式 集 分别 所含 的 模式 个数 由表 和 的 结果 可知 ， 本文 优化 学习 算法 比 文献 ［ ］ 的 优化 学习 算法 具有 更强 的 联想 容错 能力 ， 同时 说明 快速 学习 算法 的 结果 作为 迭代 算法 的 初值 可 大大提高 迭代 算法 的 联想 容错性 图 　 实验 训练 模式 集表 　 两种 优化 学习 算法 的 联想 容错 能力 比较 MFmax 本文 Fmax 文献 ［ ］ Fmax 快速 算法 Fmin 本文 Fmin 文献 ［ ］ Fmin 快速 算法 Fave 本文 Fave 文献 ［ ］ Fave 快速 算法 作者简介 ： 梁学斌 　 年 月 获 复旦大学 计算机软件 专业 博士学位 现 从事 计算机 智能 领域 的 研究 工作 ， 在 国内外 已 发表 论文 余篇 　 　 　 　 　 吴 立德 　 复旦大学 计算机科学 系 教授 ， 博士生 导师 从事 计算机 视觉 和 计算机 语言学 方面 的 研究 ， 已 发表 论文 余篇 ， 著作 多部 　 　 　 　 　 俞 　 俊 　 复旦大学 计算机科学 系 研究生 ， 主要 学习 计算机 视觉 和 图象 编码 作者 单位 ： 复旦大学 计算机科学 系 　 上海 　 参考文献 ［ ］ 　 HopfieldJJNeuralnetworksandphysicalsystemswithemergentcollectivecomputationalabilitieInProcNatlAcadSciUSA — ［ ］ 　 MichelANFarrellJAAssociativememoriesviaatrificialneuralnetworksIEEEControlSystMag — ［ ］ 　 HebbianDOTheorganizationofbehaviorNewYorkWiley ［ ］ 　 PersonnazLGuyonIDreyfusGCollectivecomputationalpropertiesofneuralnetworksnewlearningmechanismsPhysRevA — ［ ］ 　 GardnerEThespaceofinteractionsinneuralnetworkmodelsJPhysAMathGen — ［ ］ 　 KrauthWMezardMLearningAlgorithmswithOptimalStabilityinNeuralNetworksJPhysAMathGen — ［ ］ 　 HassounMHYoussefAMHighperformancerecordingalgorithmforhopfeildmodelassociativememoriesOpticalEng — ［ ］ 　 WangTZhuangXHXingXLAneuronweightedalgorithmanditshardwareimplementationinassociativememoriesIEEETransComputer — ［ ］ 　 WangTLearningofHopfieldassociativememorybyglobalminimizationIntJPatternRecogArtifIntell — ［ ］ 　 AtiyaAAbuMostafaYSAnanalogfeedbackassociativememoryIEEETransNeuralNetworks — ［ ］ 　 GolesEFogelmanFPellegrinDDecreasingenergyfunctionsasatoolforstudyingthresholdnetworksDiscreteApplMath — ［ ］ 　 AbbottLFKeplerTBOptimallearninginneuralnetwrokmemoriesJPhysAMathGen — ［ ］ 　 梁学斌 ， 吴 立德 综合 联想 记忆 神经网络 的 外积 取 等 准则 通信 学报 ， ， ： — ［ ］ 　 TouJTGonzalezRCPatternrecognitionprinciplesMassachusettsAddisonWesley — 收稿 日期 　