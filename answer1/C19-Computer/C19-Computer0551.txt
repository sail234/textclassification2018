自动化 学报 ACTAAUTOMATICASINICA 年 　 第卷 　 第期 　 Vol 　 No 　 小脑 模型 神经网络 改进 算法 的 研究 ） 刘慧 　 许晓鸣 　 　 　 摘 　 要 　 该文 介绍 了 小脑 模型 神经网络 的 基本原理 ， 在 分析 Albus ［ ］ 算法 的 基础 上 ， 指出 了 该 算法 在 批量 学习 时 的 缺陷 针对 批量 学习 提出 了 相应 的 改进 算法 ， 并 证明 了 该 算法 的 收敛性 ， 仿真 结果表明 了 该 改进 算法 具有 收敛 速度 快 的 特点 　 　 关键词 　 神经网络 ， 杂凑 编码 ， 联想 网络 ， CMACANIMPROVEDCMACNEURALNETWORKALGORITHMLIUHUIXUXIAOMINGDepartmentofAutomationShanghaiJiaotongUniversityShanghaiAbstract 　 ThebasicprincipleofCMACCerebelllarModelArticulationControllerisintroducedBasedonadetailanalysisofAlbusalgorithm ［ ］ thepaperpointsoutitsdrawbackinbatchlearningAnimprovedalgorithmisproposedandtheoreticalproofsarealsogivenSimulationresultsshowthattheimprovedmethodhashigherspeedandbetterconvergencethantheoriginalKeywords 　 NeuralnetworkhashcodingassociativenetworkCMAC 　 引言 　 　 自年 Hopfield 发表 了 关于 反馈 神经网络 的 文章 以及 Rumelhart 等 人 发表 了 专著 PDP 以来 ， 在 世界 范围 内 掀起 了 研究 神经网络 的 热潮 近年来 ， 人工 神经元网络 和 人工智能 在 控制 界 中 的 应用 研究 正在 兴起 和 蓬勃发展 ， 特别 是 神经网络 具有 充分 逼近 任意 复杂 非线性 函数 的 能力 ， 为 解决 复杂 的 非线性 问题 开辟 了 一条 控制 的 具有 特殊 联想 功能 的 神经网络 CMACCerebellarModelArticulationController 　 　 CMAC 神经网络 是 由 JSAlbus ［ ］ 在 年 提出 的 它 与 Perceptron 网 相似 ， 虽然 从 每个 神经元 看其 关系 是 一种 线性关系 ， 但 从 结果 总体 看 CMAC 模型 适合 于 非线性 的 映射 关系 同时 它 的 算法 是 十分 简单 的 δ 算法 ， 所以 速度 很快 它 把 输入 在 一个 多维 状态 空间 中 的 量 ， 映射 到 一个 比较 小 的 有限 区域 只要 对 多维 状态 空间 中 部分 样本 进行 学习 ， 就 可 达到 轨迹 学习 和 控制 的 解 ， 因此 特别 适合 于 机器人 的 轨变 学习 控制 ， 实时 学习 控制 ， 非线性 函数 映射 ， 以及 模式识别 等 领域 CMAC 具有 自 适应 的 作用 ， 并且 易于 硬件 化 实现 　 CMAC 的 基本原理 　 　 CMAC 的 简单 结构 模型 如图所示 ， 输入 空间 S 由 所有 可能 的 输入 向量 Si 组成 ， CMAC 网络 将 其 接受 到 的 任何 输入 ， 通过 感知器 M 映射 到 一个 很大 的 联想 存储器 A 中 的 c 个 单元 输入 空间 邻近 的 两个 输入 向量 在 存储器 A 中有 部分 重叠 的 单元 ； 距离 越近 ， 重叠 越 多 反之 ， 若 输入 空间 远离 的 两个 输入 向量 在 A 中 并 不 重叠 对 一个 实际 系统 ， 输入 空间 中 的 向量 数是 很大 的 ， 例如 ， 某 系统 有个 输入 ， 而 每个 输入 可取 个值 ， 则 在 输入 空间 有个 向量 ， A 的 存储量 需要 个 单元 由于 绝大多数 学习 问题 并 不 包括 所有 输入 空间 中 的 状态 ， 故所 需 的 存储空间 可 通过 常用 的 计算机 存储 压缩 技术 — — 随机 杂凑 编码 Hashcoding 技术 映射 到 一个 小得多 的 物理 可 实现 的 存储器 A ′ 任何 CMAC 网络 的 输入 激活 c 个 真实 存储 位置 ， 而 这些 位置 的 值 被 相加 得到 输出 向量 P 图 　 CMAC 网格 的 模型 结构 　 Albus 的 CMAC 算法 　 　 设 共有 P 个 训练样本 ， 联想 向量 ai 为 M → A 的 映射 ， CMAC 网络 对于 第 i 个 训练样本 的 输出 为 fSi ） wTai ， 理想 输出 为 di ， 误差 信号 δ idifsi ， CMAC 是 按 δ 学习律 调整 网络 的 权值 ， 是 在 梯度 法 的 基础 上 采用 LMS 算法 得到 的 ［ ］ 其中 β 为 学习 步长 　 　 当 我们 在 第 l 次 循环 时 对 第 k 个 样本 进行 训练 时 ， 产生 的 修整 量 为 β c ， 其中 为 第 k 个 样本 的 输出 误差 因 CMAC 网络 内在 的 泛化 能力 ， 在 输入 空间 相近 的 向量 在 实际 存储器 A ′ 中有 重叠 ， 故该 修正 量 势必 影响 其它 样本 的 输出 ， 如 它 使 第 i 个 样本 的 新 输出 为 其中 cik 为 第 ik 个 样本 在 A ′ 中 的 重叠 单元 数 ， 显然 cikcki 经 I 次 循环 后 ， 输入 样本 k 产生 的 修正 量 对 其 本身 的 累积 输出 误差 为 ， 与 之 相关 的 每个 权 的 修正 量 为 Δ k ＝ Ek ／ c ， Δ k 称为 累积 权 误差 累积 权 误差 对 第 i 个 样本 的 输出 贡献 为 cik Δ k 当 CMAC 算法 收敛 ， → ， 则 Δ k → const 若 我们 由 Δ i 得 wi ， 则 在 网络 算法 中可换 一个 角度 将 累积 权 误差 Δ i 而 不是 权 wi 视为 需要 学习 的 变量 ， 这样 CMAC 的 收敛 也 可 转化 为 Δ i 的 收敛 问题 设 网络 初始 权为 零 ， 则 对 每个 输入 向量 si ， 网络 的 输出 为 学习 的 目的 是 使 fsidi ， 即 ， 写成 矩阵 形式 C Δ ＝ D 令 　 A ［ aa … ap ］ T ， 则 AWD 　 　 通常 神经网络 权 的 学习 有 逐一 和 批量 两种 方法 ， 设 训练样本 集 为 ｛ （ sdsd … ， spdp ｝ 逐一 学习 是 每 输入 一个 样本 便 对 整个 网络 的 权 进行 修正 即先 对 sd 进行 学习 ， 利用 输出 误差 来 对 网络 的 权 进行 修正 ， 使 网络 的 对应 关系 满足 fsd ， 设 此时 网络 的 权为 w 然后 以 w 为 基础 ， 对 样本 sd 进行 学习 ， 直到 权值 w 满足 fsd 但是 一般 情况 下 ， w ≠ w ， 故 此时 未必 有 fsd 成立 所以 逐一 学习 的 方法 会 出现 学 了 新 的 ， 忘 了 旧 的 “ 遗忘 ” 现象 为了 克服 这个 缺点 需 采用 反复 循环 学习 ， 但 这样 又 会 带来 收敛 慢 的 问题 我们 建议 用 批量 学习 的 方法 来 对权 进行 更新 ， 将 样本 一一 输入 ， 为 一批 样本 输入 后用 总 的 误差 来 修整 权 ， 这种 方法 可以 克服 “ 遗忘 ” 的 缺点 ， 又 有 较 快 的 收敛 速度 　 　 Albus 本人 并未 给出 CMAC 算法 收敛性 的 证明 ， Wong ［ ］ 和 Parks ［ ］ 从 线性方程组 迭代 解 的 角度 ， Wong ［ ］ 还 从 频域 的 角度 阐述 了 Albus 算法 的 收敛性 问题 他们 的 研究 都 是 针对 逐一 学习 的 ， 对 批量 学习 的 情况 并未 进行 研究 作者 针对 批量 学习 情况 进行 了 一些 探索 ， 指出 了 Albus 算法 在 逐一 学习 时 适用 ， 而 在 批量 学习 时易 发散 的 缺陷 ， 并 提出 了 相应 的 改进 算法 　 批量 学习 时 Albus 算法 的 缺陷 及 改进 算法 　 　 定理 对于 多 输入输出 目标 中 的 一组 训练样本 ， 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 则 Albus 算法 采用 批量 学习 方法 时 的 收敛 条件 为 　 　 证明 当 Albus 算法 采用 批量 学习 时 ， 累积 权 误差 的 修正 公式 为 其中 　 　 为 方便 起 见 ， 设 输入输出 空间 均 为 一维 ， 所有 训练样本 的 量化 值 都 在 ［ ， R ］ 上 ， 则 所 需 神经元 为 Rc 个 　 　 当 收敛 到 Δ 时 ， 由式 得 令 　 ， 得 　 　 e 为 一 有限 长 的 N 点非 周期 序列 ， N ＝ Rc 将 其作 周期 延拓 ， e 可 写成 Fourier 级数 形式 　 　 将 代入 并 考虑 可 得 其中 c 一般 取为 ～ ， 故 sinc ω c 可 忽略 若 ω ≠ π ， 则 　 　 当且 仅 当 ｜ Hej ω ｜ ＜ 时 算法 收敛 ， 在 一个 数字 频率 周期 π 内 ｜ Hej ω ｜ ＜ ， 即 　 　 当 ω π 时 ， ω ∈ KerA ， 不 影响 w 和 Δ 的 收敛 所以 Albus 算法 采用 批量 学习 时 的 收敛 条件 为 　 　 推论 对于 多 输入输出 目标 中 的 一组 训练样本 ， 设 所有 训练样本 的 量化 值 都 在 ［ ， R ］ 上 ， 感受 野 宽度 为 c 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 则 Albus 算法 采用 批量 学习 方法 时 的 收敛 条件 为 　 　 证明 　 将 代入 式 ， 得 　 　 　 　 　 由 Fourier 级数 知 ， k 的 取值 为 ［ N ］ 间 的 连续 整数 ， 而 采用 Albus 算法 批量 学习 时 的 收敛 条件 为 ， k 在 和 两段 取值 时 算法 不 收敛 N 越大 ， 即 量化 取值 数 R 和 感受 野 宽度 c 越大 ， 不 收敛 的 点 越 多 　 　 为了 克服 Albus 算法 对 批量 学习 不太 适合 的 缺陷 ， 提出 了 一种 改进 的 CMAC 算法 把 式 修改 为 其中 q 为 批量 学习 一次 每个 权 平均 被 更新 的 次数 　 　 定理 对于 多 输入输出 目标 中 的 一组 训练样本 ， 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 改进 CMAC 算法 采用 批量 学习 方法 时 的 收敛 条件 为 ctgq ＜ ω ＜ （ π ctgq 　 　 证明 　 改进 CMAC 算法 采用 批量 方法 对权 进行 更新 时 ， 相应 的 算法 收敛 证明 中 的 公式 变为 　 　 当且 仅 当 ｜ Hej ω ｜ ＜ 时 算法 收敛 ， 在 一个 数字 频率 周期 π ） 内 ｜ Hej ω ｜ ＜ ， 即 ctg － q ＜ ω ＜ π ctgq 　 　 当 ω π 时 ， ω ∈ KerA ， 不 影响 w 和 Δ 的 收敛 所以 改进 CMAC 算法 采用 批量 学习 时 的 收敛 条件 为 ctgq ＜ ω ＜ π ctgq 　 　 讨论 由于 CMAC 网络 所 固有 的 局域 泛化 能力 ， 每个 输入 向量 使 其 对应 量化 值 周围 c 个 感知器 同时 被 激励 ； 相应 地 ， 当 一批 样本 被 学习 一遍 后 ， 每个 网络 的 权 也 不止一次 被 更新 所以 minq 当 q 取 最小值 时 ， 由得 改进 算法 的 收敛 条件 为 ＜ ω ＜ 弧度 ， 改进 算法 的 收敛 范围 比 Albus 算法 宽 ， q 越大 改进 算法 的 收敛 范围 将 越 宽 　 　 推论 对于 多 输入输出 目标 中 的 一组 训练样本 ， 设 所有 训练样本 的 量化 值 都 在 ［ ， R ］ 上 ， 感受 野 宽度 为 c 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 改进 CMAC 算法 采用 批量 学习 方法 时 的 收敛 条件 为 　 　 证明 　 将 代入 式 即可 得 改进 算法 采用 批量 学习 时 的 收敛 条件 为 　 　 讨论 将 进一步 展开 为 其中 　 　 当 q 取 最小值 时 ， ctgq 弧度 ， 而 反 余切 函数 在 区间 ［ ， π ］ 内 是 递减 的 ， 所以 ctgq ＜ ， 从而 b ＞ b ＞ 可见 改进 算法 的 收敛 范围 比 Albus 算法 宽 q 越大则 b 和 b 越大 ， 改进 算法 的 收敛 范围 也 将 越 宽 　 　 改进 CMAC 算法 对 逐一 学习 也 是 适用 的 ， 其 收敛性 证明 可 参见 文献 ［ ］ ， 其中 累积 权 误差 的 修正 类似 Jocobi 迭代法 解 线性方程组 　 仿真 结果 　 　 设某 CMAC 网络 用来 实现 下列 一维 非线性 函数 的 映射 ， 其中 输入 变量 si ∈ ［ ， ］ ， 分辨率 为 ， 所有 训练样本 的 量化 值 都 在 ［ ， R ］ 上 ， RCMAC 网络 的 感受 野 宽度 c 批量 学习 一次 每个 权 平均 被 更新 的 次数 q 取为 则 N ， ctgq 按 Δ s 的 间隔 均匀 取点 ， 由式 进行 理论 计算 ， 可得组 输入输出 的 样本 数据 ， 运用 这些 样本 数据 分别 按 Albus 算法 和 本文 的 改进 算法 对 CMAC 神经网络 进行 训练 为了 显示 网络 的 泛化 能力 ， 计算 网络 输出 时 采用 了 不同于 训练 时 的 数据 Δ s 由式 计算 得 Albus 算法 批量 训练 时 的 收敛 条件 为 ＜ k ＜ 而由式 知 ＜ k ＜ 时 本文 的 改进 算法 批量 训练 时 收敛 ， 易 见 改进 后 的 算法 比 Albus 算法 收敛 条件 宽 　 　 图 所示 为 逐一 学习 时 的 误差 曲线 ， 两种 算法 都 可 取得 较 好 的 收敛 效果 ， 而 本文 提出 的 改进 方法 收敛 速度 更 快 一些 图是 在 批量 学习 时 的 误差 曲线 ， 由该 图 可见 ， Albus 算法 易 导致 发散 ， 而 采用 本文 的 改进 算法 可以 获得 收敛 较 快 的 效果图 　 逐一 学习 误差 曲线图 　 批量 学习 误差 曲线 　 结论 　 　 本文 在 对 Albus 算法 研究 的 基础 上 ， 提出 了 适合 于 批量 学习 的 改进 算法 ， 并 在 理论 上 证明 了 该 算法 的 收敛性 ， 仿真 结果表明 了 该 算法 在 逐一 和 批量 学习 时 都 有 较 好 的 收敛性 和 快速性 　 留学 回国 人员 科研 基金 资助 课题 作者简介 刘 　 慧 　 年生 ， 年 获 上海交通大学 自动控制 理论 及 应用 专业 博士学位 现为 上海交通大学 自动化系 讲师 主要 研究 兴趣 是 智能 控制 ， 学习 控制 及 神经网络 在 控制 中 的 应用 　 　 许晓鸣 　 年生 ， 现任 上海交通大学 自动化系 教授 、 博士生 导师 主要 研究 兴趣 是 智能 控制 和 复杂 工业 系统 预测 控制 等 方面 的 研究 作者 单位 上海交通大学 自动化系 　 上海 　 参考文献 ［ ］ 　 AlbusJSDatastorageinthecerebellarmodelarticulationcontrollerCMACTransASMEJDynSystMeasContr — ［ ］ 　 AlbusJSAnewapproachtomanipulatorcontrolThecerebellarmodelarticulationControllerCMACTransASMEJDynSystMeasContr — ［ ］ 　 WongYFSiderisALearningconvergenceinthecerebellarmodelarticulationcontrollerIEEETransonNeuralNetworks — ［ ］ 　 ParksPCMilitzerJConvergencepropertiesofassociativememorystorageforlearningcontrolsystemAutomationandRemoteControl — ［ ］ 　 WongYFCMAClearningisgovernedbyasingleparameterIEEEIntConfonNN — 收稿 日期 　