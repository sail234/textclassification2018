自动化 学报 AGTAAUTOMATICASINICA 年 第卷 第期 VolNoQ 学习 算法 在 库存 控制 中 的 应用 蒋国飞 　 吴沧浦 摘 　 要 　 Q 学习 算法 是 Watkins 提出 的 求解 信息 不 完全 马尔可夫 决策问题 的 一种 强化 学习 方法 这里 提出 了 一种 新 的 探索 策略 ， 并 将 该 策略 和 Q 学习 算法 有效 结合 来 求解 一类 典型 的 有 连续 状态 和 决策空间 的 库存 控制 问题 仿真 表明 ， 该 方法 所 求解 的 控制策略 和 用值 迭代法 在 模型 已知 的 情况 下 所 求得 的 最优 策略 非常 逼近 ， 从而 证实 了 Q 学习 算法 在 一些 系统 模型 未知 的 工程 控制 问题 中 的 应用 潜力 关键词 　 Q 学习 ， 马尔可夫 决策 过程 ， 库存 控制 ， 连续 状态 和 决策空间 ， 探索 策略 INVENTORYCONTROLUSINGQLEARNINGJIANGGuofeiWuCangpuDepartmentofAutomaticControlBeijingInstituteofTechnologyBeijingAbstract 　 QlearningisareinforcementlearningmethodtosolveMarkoviandecisionproblemswithincompleteinformationInthispaperwepresentanovelexplorationstrategyanduseQlearningmethodwiththisstrategytosolveatypicalinventorycontrolproblemwithcontinuousstateanddecisionspaceSimulationresultsareincludedtoshowthattheoptimalpolicygivenbyQlearningcanwellapproximatetotheaccurateoneKeywords 　 QlearningMarkoviandecisionprobleminventorycontrolcontinuousstateanddecisionspaceexplorationstrategy 　 引言 　 　 Q 学习 算法 是 求解 信息 不 完全 马尔可夫 决策问题 的 一种 有效 的 强化 学习 方法 自从 Watkins ［ ］ 在 年 提出 Q 学习 算法 并 证明 了 其 收敛性 后 ， 该 算法 在 强化 学习 研究 领域 中 得到 了 人们 的 普遍 关注 然而 ， 目前 对 Q 学习 算法 的 应用 研究 还 很 不够 ， 基本上 限于 人工智能 方面 ， 如 最 短 路径 搜索 问题 ［ ， ］ 、 动物 觅食 游戏 ［ ］ 等 ， 实际 应用 的 例子 很少 ； 另一方面 对 该 算法 的 研究 也 只 局限于 一些 离散 状态 和 决策空间 迄今为止 还 没有 见到 在 连续 状态 和 决策空间 上 应用 的 例子 ， 而 实际 应用 中 许多 随机 控制系统 的 状态 和 决策 是 连续 的 本文 通过 离散 化 问题 的 状态 和 决策空间 ， 提出 用 Q 学习 算法 来 求解 运筹学 中 一类 典型 的 有 连续 状态 和 决策空间 的 库存 控制 问题 ， 同时 针对 决策空间 离散 化后 可行 控制 数目 较大 的 情况 ， 提出 了 一种 新 的 探索 策略 ， 来 对 控制 和 探索 之间 的 冲突 进行 有效 的 折衷 　 　 先 简述 有限 马尔可夫 决策问题 的 模型 如下 ： 在 每个 时间 步 k … 控制器 观察 马氏 过程 的 状态 为 xk ， 选择 决策 ak ， 收到 即时 报酬 rk ， 并 使 系统 转移 到 下 一个 状态 yk 转移 概率 为 Pxkykak 控制 目的 是 寻求 一 最优控制 策略 ， 使 报酬 函数 最大 ， 其中 ≤ γ ≤ 为 折扣 因子 　 　 在 一些 实际 例子 中 ， 状态 转移 概率 和 所 获 报酬 是 未知 的 ， 求解 这 类 信息 不 完全 的 马氏 决策问题 有 两种 主要 方法 ： 贝叶斯 方法 和 非 贝叶斯 方法 而 非 贝叶斯 方法 又 分为 直接 法和非 直接 法 直接 法是 不 估计 系统 模型 而 直接 基于 观察 值来 求解 最优 策略 ［ ］ Q 学习 算法 属于 直接 法 　 　 给定 一个 策略 π ， 定义 Q 值为 在 状态 x ， 控制 a 及 后续 策略 π 下 的 报酬 折扣 和 的 期望 即 其中 　 学习 就是 要 在 转移 概率 和 所 获 报酬 未知 的 情况 下 估计 最优 策略 的 Q 值 定义 其中 π 表示 最优 策略 　 　 在线 Q 学习 方法 实现 如下 ： 在 每个 时间 步 k ， 观察 当前 状态 xk ， 选择 和 执行 控制 ak ， 再 观察 后继 状态 yk 及 接受 即时 报酬 rk ， 然后 根据 下 式 调整 Qk 值 ： 其中 　 α k 为 学习 因子 ， 　 　 Watkins ［ ］ 证明 了 学习 因子 序列 ｛ α k ｝ 在 满足 一定 的 条件 下 ， 如果 任 一个 xa 二元 组能 用 方程 进行 无穷 多次 迭代 ， 则 当 k → ∞ 时 ， Qkxa 以 概率 收敛 于 Qxa 　 库存 控制 及其 有限 马氏 决策 过程 模型 　 　 设 xk 是 产品 在 k 段 开始 时 的 库存量 ， uk 是 k 段 开始 时 的 订货量 假设 订 了 货 即可 立即 交货 ， wk 是 k 段 中 的 需求量 ， 是 一个 随机变量 ， R 是 产品 的 仓库 库存 限量 hh ≥ 是 单位 产品 的 库存 费用 pp ＞ 是 单位 产品 的 脱 需 费用 ， cc ＞ 是 单位 产品 的 订货 费用 假定 如 需求量 超出 可能 的 供应量 时 ， 不 允许 先 记帐 而后 补 ， 即 xk 不 允许 取 负值 此时 可 建立 库存 优化 控制 问题 的 模型 如下 ： 　 　 状态 转移 规律 由 下列 方程 确定 　 　 wkk … 的 概率分布 相同 且 相互 独立 ； 　 　 控制 的 目的 是 选择 订货 策略 ukxk 来 极小 化 指标 函数 其中 γ 为 折扣 因子 ， Cu 为 如下 定义 函数 ： 　 　 如果 产品 的 仓库 库存 没有 限量 即 取消 式 中 的 约束 xkuk ≤ R ， Bertsekas 在 文献 ［ ］ 中 证明 了 当 p ＞ c 时 可 求得 以上 问题 的 最优 策略 为 其中 S 为 函数 的 极小 点 ， 而 为 满足 GyKGS 的 y 的 最小值 　 　 用 Q 学习 算法 求解 马氏 决策问题 ， 不须 知道 过程 的 状态 转移 规律 ， 只要 在 决策 过程 中 能 观测 到 系统 的 四元组 序列 xkukrxkukyk 其中 xk 为 当前 状态 ， uk 为 当前 控制 ， rxkuk 为 在 系统 状态 xk 上 执行 控制 uk 后 得到 的 报酬 ， yk 为 后继 状态 ， 就 能 通过 学习 求得 问题 的 最优 策略 在 仿真 中 ， 以上 所 建立 的 库存 控制 模型 代表 实际 过程 向 Q 学习 提供 这个 四元组 序列 而 在 实际 应用 中 ， 在线 Q 学习 算法 可以 直接 从 系统 观测 这个 四元组 序列 　 　 要 应用 Q 学习 方法 来 求解 以上 有 连续 状态 和 决策空间 的 库存 控制 问题 ， 首先 需 将 状态 和 决策空间 离散 化后 建立 一个 信息 不 完全 的 有限 马尔可夫 决策 过程 设将 状态 离散 化成 N 段 ， x ， … ， i … j … N 将 控制 离散 化成 M 段 ， u … k … M ； 格子 点 间距 为 Lwk 为 一个 连续 的 随机变量 ， 不 需要 离散 化 这样 就 可以 推出 即时 报酬 rikCkLhmaxiLkLwkpmaxwkiLkL ； 状态 转移 概率 PijkPjL ≤ maxiLkLwk ＜ jL 然而 由于 随机 需求量 wk 的 概率分布 是 未知 的 ， 所以 式 中 的 Pijk 实际上 也 是 未知 的 在 仿真 中 系统 状态 根据 式 进行 转移 并 集结 到 相应 的 格子 点上 在 决策空间 离散 化后 ， 为了 加快 Q 学习 算法 的 收敛 速度 和 求得 最优 策略 ， 下面 提出 了 一种 新 的 探索 策略 来 选择 控制 行为 　 探索 策略 　 　 在 Q 学习 方法 的 实现 中 ， 有 多种 探索 方法 ［ ］ 但 最为 常用 的 是 Boltzmann 分布 探索 假定 在 时间 步 k 时 ， 状态 为 xk 控制 a ∈ Axk ， Axk 是 状态 为 xk 时 的 控制 集合 定义 EaQxka ， a ∈ Axk 则 控制器 用 以下 概率 来 选择 控制 a 其中 T 为 温度 因子 ， 随着 学习 进行 而 逐渐 衰减 　 　 根据 Boltzmann 分布 探索 方法 的 特点 ， 在 其 基础 上 提出 了 一种 新 的 基于 计数器 的 直接 探索 方法 在 每个 时间 步 k ， 控制器 用 以下 概率 来 选择 控制 a 其中 Pexploita 为 Boltzmann 分布 探索 ， 如式 所示 式 中 的 Γ ＜ Γ ＜ 选一 固定值 或 一 简单 的 分段 函数 而式 中 的 Pexplorea 由下式 确定 其中 式 中 nxka 为 到 时间 步 k 为止 ， 系统 在 状态 xk 执行 控制 ak 的 次数 α β 为 常数 且 满足 α ＞ β ＞ 参数 mm ＞ 随 学习 进行 而 逐渐 衰减 到 零 由于 ≤ Pexplorea ≤ ≤ Pexploita ≤ 由式 显然 可证 ≤ Pa ≤ 及 在 学习 过程 的 初始 阶段 ， 对 a ∈ Axk ， 式 中 的 温度 T 和 式 中 的 参数 m 越大 ， Pexploita 分布 越 均匀 而 Pexplorea 的 分布 越有 差异 ， 此时 系统 将 主要 根据 Pexplorea 来 选择 控制 行为 ， 从而 保证 环境 被 充分 探索 和 获取 各种 控制 经验 随着 学习 的 进行 ， T ， m 逐渐 衰减 ， a ∈ AxkEexplorea 的 值 将 逐渐 趋向 一致 ， 从而 使 Eexploita 分布 越有 差异 而 Eexplorea 的 分布 越为 均匀 ， 此时 代表 控制 经验 的 Eexploit 在 控制 行为 a 的 选择 中 逐渐 增加 了 作用 而 代表 环境 探索 的 Eexplore 逐渐 减少 作用 ， 从而 增加 已有 控制 经验 的 利用 和 改善 系统控制 性能 随着 学习 的 进一步 进行 ， mT 将 趋向于 零 ， a ∈ Axk ， 为 集合 Axk 中 控制 的 个数 ， 此时 系统 将 主要 根据 Pexploita 来 选择 控制 行为 ， 从而 来 充分利用 控制 经验 和 保证 算法 的 收敛 速度 在 mT 都 趋于 零时 探索 策略 实质 上 已 变成 一半 均匀分布 探索 ， 即以 概率 值为 Γ 选择 对应 Q 值 最大 的 控制 ， 而 以 概率 值为 Γ 随机 均匀 地 在 所有 可行 控制 中 进行 选择 ， 这样 在 学习 过程 的 最后 阶段 一方面 能 保证 算法 的 收敛 速度 ， 另一方面 也 仍然 保留 了 对 环境 的 一定 探索 以 求得 最优 策略 　 仿真 及 结果 　 　 在 仿真 实例 中 ， 产品 的 仓库 库存 限量 R ， 单位 产品 的 库存 费用 h 单位 产品 的 脱 需 费用 p ， 单位 产品 的 订货 费用 c ， 订货 固定 费用 K 折扣 因子 γ 随机 需求 wk 服从 ， 的 正态分布 在 模型 已知 时 ， 将 状态 、 控制 分别 离散 化成 NM 段 ， 格子 点 间距 为 L ， 用值 迭代法 求得 问题 的 最优 策略 如图所示 同时 在 模型 未知 时 ， 将 状态 、 控制 分别 离散 化成 NM 段 ， 格子 点 间距 为 LQ 学习 方法 结合 新 的 基于 计数器 的 探索 方法 在 经过 万次 迭代 后 求得 问题 的 控制策略 如图所示 ， 继续 迭代 控制策略 可 收敛 到 图 所示 的 最优 策略 而 Q 学习 方法 结合 Boltzmann 分布 探索 在 经过 万次 迭代 后 求得 问题 的 控制策略 如图所示 ， 但 该 方法 在 经过 上 千万次 迭代 后 仍然 不能 完全 收敛 到 问题 的 最优 策略 图中 ux 表示 控制策略 ， x 表示 状态 　 　 图 　 　 　 　 　 　 　 　 　 图 　 　 　 　 　 　 　 　 　 图注 ． 图设 模型 已知 时 ， 用值 迭代法 求解 的 最优 策略 　 NML ； 　 　 图设 模型 未知 时 ， 用 Q 学习 算法 结合 新 的 探索 策略 求解 的 控制策略 　 NML ； 　 　 图设 模型 未知 时 ， 用 Q 学习 算法 结合 Boltzmann 分布 探索 策略 求解 的 控制策略 　 NML 　 　 比较 图 、 图 可知 ， Q 学习 算法 所 求得 的 控制策略 和 在 模型 已知 的 情况 下用值 迭代法 求得 问题 的 最优 策略 非常 逼近 ， 均 为 式 所示 的 S 策略 ， S 约 为 而且 仿真 表明 通过 Q 学习 的 继续 迭代 ， 图 所示 的 控制策略 可以 收敛 到 最优 策略 这一 结果 说明 了 在 系统 模型 未知 的 情况 下 ， 通过 离散 化 问题 的 状态 和 决策空间 ， 可以 用 Q 学习 算法 来 求解 某些 有 连续 状态 和 决策空间 的 马氏 决策问题 的 最优 策略 同时 通过 比较 图 、 图 可知 ， 新 的 探索 策略 和 Boltzmann 分布 探索 相比 ， 不仅 提高 了 学习 速度 而且 求解 的 控制策略 明显 更 逼近 最优 解 以至 能 收敛 到 最优 解 　 结语 　 　 本文 讨论 了 Q 学习 算法 及 相关 的 探索 技术 ， 提出 了 一种 新 的 探索 策略 并 将 该 策略 和 Q 学习 算法 有效 结合 来 求解 一类 典型 的 有 连续 状态 和 决策空间 的 库存 控制 问题 仿真 表明 ： 该 方法 所 求解 的 控制策略 和 用值 迭代法 在 模型 已知 的 情况 下 所 求得 的 最优 策略 非常 逼近 ， 从而 证实 了 Q 学习 算法 在 一些 系统 模型 未知 的 工程 控制 问题 中 的 应用 潜力 国家自然科学基金 资助 项目 作者简介 ： 蒋国飞 　 年生 ， 年 毕业 于 北京理工大学 自动控制 系现 为 北京理工大学 博士生 主要 研究 方向 是 最优控制 和 智能 控制 等 　 　 吴沧浦 　 简介 见 本刊 第卷 第期 作者 单位 ： 北京理工大学 自动控制 系 　 北京 　 参考文献 　 WatkinsCJCHLearningfromdelayedrewardsPhDDissertationKingsCollegeUK 　 BartoAGBradtkeSJSinghSPLearningtoactusingrealtimedynamicprogrammingArtificialIntelligence 　 LinLJSelfimprovingreactiveagentsbasedonreinforcementlearningplanningandteachingMachineLearning 　 BerstekasDPDynamicProgrammingandStochasticControlNewYork ： AcademicPress 　 ThrunSBTheroleofexplorationinlearningcontrolHandbookofIntelligentControlNeuralFuzzyandAdaptiveApproachesNY ： VanostrandReinhold 　 SuttonRSBartoAGWilliamsRJReinforcementlearningisdirectadaptiveoptimalcontrolIEEEControlSystemMagazine 　 PengJEfficientdynamicprogrammingbasedlearningforcontrolPhDthesisNortheasternUniversityUSA 收稿 日期 　 收 修改稿 日期 　