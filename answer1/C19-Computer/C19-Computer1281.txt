自动化 学报 ACTAAUTOMATICASINICA 　 Vol 　 No 　 P 关于 统计 学习 理论 与 支持 向量 机张 学工 摘要 模式识别 、 函数 拟合 及 概率密度 估计 等 都 属于 基于 数据 学习 的 问题 ， 现有 方法 的 重要 基础 是 传统 的 统计学 ， 前提 是 有 足够 多样 本 ， 当 样本 数目 有限 时 难以 取得 理想 的 效果 统计 学习 理论 SLT 是 由 Vapnik 等 人 提出 的 一种 小 样本 统计 理论 ， 着重 研究 在 小 样本 情况 下 的 统计 规律 及 学习 方法 性质 SLT 为 机器 学习 问题 建立 了 一个 较 好 的 理论 框架 ， 也 发展 了 一种 新 的 通用 学习 算法 — — 支持 向量 机 SVM ， 能够 较 好 的 解决 小 样本 学习 问题 目前 ， SLT 和 SVM 已 成为 国际 上 机器 学习 领域 新 的 研究 热点 本文 是 一篇 综述 ， 旨在 介绍 SLT 和 SVM 的 基本 思想 、 特点 和 研究 发展 现状 ， 以 引起 国内 学者 的 进一步 关注 关键词 统计 学习 理论 ， 支持 向量 机 ， 机器 学习 ， 模式识别 INTRODUCTIONTOSTATISTICALLEARNINGTHEORYANDSUPPORTVECTORMACHINESZHANGXuegongDeptofAutomationTsinghuaUniversityBeijingStateKeyLaboratoryofIntelligentTechnologyandSystemsofChinaAbstractDatabasedmachinelearningcoversawiderangeoftopicsfrompatternrecognitiontofunctionregressionanddensityestimationMostoftheexistingmethodsarebasedontraditionalstatisticswhichprovidesconclusiononlyforthesituationwheresamplesizeistendingtoinfinitySotheymaynotworkinpracticalcasesoflimitedsamplesStatisticalLearningTheoryorSLTisasmallsamplestatisticsbyVapniketalwhichconcernsmainlythestatisticprincipleswhensamplesarelimitedespeciallythepropertiesoflearningprocedureinsuchcasesSLTprovidesusanewframeworkforthegenerallearningproblemandanovelpowerfullearningmethodcalledSupportVectorMachineorSVMwhichcansolvesmallsamplelearningproblemsbetterItisbelievedthatthestudyofSLTandSVMisbecominganewhotareainthefieldofmachinelearningThisreviewintroducesthebasicideasofSLTandSVMtheirmajorcharacteristicsandsomecurrentresearchtrendsKeywordsStatisticallearningtheorysupportvectormachinemachinelearningpatternrecognition 引言 　 　 基于 数据 的 机器 学习 是 现代 智能 技术 中 的 重要 方面 ， 研究 从 观测 数据 （ 样本 ） 出发 寻找 规律 ， 利用 这些 规律 对 未来 数据 或 无法 观测 的 数据 进行 预测 包括 模式识别 、 神经网络 等 在内 ， 现有 机器 学习 方法 共同 的 重要 理论 基础 之一 是 统计学 传统 统计学 研究 的 是 样本 数目 趋于 无穷大 时 的 渐近 理论 ， 现有 学习 方法 也 多 是 基于 此 假设 但 在 实际 问题 中 ， 样本数 往往 是 有限 的 ， 因此 一些 理论 上 很 优秀 的 学习 方法 实际 中 表现 却 可能 不尽人意 　 　 与 传统 统计学 相比 ， 统计 学习 理论 （ StatisticalLearningTheory 或 SLT ） 是 一种 专门 研究 小 样本 情况 下 机器 学习 规律 的 理论 VVapnik 等 人 从 六 、 七十年代 开始 致力于 此 方面 研究 ［ ］ ， 到 九十年代 中期 ， 随着 其 理论 的 不断 发展 和 成熟 ， 也 由于 神经网络 等 学习 方法 在 理论 上 缺乏 实质性 进展 ， 统计 学习 理论 开始 受到 越来越 广泛 的 重视 ［ ］ 　 　 统计 学习 理论 是 建立 在 一套 较 坚实 的 理论 基础 之上 的 ， 为 解决 有限 样本 学习 问题 提供 了 一个 统一 的 框架 它 能 将 很多 现有 方法 纳入 其中 ， 有望 帮助 解决 许多 原来 难以解决 的 问题 （ 比如 神经网络 结构 选择 问题 、 局部 极小 点 问题 等 ） ； 同时 ， 在 这 一 理论 基础 上 发展 了 一种 新 的 通用 学习 方法 — — 支持 向量 机 （ SupportVectorMachine 或 SVM ） ， 它 已 初步 表现 出 很多 优于 已有 方法 的 性能 一些 学者 认为 ， SLT 和 SVM 正在 成为 继 神经网络 研究 之后 新 的 研究 热点 ， 并 将 有力 地 推动 机器 学习 理论 和 技术 的 发展 ［ ］ 　 　 我国 早 在 八十年代 末 就 有 学者 注意 到 统计 学习 理论 的 基础 成果 ［ ］ ， 但 之后 较 少 研究 ， 目前 只有 少部分 学者 认识 到 这个 重要 的 研究 方向 本文 旨在 向 国内 介绍 统计 学习 理论 和 支持 向量 机 方法 的 基本 思想 和 特点 ， 以使 更 多 的 学者 能够 看到 它们 的 优势 从而 积极 进行 研究 文章 第二节 给出 机器 学习 问题 的 一般 表示 ， 并 简要 讨论 现有 方法 的 一些 问题 ； 第三节 介绍 SLT 的 基本 思想 和 最 有 影响 的 结论 ； 第四节 介绍 SVM 方法 的 原理 、 应用 及 由此 发展 出 的 其它 方法 ； 第五节 是 讨论 机器 学习 的 基本 问题 问题 的 表示 　 　 机器 学习 的 目的 是 根据 给定 的 训练样本 求 对 某 系统 输入输出 之间 依赖 关系 的 估计 ， 使 它 能够 对 未知 输出 作出 尽可能 准确 的 预测 可以 一般 地 表示 为 ： 变量 y 与 x 存在 一定 的 未知 依赖 关系 ， 即 遵循 某一 未知 的 联合 概率 Fxy ， （ x 和 y 之间 的 确定性 关系 可以 看作 是 其 特例 ） ， 机器 学习 问题 就是 根据 n 个 独立 同 分布 观测 样本 　 在 一组 函数 ｛ fxw ｝ 中求 一个 最优 的 函数 fxw 对 依赖 关系 进行 估计 ， 使 期望 风险 Rw ∫ LyfxwdFxy 　 最小 其中 ， ｛ fxw ｝ 称作 预测 函数 集 ， w 为 函数 的 广义 参数 ， ｛ fxw ｝ 可以 表示 任何 函数 集 ； Lyfxw 为 由于 用 fxw 对 y 进行 预测 而 造成 的 损失 ， 不同 类型 的 学习 问题 有 不同 形式 的 损失 函数 预测 函数 也 称作 学习 函数 、 学习 模型 或 学习 机器 　 　 有 三类 基本 的 机器 学习 问题 ， 即 模式识别 、 函数 逼近 和 概率密度 估计 对 模式识别 问题 ， 输出 y 是 类别 标号 ， 两类 情况 下 y ｛ ｝ 或 ｛ ｝ ， 预测 函数 称作 指示 函数 ， 损失 函数 可以 定义 为 　 使 风险 最小 就是 Bayes 决策 中使 错误率 最小 在 函数 逼近 问题 中 ， y 是 连续变量 （ 这里 假设 为 单值 函数 ） ， 损失 函数 可定义 为 Lyfxwyfxw 　 即 采用 最小 平方 误差 准则 而 对 概率密度 估计 问题 ， 学习 的 目的 是 根据 训练样本 确定 x 的 概率密度 记 估计 的 密度 函数 为 pxw ， 则 损失 函数 可以 定义 为 Lpxwlogpxw 　 经验 风险 最小化 　 　 在 上面 的 问题 表述 中 ， 学习 的 目标 在于 使 期望 风险 最小化 ， 但是 ， 由于 我们 可以 利用 的 信息 只有 样本 ， 式 的 期望 风险 并 无法 计算 ， 因此 传统 的 学习 方法 中 采用 了 所谓 经验 风险 最小化 （ ERM ） 准则 ， 即用 样本 定义 经验 风险 　 作为 对式 的 估计 ， 设计 学习 算法 使 它 最小化 对 损失 函数 ， 经验 风险 就是 训练样本 错误率 ； 对式 的 损失 函数 ， 经验 风险 就是 平方 训练 误差 ； 而 采用 式 损失 函数 的 ERM 准则 就 等价 于 最大 似然 方法 　 　 事实上 ， 用 ERM 准则 代替 期望 风险 最小化 并 没有 经过 充分 的 理论 论证 ， 只是 直观 上 合理 的 想当然 做法 ， 但 这种 思想 却 在 多年 的 机器 学习 方法 研究 中 占据 了 主要 地位 人们 多年 来 将 大部分 注意力 集中 到 如何 更好 地 最小化 经验 风险 上 ， 而 实际上 ， 即使 可以 假定 当 n 趋向于 无穷大 时 式 趋近 于式 ， 在 很多 问题 中 的 样本 数目 也 离 无穷大 相去甚远 那么 在 有限 样本 下 ERM 准则 得到 的 结果 能 使 真实 风险 也 较 小 吗 ？ 复杂性 与 推广 能力 　 ERM 准则 不 成功 的 一个 例子 是 神经网络 的 过 学习 问题 开始 ， 很多 注意力 都 集中 在 如何 使 Rempw 更 小 ， 但 很快 就 发现 ， 训练 误差 小 并 不 总能 导致 好 的 预测 效果 某些 情况 下 ， 训练 误差 过 小 反而 会 导致 推广 能力 的 下降 ， 即 真实 风险 的 增加 ， 这 就是 过 学习 问题 　 　 之所以 出现 过 学习 现象 ， 一 是因为 样本 不 充分 ， 二是 学习 机器 设计 不合理 ， 这 两个 问题 是 互相 关联 的 设想 一个 简单 的 例子 ， 假设 有 一组 实数 样本 xy ， y 取值 在 ［ ］ 之间 ， 那么 不论 样本 是 依据 什么 模型 产生 的 ， 只要 用 函数 fx α sin α x 去 拟合 它们 （ α 是 待定 参数 ） ， 总 能够 找到 一个 α 使 训练 误差 为 零 ， 但 显然 得到 的 “ 最优 ” 函数 并 不能 正确 代表 真实 的 函数 模型 究其原因 ， 是 试图用 一个 十分复杂 的 模型 去 拟合 有限 的 样本 ， 导致 丧失 了 推广 能力 在 神经网络 中 ， 若 对 有限 的 样本 来说 网络 学习 能力 过 强 ， 足以 记住 每个 样本 ， 此时 经验 风险 很快 就 可以 收敛 到 很小 甚至 零 ， 但 却 根本无法 保证 它 对 未来 样本 能 给出 好 的 预测 学习 机器 的 复杂性 与 推广性 之间 的 这种 矛盾 同样 可以 在 其它 学习 方法 中 看到 　 　 文献 ［ ］ 给出 了 一个 实验 例子 ， 在 有 噪声 条件 下用 模型 yx 产生 个 样本 ， 分别 用 一个 一次函数 和 一个 二次 函数 根据 ERM 原则 去 拟合 ， 结果显示 ， 虽然 真实 模型 是 二次 ， 但 由于 样本数 有限 且 受 噪声 的 影响 ， 用 一次函数 预测 的 结果 更好 同样 的 实验 进行 了 次 ， 的 结果 是 一次 拟合 好于 二次 拟合 　 　 由此 可 看出 ， 有限 样本 情况 下 ， 经验 风险 最小 并不一定 意味着 期望 风险 最小 ； 学习 机器 的 复杂性 不但 应 与 所 研究 的 系统 有关 ， 而且 要 和 有限 数目 的 样本 相适应 我们 需要 一种 能够 指导 我们 在 小 样本 情况 下 建立 有效 的 学习 和 推广 方法 的 理论 统计 学习 理论 的 核心内容 　 　 统计 学习 理论 就是 研究 小 样本 统计 估计 和 预测 的 理论 ， 主要 内容 包括 四个 方面 ［ ］ ： 　 　 经验 风险 最小化 准则 下 统计 学习 一致性 的 条件 ； 　 　 在 这些 条件 下 关于 统计 学习 方法 推广性 的 界 的 结论 ； 　 　 在 这些 界 的 基础 上 建立 的 小 样本 归纳推理 准则 ； 　 　 实现 新 的 准则 的 实际 方法 （ 算法 ） 其中 ， 最有 指导性 的 理论 结果 是 推广性 的 界 ， 与此相关 的 一个 核心 概念 是 VC 维 VC 维 　 　 为了 研究 学习 过程 一致 收敛 的 速度 和 推广性 ， 统计 学习 理论 定义 了 一系列 有关 函数 集 学习 性能 的 指标 ， 其中 最 重要 的 是 VC 维 （ VapnikChervonenkisDimension ） 模式识别 方法 中 VC 维 的 直观 定义 是 ： 对 一个 指示 函数 集 ， 如果 存在 h 个 样本 能够 被 函数 集中 的 函数 按 所有 可能 的 h 种 形式 分开 ， 则 称 函数 集 能够 把 h 个 样本 打散 ； 函数 集 的 VC 维 就是 它 能 打散 的 最大 样本 数目 h 若 对 任意 数目 的 样本 都 有 函数 能 将 它们 打散 ， 则 函数 集 的 VC 维是 无穷大 有界实 函数 的 VC 维 可以 通过 用 一定 的 阈值 将 它 转化成 指示 函数 来 定义 　 　 VC 维 反映 了 函数 集 的 学习 能力 ， VC 维越 大则 学习 机器 越 复杂 （ 容量 越大 ） 遗憾 的 是 ， 目前 尚 没有 通用 的 关于 任意 函数 集 VC 维 计算 的 理论 ， 只 对 一些 特殊 的 函数 集 知道 其 VC 维 比如 在 n 维 实数 空间 中 线性 分类器 和 线性 实 函数 的 VC 维是 n ， 而 上 一节 例子 中 fx α sin α x 的 VC 维则 为 无穷大 对于 一些 比较复杂 的 学习 机器 （ 如 神经网络 ） ， 其 VC 维 除了 与 函数 集 （ 神经网 结构 ） 有关 外 ， 还 受 学习 算法 等 的 影响 ， 其 确定 更加 困难 对于 给定 的 学习 函数 集 ， 如何 （ 用 理论 或 实验 的 方法 ） 计算 其 VC 维是 当前 统计 学习 理论 中 有待 研究 的 一个 问题 ［ ］ 推广性 的 界 　 　 统计 学习 理论 系统地 研究 了 对于 各种类型 的 函数 集 ， 经验 风险 和 实际 风险 之间 的 关系 ， 即 推广性 的 界 ［ ］ 关于 两类 分类 问题 ， 结论 是 ： 对 指示 函数 集中 的 所有 函数 （ 包括 使 经验 风险 最小 的 函数 ） ， 经验 风险 Rempw 和 实际 风险 Rw 之间 以 至少 η 的 概率 满足 如下 关系 ［ ］ ： 　 其中 h 是 函数 集 的 VC 维 ， n 是 样本数 　 　 这一 结论 从 理论 上 说明 了 学习 机器 的 实际 风险 是 由 两 部分 组成 的 ： 一是 经验 风险 （ 训练 误差 ） ， 另 一部分 称作 置信 范围 ， 它 和 学习 机器 的 VC 维及 训练样本 数 有关 可以 简单 地 表示 为 Rw ≤ Rempw Φ hn 　 它 表明 ， 在 有限 训练样本 下 ， 学习 机器 的 VC 维越 高 （ 复杂性 越高 ） 则 置信 范围 越大 ， 导致 真实 风险 与 经验 风险 之间 可能 的 差别 越大 这 就是 为什么 会 出现 过 学习 现象 的 原因 机器 学习 过程 不但 要 使 经验 风险 最小 ， 还要 使 VC 维 尽量 小以 缩小 置信 范围 ， 才能 取得 较 小 的 实际 风险 ， 即 对 未来 样本 有 较 好 的 推广性 　 　 需要 指出 ， 推广性 的 界 是 对于 最坏 情况 的 结论 ， 在 很多 情况 下 是 较松 的 ， 尤其 当 VC 维较 高时 更是如此 （ 文献 ［ ］ 指出 当 h ／ n ＞ 时 这个 界 肯定 是 松弛 的 ， 当 VC 维 无穷大 时 这个 界 就 不再 成立 ） 而且 ， 这种 界 只 在 对 同 一类 学习 函数 进行 比较 时 有效 ， 可以 指导 我们 从 函数 集中 选择 最优 的 函数 ， 在 不同 函数 集 之间 比较 却 不 一定 成立 Vapnik 指出 ［ ］ ， 寻找 更好 地 反映 学习 机器 能力 的 参数 和 得到 更紧 的 界 是 学习 理论 今后 的 研究 方向 之一 结构 风险 最小化 　 　 上面 的 结论 看到 ， ERM 原则 在 样本 有限 时 是 不合理 的 ， 我们 需要 同时 最小化 经验 风险 和 置信 范围 其实 ， 在 传统 方法 中 ， 选择 学习 模型 和 算法 的 过程 就是 调整 置信 范围 的 过程 ， 如果 模型 比较 适合 现有 的 训练样本 （ 相当于 hn 值 适当 ） ， 则 可以 取得 比较 好 的 效果 但 因为 缺乏 理论指导 ， 这种 选择 只能 依赖 先验 知识 和 经验 ， 造成 了 如 神经网络 等 方法 对 使用者 “ 技巧 ” 的 过分 依赖 　 　 统计 学习 理论 提出 了 一种 新 的 策略 ， 即 把 函数 集 构造 为 一个 函数 子集 序列 ， 使 各个 子集 按照 VC 维 的 大小 （ 亦 即 Φ 的 大小 ） 排列 ； 在 每个 子 集中 寻找 最小 经验 风险 ， 在 子集 间 折衷 考虑 经验 风险 和 置信 范围 ， 取得 实际 风险 的 最小 ， 如图所示 这种 思想 称作 结构 风险 最小化 （ StructuralRiskMinimization 或译 有序 风险 最小化 ［ ］ ） 即 SRM 准则 统计 学习 理论 还 给出 了 合理 的 函数 子集 结构 应 满足 的 条件 及 在 SRM 准则 下 实际 风险 收敛 的 性质 ［ ］ 图 有序 风险 最小化 示意图 　 　 实现 SRM 原则 可以 有 两种 思路 ， 一是 在 每个 子 集中 求 最小 经验 风险 ， 然后 选择 使 最小 经验 风险 和 置信 范围 之 和 最小 的 子集 显然 这种 方法 比较 费时 ， 当 子集 数目 很大 甚至 是 无穷 时 不 可行 因此 有 第二种 思路 ， 即 设计 函数 集 的 某种 结构 使 每个 子 集中 都 能 取得 最小 的 经验 风险 （ 如使 训练 误差 为 ） ， 然后 只 需 选择 选择 适当 的 子集 使 置信 范围 最小 ， 则 这个 子 集中 使 经验 风险 最小 的 函数 就是 最优 函数 支持 向量 机 方法 实际上 就是 这种 思想 的 具体 实现 文献 ［ ］ 中 讨论 了 一些 函数 子集 结构 的 例子 和 如何 根据 SRM 准则 对 某些 传统 方法 进行 改进 的 问题 支持 向量 机 　 　 支持 向量 机 简称 SVM ， 是 统计 学习 理论 中 最 年轻 的 内容 ， 也 是 最 实用 的 部分 其 核心内容 是 在 到 年间 提出 的 ［ ］ ， 目前 仍 处在 不断 发展 阶段 广义 最优 分类 面 　 　 SVM 是从 线性 可 分 情况 下 的 最优 分类 面 发展 而来 的 ， 基本 思想 可用 图 的 两维 情况 说明 图中 ， 实心 点 和 空心 点 代表 两类 样本 ， H 为 分类 线 ， HH 分别 为 过 各类 中离 分类 线 最近 的 样本 且 平行 于 分类 线 的 直线 ， 它们 之间 的 距离 叫做 分类 间隔 （ margin ） 所谓 最优 分类 线 就是 要求 分类 线 不但 能 将 两类 正确 分开 （ 训练 错误率 为 ） ， 而且 使 分类 间隔 最大 分类 线 方程 为 x ． wb ， 我们 可以 对 它 进行 归一化 ， 使得 对 线性 可分 的 样本 集 xiyi ， i … nx ∈ Rd ， y ∈ ｛ ｝ ， 满足 　 图 线性 可 分 情况 下 的 最优 分类 线 此时 分类 间隔 等于 ‖ w ‖ ， 使 间隔 最大 等价 于 使 ‖ w ‖ 最小 满足条件 且 使 最小 的 分类 面 就 叫做 最优 分类 面 ， HH 上 的 训练样本 点 就 称作 支持 向量 　 　 使 分类 间隔 最大 实际上 就是 对 推广 能力 的 控制 ， 这是 SVM 的 核心思想 之一 统计 学习 理论 指出 ［ ］ ， 在 N 维空间 中 ， 设 样本分布 在 一个 半径 为 R 的 超球 范围 内 ， 则 满足条件 ‖ w ‖ ≤ A 的 正则 超平面 构成 的 指示 函数 集 fxwbsgn ｛ w ． xb ｝ sgn 为 符号 函数 ） 的 VC 维 满足 下面 的 界 h ≤ min ［ RA ］ N 　 因此 使 ‖ w ‖ 最小 就是 使 VC 维 的 上界 最小 ， 从而 实现 SRM 准则 中 对 函数 复杂性 的 选择 　 　 利用 Lagrange 优化 方法 可以 把 上述 最优 分类 面 问题 转化 为 其 对偶 问题 ［ ］ ， 即 在 约束条件 　 a 和 α i ≥ i … n 　 b 下 对 α i 求解 下列 函数 的 最大值 　 α i 为 与 每个 样本 对应 的 Lagrange 乘子 这 是 一个 不等式 约束 下 二次 函数 寻优 的 问题 ， 存在 唯一 解 容易 证明 ， 解 中将 只有 一部分 （ 通常 是 少部分 ） α i 不为 零 ， 对应 的 样本 就是 支持 向量 解 上述 问题 后 得到 的 最优 分类 函数 是 　 式 中 的 求和 实际上 只 对 支持 向量 进行 b 是 分类 阈值 ， 可以 用任 一个 支持 向量 （ 满足 式 中 的 等 号 ） 求得 ， 或 通过 两类 中 任意 一对 支持 向量 取中值 求得 　 　 在 线性 不可 分 的 情况 下 ， 可以 在 条件 中 增加 一个 松弛 项 ξ i ≥ ， 成为 　 将 目标 改为 求 最小 ， 即 折衷 考虑 最少 错 分 样本 和 最大 分类 间隔 ， 就 得到 广义 最优 分类 面 其中 ， C 是 一个 常数 ， 它 控制 对错 分 样本 惩罚 的 程度 广义 最优 分类 面 的 对偶 问题 与 线性 可 分 情况 下 几乎 完全相同 ， 只是 条件 b 变为 ≤ α i ≤ Ci … n 　 支持 向量 机 　 　 对于 N 维空间 中 的 线性 函数 ， 其 VC 维为 N ， 但 根据 式 的 结论 ， 在 ‖ w ‖ ≤ A 的 约束 下 其 VC 维 可能 大大 减小 ， 即使 在 十分 高维 的 空间 中 也 可以 得到 较 小 VC 维 的 函数 集 （ 比如 文 ［ ］ 中 介绍 了 在 维空间 中 取得 VC 维在 左右 的 分类 面 的 例子 ） ， 以 保证 有 较 好 的 推广性 同时 我们 看到 ， 通过 把 原 问题 转化 为 对偶 问题 ， 计算 的 复杂度 不再 取决于 空间 维数 ， 而是 取决于 样本数 ， 尤其 是 样本 中 的 支持 向量 数 这些 特点 使 有效 地 对付 高维 问题 成为 可能 　 　 对 非线性 问题 ， 可以 通过 非线性 变换 转化 为 某个 高 维空间 中 的 线性 问题 ， 在 变换 空间 求 最优 分类 面 这种 变换 可能 比较复杂 ， 因此 这种 思路 在 一般 情况 下 不易 实现 但是 注意 到 ， 在 上面 的 对偶 问题 中 ， 不论是 寻优 函数 还是 分类 函数 都 只 涉及 训练样本 之间 的 内积 运算 xi ． xj ， 这样 ， 在 高 维空间 实际上 只 需 进行 内积 运算 ， 而 这种 内积 运算 是 可以 用原 空间 中 的 函数 实现 的 ， 我们 甚至 没有 必要 知道 变换 的 形式 根据 泛函 的 有关 理论 ， 只要 一种 核 函数 Kxixj 满足 Mercer 条件 ， 它 就 对应 某一 变换 空间 中 的 内积 ［ ］ 　 　 因此 ， 在 最优 分类 面中 采用 适当 的 内积 函数 Kxixj 就 可以 实现 某一 非线性 变换 后 的 线性 分类 ， 而 计算 复杂度 却 没有 增加 ， 此时 目标 函数 变为 　 而 相应 的 分类 函数 也 变为 　 这 就是 支持 向量 机 　 　 概括地说 ， 支持 向量 机 就是 首先 通过 用 内积 函数 定义 的 非线性 变换 将 输入 空间 变换 到 一个 高 维空间 ， 在 这个 空间 中求 （ 广义 ） 最优 分类 面 SVM 分类 函数 形式 上 类似 于 一个 神经网络 ， 输出 是 中间 节点 的 线性组合 ， 每个 中间 节点 对应 一个 支持 向量 ， 如图所示 图 支持 向量 机 示意图 核 函数 　 　 SVM 中 不同 的 内积 核 函数 将 形成 不同 的 算法 ， 目前 研究 最多 的 核 函数 主要 有 三类 ， 一是 多项式 核 函数 　 所 得到 的 是 q 阶 多项式 分类器 ； 二是 径向 基 函数 （ RBF ） 所得 分类器 与 传统 RBF 方法 的 重要 区别 是 ， 这里 每个 基 函数 中心 对应 一个 支持 向量 ， 它们 及 输出 权值 都 是 由 算法 自动 确定 的 也 可以 采用 Sigmoid 函数 作为 内积 ， 即 Kxxitanhvx ． xic 　 这时 SVM 实现 的 就是 包含 一个 隐层 的 多层 感知器 ， 隐层 节点 数是 由 算法 自动 确定 的 ， 而且 算法 不 存在 困扰 神经网络 方法 的 局部 极小 点 问题 用于 函数 拟合 的 SVM 　 　 SVM 方法 也 可以 很 好 地 应用 于 函数 拟合 问题 中 ［ ～ ］ ， 其 思路 与 在 模式识别 中 十分相似 首先 考虑 用 线性 回归 函数 fxw ． xb 拟合 数据 ｛ xiyi ｝ i … nxi ∈ Rdyi ∈ R 的 问题 ， 并 假设 所有 训练 数据 都 可以 在 精度 ε 下 无 误差 地用 线性 函数 拟合 ， 即 　 与 最优 分类 面中 最大化 分类 间隔 相似 ， 这里 控制 函数 集 复杂性 的 方法 是 使 回归 函数 最 平坦 ， 它 等价 于 最小化 考虑 到 允许 拟合 误差 的 情况 ， 引入 松弛 因子 ξ i ≥ 和 ξ i ≥ ， 则 条件 变成 　 优化 目标 变成 最小化 常数 C ＞ 控制 对 超出 误差 ε 的 样本 的 惩罚 程度 采用 同样 的 优化 方法 可以 得到 其 对偶 问题 在 条件 　 下 ， 对 Lagrange 因子 α i α i 最大化 目标 函数 　 得 回归 函数 为 　 　 　 与 模式识别 中 的 SVM 方法 一样 ， 这里 α i α i 也 将 只有 小 部分 不 为 ， 它们 对应 的 样本 就是 支持 向量 ， 一般 是 在 函数 变化 比较 剧烈 的 位置 上 的 样本 ； 而且 这里 也 是 只 涉及 内积 运算 ， 只要 用核 函数 Kxixj 替代 ， 中 的 内积 运算 就 可以 实现 非线性 函数 拟合 核 函数 主 成分 分析 　 SVM 方法 中 一个 重要 启示 是 用 内积 运算 实现 某种 非线性 变换 ， 这种 思想 也 可以 在 其它 问题 中 得到 的 应用 ， 比较 成功 的 例子 就是 用核 函数 实现 非线性 主 成分 分析 ［ ， ］ ， 它 是 传统 主 成分 分析 （ PCA ） 方法 的 推广 　 　 对于 样本 集 ｛ x … xn ｝ ， 主 成分 方向 是 矩阵 的 特征向量 对 x 进行 非线性 变换 φ x ， 可得 其 特征向量 v 就是 原 样本 集 的 非线性 主 成分 方向 ， 满足 将 每个 样本 与 该式 内积 ， 得 　 可以 证明 ， 特征向量 v 可以 写成 将 它 代入 式 中 ， 并 定义 矩阵 　 （ Kij 为 矩阵 的 第 i 行第 j 列个 元素 ） ， 可以 得到 n λ aKa 　 其中 从 矩阵 K 的 特征向量 a 即可 求出 的 特征向量 v ， 即 φ x 空间 的 主 成分 方向 对于 原 空间 中 的 任意 向量 x ， 它 在 变换 空间 中 的 主 成分 是 φ x 在 主 成分 方向 v 上 的 投影 ， 即 　 显然 ， 与 SVM 算法 中 类似 ， 这里 得到 的 非线性 主 成分 方法 只 需 在 原 空间 中 计算 用作 内积 的 核 函数 Kxixj ， 而 无需 真正 计算 对应 的 非线性 变换 ， 因此 称作 核 函数 主 成分 分析 应用 研究 　 　 比较 遗憾 的 是 ， 虽然 SVM 方法 在 理论 上 具有 很 突出 的 优势 ， 但 与其 理论 研究 相比 ， 应用 研究 尚 相对 比较 滞后 ， 目前 只有 较 有限 的 实验 研究 报道 ， 且 多 属 仿真 和 对比 实验 ［ ］ SVM 的 应用 应该 是 一个 大有作为 的 方向 　 　 在 模式识别 方面 最 突出 的 应用 研究 是 贝尔实验室 对 美国 邮政 手写 数字 库 进行 的 实验 ［ ］ ， 这是 一个 可 识别性 较差 的 数据库 ， 人工 识别 平均 错误率 是 ， 用 决策树 方法 识别 错误率 是 ， 两层 神经网络 中 错误率 最小 的 是 ， 专门 针对 该 特定 问题 设计 的 五层 神经网络 错误率 为 （ 其中 利用 了 大量 先验 知识 ） ， 而用 三种 SVM 方法 （ 采用 式 ～ 的 核 函数 ） 得到 的 错误率 分别 为 、 和 ， 且 其中 直接 采用 了 × 的 字符 点阵 作为 SVM 的 输入 ， 并 没有 进行 专门 的 特征提取 实验 一方面 说明 了 SVM 方法 较 传统 方法 有 明显 的 优势 ， 同时 也 得到 了 不同 的 SVM 方法 可以 得到 性能 相近 的 结果 （ 不像 神经网络 那样 依赖于 模型 的 选择 ） 实验 还 观察 到 ， 三种 SVM 求出 的 支持 向量 中有 以上 是 重合 的 ， 它们 都 只是 总 样本 中 很少 的 一部分 ， 说明 支持 向量 本身 对 不同 方法 具有 一定 的 不 敏感性 （ 遗憾 的 是 这些 结论 仅仅 是 有限 的 实验 中 观察 到 的 现象 ， 如果 能 得到 证明 ， 将会 使 SVM 的 理论 和 应用 有 更 大 的 突破 ） 围绕 这一 字符识别 实验 ， 还 提出 了 一些 对 SVM 的 改进 ， 比如 引入 关于 不变性 的 知识 ［ ］ 、 识别 和 去除 样本 集中 的 野值 ［ ］ 、 通过 样本 集 预处理 提高 识别 速度 ［ ］ 等 ， 相关 的 应用 还 包括 SVM 与 神经网络 相结合 对 笔迹 进行 在线 适应 ［ ］ 除此之外 ， MIT 用 SVM 进行 的 人脸 检测 实验 也 取得 了 较 好 的 效果 ， 可以 较 好 地 学会 在 图像 中 找出 可能 的 人脸 位置 ［ ～ ］ 其它 有 报道 的 实验 领域 还 包括 文本 识别 ［ ］ 、 人脸识别 ［ ］ 、 三维 物体 识别 ［ ］ 、 遥感 图像 分析 ［ ］ 等 　 　 在 函数 拟合 方面 ， 主要 实验 尚 属于 原理 性 研究 ， 包括 函数 逼近 、 时间 序列 预测 及 数据压缩 ［ ～ ］ 等 　 　 其它 有关 研究 还有 对 SVM 中 优化 算法 实现 的 研究 ［ ］ 、 改进 的 SVM 方法 ［ ］ 甚至 硬件 实现 研究 ［ ］ 等 讨论 　 　 由于 统计 学习 理论 和 支持 向量 机 建立 了 一套 较 好 的 有限 样本 下 机器 学习 的 理论 框架 和 通用 方法 ， 既有 严格 的 理论 基础 ， 又 能 较 好地解决 小 样本 、 非线性 、 高维 数 和 局部 极小 点 等 实际 问题 ， 因此 成为 九十年代 末 发展 最快 的 研究 方向 之一 ， 其 核心思想 就是 学习 机器 要 与 有限 的 训练样本 相适应 本文 对 它们 的 基本 思想 、 方法 及 研究 方向 进行 了 介绍 ， 希望 使 读者 对 这 一 领域 有 一个 基本 的 了解 统计 学习 理论 虽然 已经 提出 多年 ， 但 从 它 自身 趋向 成熟 和 被 广泛 重视 到 现在 毕竟 才 只有 几年 的 时间 ， 其中 还有 很多 尚未 解决 或 尚未 充分 解决 的 问题 ， 在 应用 方面 的 研究 更是 刚刚开始 我们 认为 ， 这是 一个 十分 值得 大力 研究 的 领域 本文 受到 国家自然科学基金 赞助 ， 项目编号 为 张 学工 年生 ， 年于 清华大学 获 博士学位 ， 现为 清华大学 自动化系 副研究员 ， 主要 研究 方向 为 模式识别 的 理论 与 方法 、 智能 信息处理 与 融合 技术 及其 在 地球物理 勘探 等 领域 中 的 应用 张 学工 清华大学 自动化系 ， 智能 技术 与 系统 国家 重点 实验室 北京 参考文献 ， VapnikVNEstimationofDependenciesBasedonEmpiricalDataBerlinSpringerVerlag ， VapnikVNTheNatureofStatisticalLearningTheoryNYSpringerVerlag 张 学工 译 统计 学习 理论 的 本质 北京 ： 清华大学出版社 ， （ 待 出版 ） ， CherkasskyVMulierFLearningfromDataConceptsTheoryandMethodsNYJohnVileySons ， 边肇祺 等 模式识别 北京 ： 清华大学出版社 ， ， VapnikVLevinELeCunYMeasuringtheVCdimensionofalearningmachineNeuralComputation ～ ， BurgesCJCAtutorialonsupportvectormachinesforpatternrecognitionDataMiningandKnowledgeDiscovery ， BoserBGuyonIVapnikVAtrainingalgorithmforoptimalmarginclassifiersFifthAnnualWorkshoponComputationalLearningTheoryPittsburghACMPress ， CortesCVapnikVSupportvectornetworksMachineLearning ～ ， SchlkopfBBurgesCVapnikVExtractingsupportdataforagiventaskInFayyadUMUthurusamyRedsProcofFirstIntlConfonKnowledgeDiscoveryDataMiningAAAIPress ～ ， VapnikVGolowichSSmolaASupportvectormethodforfunctionapproximationregressionestimationandsignalprocessingInMozerMJordanMPetscheTedsNeuralInformationProcessingSystemsMITPress ， M ü llerKRSmolaAJRtschGetalPredictingtimeserieswithsupportvectormachinesInProcofICANNSpringerLectureNotesinComputerScience ～ ， DruckerHBurgesCKaufmanLetalSupportvectorregressionmachinesInMozerMJordanMPetscheTedsNeuralInformationProcessingSystemsMITPress ， SchlkopfBSmolaAM ü llerKRKernelprincipalcomponentanalysisInProcofICANN ～ ， SchlkopfBSmolaAM ü llerKRNonlinearcomponentanalysisaskerneleigenvalueproblemNeuralComputation ～ ， SchlkopfBSungKKBurgesCetalComparingsupportvectormachineswithGaussiankernelstoradialbasisfunctionclassifiersIEEETransonSignalProcessing ～ ， SchlkopfBBurgesCVapnikVIncorporatinginvariancesinsupportvectorlearningmachinesInvonderMalsburgCvonSeelenWVorbr ü ggenJCetaledsArtificialNeuralNetworksICANNSpingersLectureNotesinComputerScienceBerlin ～ ， SchlkopfBSimardPSmolaAetalPriorknowledgeinsupportvectorkernelsNIPS ， GuyonIMaticNVapnikVDiscoveringinformativepatternsanddatacleaningInFayyadUMPiatetskyShapiroGSmythPetaledsAdvancesinKnowledgeDiscoveryDataMiningMITPress ～ ， BurgesCSchlkopfBImprovingtheaccuracyandspeedofsupportvectormachinesInMozerMJordanMPetscheTedsNeuralInformationProcessingSystemsMITPress ， MaticNGuyonIDenkerJetalWriteradaptationforonlinehandwrittencharacterrecognitionInndIntlConfonPatternRecognitionandDocumentAnalysis ～ ， OrenMPapageorgiouCSinhaPetalPedestriandetectionusingwavelettemplatesInProcofCVPRPuertoRico ， HearstMASchlkopfBDumaisSetalTrendsandcontroversiessupportvectormachinesIEEEIntelligentSystems ～ ， OsunaEFreundRGirosiFTrainingsupportvectormachinesanapplicationtofacedetectionInProcofCVPRPuertoRico ， 卢增祥 ， 李衍达 交互 SVM 学习 算法 及其 在 文本 信息 过滤 中 的 应用 清华大学 学报 ， （ 待 发表 ） ， LuChunyuYanPingfanZhangChangshuiZhouJieFacerecognitionusingsupportvectormachineInProcofICNNBBeijing ～ ， BlanzVSchlkopfBB ü lthoffHetalComparisonofviewbasedobjectrecognitionalgorithmsusingrealisticDmodelsInvonderMalsburgCvonSeelenWVorbr ü ggenJCetaledsArtificialNeuralNetworks — — ICANNSpingersLectureNotesinComputerScienceBerlin ～ ， BrownMLewisHGGunnSRLinearspectralmixturemodelsandsupportvectormachinesforremotesensingsubmittedtoIEEETransGeoscienceandRemoteSensing ， MukherjeeSOsunaEGirosiFNonlinearpredictionofchaotictimeseriesusingasupportvectormachineInProcofNNSP ， KwokJTYSupportvectormixtureforclassificationandregressionproblemsICPR ， BennettKMangasarianORobustlinearprogrammingdiscriminationoftwolinearlyinseparablesetsOptimizationMethodsandSoftware ～ ， OsunaEFreundRGirosiFAnimprovedtrainingalgorithmforsupportvectormachinesInProcofNNSP ， BennettKPDemirizASemisupervisedsupportvectormachinesInProcofNIPS ， ZhangXuegongUsingclasscentervectorstobuildsupportvectormachinesInProcofNNSP ～ ， AnguitaDRidellaSRovettaSCircuitalimplementationofsupportvectormachinesElectronicsLetters ～ 收稿 日期 收 修改稿 日期