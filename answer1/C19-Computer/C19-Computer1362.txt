　 自动化 学报 ACTAAUTOMATICASINICA 年 第卷 第期 volNo 多层 感知器 的 局部 化 设计 ） 许 　 力 　 　 摘 　 要 　 结合 前馈 网络 的 全局 化 设计 和 局部 化 设计 的 各自 特点 ， 提出 一种 由 多个 既 独立 又 关联 的 同构 子 网络 构成 的 局部 化 多层 感知器 仿真 结果表明 ， 该 网络 对 复杂 非线性 系统 具有 良好 的 学习 性能 ， 并 适用 于 学习 控制 的 直接 逆 模型 法 和 远程 学习 法 　 　 关键词 　 多层 感知器 ， 局部 化 设计 ， CMAC 网络 ALOCALDESIGNFORMULTILAYERPERCEPTRONXULIDepartmentofElectricalEngineeringZhejiangUniversityHangzhou 　 　 Abstract 　 BysyncretizingtheglobaldesignandlocaldesignoffeedforwardneuralnetworksthispaperproposesalocalizedmultilayerPerceptronLMLPwhichisconstructedbyasetofdistributivelyassociatedsubnetswiththesamestructureSimulationresultsshowthatthisnetworkhasstronglearningcapacityforcomplexnonlinearsystemsandisapplicableforrealtimelearningcontrolbyusingeitherdirectinversemodelingordistallearningstrategy 　 　 Keywords 　 MultilayerperceptronlocaldesignCMACnetwork 　 　 　 引言 　 　 全局 化 设计 和 局部 化 设计 是 前馈 网络 的 两种 不同 的 设计 方法 作为 全局性 网络 的 典型 代表 ， 多层 感知器 MLP ， 又称 BP 网 的 输出 受 所有权 值 的 影响 ， 因而 学习 缓慢 ， 较 适合 于 对 固定 样本 集 的 学习 ； 局部性 网络 以 CMAC ［ ］ 为 代表 ， 其 输出 在 任何时刻 只 与 部分 权值 即 一个 子 网络 相关 ， 所以 有 较 快 的 学习 速度 ， 较 适合 于 实时控制 ［ ］ 另一方面 ， 尽管 MLP 能 逼近 任意 Borel 可测 函数 ［ ］ ， 但 当 样本 在 局部 区域 存在 较大 差异 时 ， 网络 的 训练 通常 变得 非常 困难 ［ ］ ； CMAC 虽能 较 好 地 处理 学习 的 局部性 问题 ， 但 因 其 输入 只是 用来 确定 各子 网络 存贮 单元 的 地址 ， 而 与 输出 值 无 直接 关系 这样 ， CMAC 对 每个 输入 子 空间 中 的 信号 进行 “ 多到 一 ” 的 映射 ， 因而 存在 着 所谓 “ 分辨率 ” 的 问题 ［ ］ 同时 ， 这种 映射 使得 输出 误差 难以 反向 传播 到 输入 端 ， 所以 不能 用于 远程 学习 控制 ［ ］ 从 理论 上 讲 ， 一种 得益于 这 两种 设计 方法 的 混合型 结构 是 存在 的 ， 但 其 具体 实现 有待 研究 ［ ］ 　 　 这里 提出 的 局部 化 多层 感知器 LocalizedMultiLayerPerceptronLMLP 融合 MLP 与 CMAC 的 各自 特点 ， 是 对 文献 ［ ］ 提出 的 初步 设想 的 一种 完善 文中 着重 研究 该 网络 在 复杂 非线性 系统 的 在线 学习 控制 中 的 应用 　 　 　 局部 化 的 多层 感知器 　 　 　 LMLP 的 结构 和 原理 　 　 设计 的 基本 思想 是 ， 由 N 个 输入 变量 xx … ， xN 构成 的 输入 空间 S 被 剖 分为 n 个 输入 子 空间 SS … ， Sn ， 不同 的 子 空间 用 不同 的 子 网络 来 处理 ， 而 每个 子 网络 均 采用 MLP 或 其 类似 形式 整个 网络 就 由 这 n 个 既 相互 关联 又 相对 独立 的 子 网络 SN ， SN ， … ， SNn 构成 　 　 在 图 和 所示 的 原理图 中 ， LMLP 具有 N 个 输入 单元 、 m 个 隐 单元 和 M 个 输出 单元 每个 隐 单元 只能 处于 两种 状态 之一 ： “ 激活 ” 或 “ 禁止 ” 每个 子 网络 占用 m 个 隐 单元 ， 不同 子 网络 的 差异 仅 在于 其 m 个 隐 单元 在 整个 隐含 层中 的 位置 不同 ， 而 位置 由 网络 协调 器来 确定 图 　 LMLP 原理图 图 　 网络 协调 器 原理图 　 　 设 每个 输入 变量 被 划分 成 Rjj … ， N 个 不同 的 区域 ， 则 由 N 个 输入 变量 构成 n 个子 空间 ， 其中 nR × R × … × RN 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 每个 子 空间 对应 一个 子 网络 ， 而 每个 子 网络 对应 m 个 隐 单元 图中 m 以 V 表示 按 Si → mi … n 顺序 映射 而 建立 的 虚拟 隐含 层 位置 表 ， Pij 表示 子 网络 SNi 中 第 j 个 隐 单元 的 位置 ， 则 ｛ PiPi ， … ， Pim ｝ V 就 表示 SNi 中 所有 隐 单元 在 V 中 的 位置 集合 ， 记 为 ｛ SNi ｝ V 在 图 中 ， ｛ SN ｝ V ｛ ， ， ｝ V ， ｛ SN ｝ V ｛ ， ， ｝ V ， ｛ SNn ｝ V ｛ mVmVmV ｝ V ， 这里 mV 是 V 中 位置 单元 的 总数 　 　 设 相邻 子 网络 中 不同 的 隐 单元 个数 为 ρ 图中 　 ρ 则 mV 应 满足 mV ≥ n ρ m 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 显然 ， 其 数值 可能 会 非常 大 ， 为 使 实际 隐含 层 H 的 单元 个数 m 不至于 太 大 ， 这里 采用 在 CMAC 中 已有 成功 应用 的 杂凑 编码 hashcoding 技术 ［ ］ 来 实现 从 mV 到 m 的 “ 多到 少 ” 的 映射 于是 ， SNi 的 隐 单元 在 实际 隐含 层中 的 位置 ｛ SNi ｝ H ｛ Pi ， Pi … ， Pim ｝ H 可 由 下列 映射 确定 Si → ｛ SNi ｝ V → ｛ SNi ｝ H ， i … n 这样 ， 子 网络 SNi 就 由 ｛ SNi ｝ H 所 确定 的 隐 单元 、 连同 为 所有 子 网络 公用 的 输入 层 和 输出 层 构成 例如 ， 在 图 中 ｛ SN ｝ H ｛ ， ， m ｝ H ， ｛ SN ｝ H ｛ ， mm ｝ H ， ｛ SNn ｝ H ｛ ， ， m ｝ H 显然 ， SN 与 SN 具有 二个 公共 单元 ， 而 SN 与 SNn 公用 一个 隐 单元 图 的 输出 就是 确定 SN 的 隐 单元 位置 时 的 情形 　 　 综上所述 ， 在 任何时刻 ， 只有 m 个 隐 单元 被 网络 协调 器 “ 激活 ” ， 即 只有 一个 子 网络 在 工作 ， 其余 的 隐 单元 均 被 “ 禁止 ” 各子 网络 相对 独立 ， 没有 两个 子 网络 完全 一样 ； 又 没有 一个 子 网络 是 完全 独立 的 ； 整个 网络 呈 一种 部分 连接 状态 ， 兼顾 了 学习 的 分断性 dichotomization 和 记忆 的 联想 性 generalization 有关 参数 的 取值 ， 建议 取 m ≈ m ， 而 m 的 数值 视 具体情况 而定 　 　 本文 忽略 杂凑 映射 可能 产生 的 映射 冲突 ， 有关 这方面 的 讨论 详见 文 ［ ， ］ 各子 网络 的 隐 单元 个数 不 一定 要 相同 ， 限于 篇幅 ， 在 此 不作 讨论 另外 ， 参量 ρ 可取 其它 数值 ， 这里 只 考虑 ρ 的 情况 　 　 　 LMLP 的 两种 形式 　 　 LMLP 的 子 网络 采用 MLP 或 其 类似 形式 ， 这里 考虑 两种 极端 情形 ： aLMLPI ， 其子 网络 采用 常规 的 多 输入 多 输出 MIMO 的 MLP ； bLMLP Ⅱ ， 其子 网络 采用 多 输入 单 输出 MISO 的 MLP 结构 　 　 常规 MLP 是 指 输入 层 和 输出 层均 采用 等值 函数 作为 激发 函数 ， 而 隐含 层则 采用 Sigmoid 函数 ， 其 结构 和 算法 在 许多 文献 中 已有 介绍 　 　 下面 介绍 LMLP Ⅱ 的 子 网络 具有 一个 隐含 层 的 常规 MLP 有 两组 权值 ， 分别 连接 输入 隐含 层 和 隐含 输出 层 ， 计算 量 相对 较大 同时 ， 许多 实际 系统 是 MISO 的 ， 而 即便 是 MIMO 系统 也 可用 多个 MISO 系统 来 表达 所以 ， LMLP Ⅱ 的 子 网络 采用 只有 一组 可调 权值 的 MISO 结构 ， 连接 隐含 输出 层 的 权值 不作 修正 根据 梯度 法 并 参考 CMAC 的 工作 原理 ， 可得 其 算法 如下 ： 　 　 前向 计算 ， 　 　 反向 修正 ， δ jy － ymwijwij α δ jxi ， θ j θ j β δ j 其中 　 xi 为 第 i 个 输入 变量 ， yj 为 第 j 个 隐 单元 的 输出 ， 而 wij 为 连接 第 i 个 输入 单元 和 第 j 个 隐 单元 的 权值 ， α 和 β 是 学习 因子 由于 需 修正 的 权值 只有 一组 ， 且 各 单元 均 采用 线性 激发 函数 ， 因而 可 显著 减少 计算 量 　 　 需要 说明 的 是 ， 这里 只 列举 了 子 网络 的 两种 极端 情形 ， 而 事实上 ， 对 其它 MLP 类 的 形式 也 同样 适用 　 　 　 基于 LMLP 的 学习 控制 　 　 监督 式 学习 控制策略 可 分为 两大类 ［ ］ ， 即 直接 逆 模型 法 ［ ， ］ 和 远程 学习 法 DistalLearning ［ ］ 其 作用 都 是 要 寻找 被控 对象 从 状态 空间 到 控制 空间 的 逆 模型 直接 逆 模型 法 存在 “ 中凸性 ” Convexity 问题 ［ ］ ， 即 被控 对象 是 关于 控制 空间 到 状态 空间 的 “ 多到 一 ” 的 映射 ， 它 只能 给出 关于 控制 空间 某个 区域 的 平均值 ， 所以 可能 会 找 不到 逆 模型 ； 而 远程 学习 法 求得 的 是 某个 具体 的 逆 模型 　 　 在 上述 两种 控制 方法 中 ， LMLP 的 应用 没有 任何 特殊要求 ， 相应 地用 LMLP 代替 文 ［ ， ］ 中 的 CMAC 或文 ［ ］ 中 的 BP 网 即可 这里 采用 两条 辅助 措施 ： 给 神经网络 控制器 叠加 一个 简单 的 比例 控制器 ； 引入 极值 控制 ， 当 输出 误差 超出 某个 允许 范围 时 ， 就 采取 最大 或 最小 的 控制 量 　 　 仿真 用 被控 对象 是 化工 过程 CSTR ［ ］ ， 其 无量 纲化 数学模型 为 其中 　 Da β γ H 状态变量 x 和 x 分别 为 反应 转化率 和 反应 温度 ； 控制变量 u 和 u 分别 代表 进料 流量 和 冷却剂 温度 ， u 通常 固定 为 ； x 又 是 输出 变量 ， 反映 了 产品 的 质量 　 　 　 仿真 计算 　 　 以 CSTR 系统 为例 ， 研究 LMLP 在 学习 控制 中 的 应用 跟踪 的 目标 是 上 、 下幅值 分别 为 和 的 方波 控制 周期 T 为 秒 ， 学习 周期 为个 控制 周期 ， 比例控制 系数 为 ， 极值 控制 的 阈值 为 ± ， 网络 输入 变量 xx Δ xu 分别 划分 为 ， ， ， 个子 区域 m ， m 统一 选 为 ， 网络 权值 都 初始化 为 小 的 随机数 ， 作为 对照 的 MLP 采用 个 隐 单元 　 　 　 直接 逆 模型 法 　 　 由 xx Δ x 构成 的 控制器 网络 的 输入 空间 被 划分 为 × × 个子 空间 图 所示 为 前个 学习 周期 的 跟踪 情况 由于 “ 中凸性 ” 问题 ， 基于 CMAC 的 系统 在 第四 、 五个 学习 周期 下幅值 处 的 响应 偏离 已经 跟踪 上 的 期望值 ， 类似 的 情况 也 出现 在 LMLP Ⅰ 中 基于 LMLP Ⅱ 的 系统 ， 则 从 第二个 学习 周期 起 就 一直 表现 出 良好 的 跟踪 性能 图 　 直接 逆 模型 法 　 　 　 远程 学习 法 　 　 控制器 网络 和 前 向 网络 的 输入 分别 为 xx Δ x 和 xxu ， 各 被 划分 成 个子 空间 图 表明 ， 远程 学习 法能 很 好地解决 “ 中凸性 ” 问题 ， 因而 基于 LMLP 两种 形式 的 控制系统 均 表现 为 逐步 改善 的 跟踪 性能 其中 基于 LMLP Ⅰ 的 系统 从 第四个 周期 起 完全 跟上 ， 优于 LMLP Ⅱ 图 表明 ， 基于 常规 MLP 的 控制系统 在 跟踪 方波 时 存在 着 困难 图 　 远程 学习 法图 　 基于 MLP 的 学习 控制 　 　 　 讨论 　 　 LMLP 采用 类似 于 CMAC 确定 存贮 单元 地址 的 方法 来 确定 各子 网络 的 隐 单元 在 隐含 层中 的 位置 ， 其子 网络 采用 MLP 类 的 形式 所以 ， 对 各 输入 子 空间 内 的 信号 均 实现 “ 一到 一 ” 的 映射 其中 LMLP Ⅰ 的 子 网络 实现 非线性 映射 ， 而 LMLP Ⅱ 的 子 网络 则 为 线性 逼近 ， 但 计算 量 小 从总体上 讲 ， LMLP Ⅰ 的 非线性 逼近 能力 强于 LMLP Ⅱ ， 但是 ， 当 对象 的 输入输出 在 各子 空间 呈 线性 或 近似 线性关系 时 ， LMLP Ⅱ 的 学习 能力 可能 强于 LMLP Ⅰ 　 　 MLP 的 完全 联结 机制 ， 使 其 动态 响应 能力 较弱 ， 对 跳变 样本 的 学习 缓慢 CMAC 充分考虑 学习 的 局部性 问题 ， 因而 有 较 强 的 动态 响应 能力 ， 但 其 学习 能力 与 分辨率 密切相关 由于 采用 相似 的 子 网络 分配原则 ， LMLP 与 CMAC 均 表现 为 相似 的 动态 响应 能力 ， 但 LMLP 的 “ 一到 一 ” 的 映射 机理 使 它 具有 更强 的 学习 能力 对于 LMLP 子 网络 的 结构 和 关联 形式 仍 需 做 进一步 地 研究 国家自然科学基金 和 国防科委 预研 基金 资助 课题 作者简介 ： 许 　 力 　 年生 ， 分别 于 年 和 年 在 浙江大学 电机系 获学士 和 硕士学位 ， 现为 浙江大学 电机系 副教授 、 博士生 主要 研究 领域 是 专家系统 、 模糊控制 、 神经网络 和 学习 控制 作者 单位 ： 浙江大学 电机系 　 杭州 　 参考文献 　 ［ ］ WerbosPJNeurocontrolandelasticfuzzylogiccapabilitiesconceptandapplicationsIEEETransIndustrElectronics 　 ［ ］ AlbusJSAnewapproachtothemanipulatorcontrolThecerebellarmodelarticulationcontrollerTransASMEJDynSystMeasContr 　 ［ ］ AlbusJSDatastorageinthecerebellarmodelarticulationcontrollerTransASMEJDynSysMeasContr 　 ［ ］ Miller Ⅲ WTRealtimeapplicationofneuralnetworksforsensorbasedcontrolofrobotswithvisionIEEETransSMC 　 ［ ］ KnuthDSortingandsearchingTheArtofComputerProgrammingAddison 　 WesleyhenloParkCalif 　 ［ ］ JordanMIRumelhartDEForwardmodelssupervisedlearningwithadistalteacherCognitiveSci 　 ［ ］ HornikKMultilayerfeedforwardnetworksareuniversalapproximatorsNeuralNetworks 　 ［ ］ JacobsRAJordanMILearningpiecewisecontrolstrategiesinamodularneuralnetworkarchitectureIEEETransSMC 　 ［ ］ WongYFSiderisALearningconvergenceinthecerebellarmodelarticulationcontrollerIEEETransNeuralNetworks 　 ［ ］ ChowMYMenozziAAselforganizedCMACcontrollerProcIEEEIntlConfonIndustrTechGuangzhouChinaDec 　 ［ ］ XuLJiangJPZhuJSupervisedlearningcontrolofanonlinearpolymerisationreactorusingtheCMACneuralnetworkforknowledgestorageIEEProcControlTheoryAppl 　 ［ ］ 许力 一种 局部 化 的 反向 传播 网络 控制 与 决策 ， ， ： 收稿 日期 　