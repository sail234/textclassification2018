计算机 研究 与 发展 JOURNALOFCOMPUTERRESEARCHANDDEVELOPMENT 年 第卷 第期 VolNoQ 学习 及其 在 智能 机器人 局部 路径 规划 中 的 应用 研究 张汝波 　 杨广铭 　 顾国昌 　 张国 印摘 　 要 　 强化 学习 一词 来自 于 行为 心理学 ， 这门 学科 把 行为 学习 看成 反复 试验 的 过程 ， 从而 把 环境 状态 映射 成 相应 的 动作 在 设计 智能 机器人 过程 中 ， 如何 来 实现 行为主义 的 思想 、 在 与 环境 的 交互 中 学习 行为 动作 ？ 文中 把 机器人 在 未知 环境 中为 躲避 障碍 所 采取 的 动作 看作 一种 行为 ， 采用 强化 学习 方法 来 实现 智能 机器人 避碰 行为 学习 Q 学习 算法 是 类似 于 动态 规划 的 一种 强化 学习 方法 ， 文中 在 介绍 了 Q 学习 的 基本 算法 之后 ， 提出 了 具有 竞争 思想 和 自 组织 机制 的 Q 学习 神经网络 学习 算法 ； 然后 研究 了 该 算法 在 智能 机器人 局部 路径 规划 中 的 应用 ， 在 文中 的 最后 给出 了 详细 的 仿真 结果 关键词 　 Q 学习 ， 神经网络 ， 智能 机器人 ， 局部 路径 规划 中图法 分类号 　 TPQLEARNINGANDITSAPPLICATIONINLOCALPATHPLANNINGOFINTELLIGENTROBOTSZHANGRuBoYANGGuangMingGUGuoChangandZHANGGuoYinDepartmentofComputerandInformationScienceHarbinEngineeringUniversityHarbinAbstract 　 TheconceptofreinforcementlearningcomesfrombehaviorpsychologythattakesbehaviorlearningastrialanderrorbywhichthestatesofenvironmentaremappedintocorrespondingactionsTheresaquestionofhowthebehaviorismcanbeusedtolearntheactionsininteractionwiththeenvironmentindesigningintelligentrobotsInthispapertheactionsthatarobottakestoavoidobstaclesaretakenasoneclassofbehaviorsandthereinforcementlearningisusedtorealizebehaviorlearningofobstacleavoidanceQlearningisonekindofreinforcementlearningmethodthatissimilartodynamicprogrammingAfterbasicideasofQlearningareintroducedaneuralnetworklearningalgorithmofQlearningwithconceptsofcompetitionandselforganizationispresentedItsapplicationinlocalpathplanningofintelligentrobotsisalsointroducedFinallythedetailedsimulationresultsarepresentedKeywords 　 Qlearningneuralnetworkintelligentrobotlocalpathplan 　 引 　 　 言 　 　 在 连接 主义 学习 中 ， 学习 算法 基本上 可以 分为 种 类型 ， 非 监督 学习 unsupervisedlearning 、 监督 学习 supervisedlearning 和 强化 学习 reinforcementlearning 　 　 非 监督 学习 规则 在 生理学 上 就是 Pavlov 的 条件反射 原理 ， 当 用 一个 毫无意义 的 刺激 信号 （ 如铃 的 响声 ） 同时 伴有 另 一个 刺激 信号 （ 如 食物 ） 反复 加 给 动物 的 时候 ， 经过 一段时间 的 训练 后 ， 动物 就 会 建立 一种 联想 当 再 接受 到 刺激 信号 时 ， 动物 就 会 产生 条件反射 这种 类型 的 学习 完全 是 开环 的 ， 在 神经网络 学习 中 ， 称之为 相关 规则 ， 即 神经网络 中 的 Hebb 学习 规则 　 　 监督 学习 规则 是 一种 反馈 学习 规则 ， 当 输入 信号 作用 于 系统 后 ， 观察 其 输出 ， 由 教师 提供 理想 的 输出 信号 ， 所 产生 的 误差 信号 反馈 给 系统 来 指导 学习 ， 在 神经网络 学习 中 ， 称之为 最小 误差 学习 规则 或 称之为 δ 规则 ［ ］ 　 　 观察 生物 （ 特别 是 人 ） 为 适应环境 的 学习 过程 可以 发现 它 有 两个 特点 ， 一是 人 从来不 是 静止地 被动 等待 而是 主动 对 环境 作 试探 ， 二 是从 环境 对 试探 动作 通过 的 反馈 信号 看 ， 多数 情况 下 是 评价性 （ 奖 或 罚 ） 的 ， 而 不是 像 监督 学习 那样 给出 正确 答案 生物 在 行动 － － 评价 的 环境 中 获得 知识 ， 改进 行动 方案 以 适应环境 达到 预想 的 目的 具有 上述 特点 的 学习 就是 强化 学习 （ 或称 再励 学习 ， 评价 学习 ， 简记 为 RL ） ［ ］ 　 　 Q 学习 算法 是 由 Watkins 在 年 提出 的 类似 于 动态 规划 算法 的 一种 强化 学习 方法 它 提供 智能 系统 在 马尔科夫 环境 中 利用 经历 的 动作 序列 选择 最优 动作 的 一种 学习 能力 ， 并且 不 需要 建立 环境 模型 Q 学习 算法 实际 是 MDP （ Markovdecisionprocesses ） 的 一种 变化 形式 Watkins 采用 lookup 表来 表示 输入 状态 ， 证明 了 Q 学习 的 收敛性 ［ ］ Szepesv á ri 在 一定 条件 下 证明 了 Q 学习 的 收敛 速度 ［ ］ Williams 等 人 采用 Q 学习 算法 对 倒 摆 系统 进行 实验 研究 ， 并 与 Anderson 等 人 采用 AHC 方法 进行 了 比较 分析 ［ ］ 由于 自身 的 特性 ， 强化 学习 被 广泛 地 应用 在 智能 控制 领域 ， 许多 学者 都 取得 了 令人满意 的 成果 Beom 利用 模糊 逻辑 和 强化 学习 实现 了 陆上 移动机器人 导航系统 ， 机器人 通过 学习 能够 在 未知 的 环境 中 运动 ， 可以 完成 避碰 和 到达 指定 目标 点 两种 行为 ［ ］ Winfried 采用 强化 学习 来 使 昆虫 机器人 学会 条腿 的 协调 动作 ［ ］ CarnegieMellon 大学 的 Sebastian 采用 神经网络 结合 强化 学习 方式 使 机器人 通过 学习 能够 到达 室内环境 中 的 目标 ［ ］ 　 　 本文 在 介绍 了 Q 学习 的 基本 算法 之后 ， 提出 了 具有 竞争 思想 和 自 组织 机制 的 Q 学习 神经网络 学习 算法 ； 然后 研究 了 该 算法 在 智能 机器人 局部 路径 规划 中 的 应用 情况 ， 即 把 机器人 在 未知 环境 中为 躲避 障碍 所 采取 的 动作 看作 一种 行为 ， 采用 Q 学习 机制 进行 机器人 的 避 碰 行为 学习 ， 在 文中 的 最后 给出 了 详细 的 仿真 结果 　 Q 学习 的 基本 算法 ［ ］ 　 　 设 环境 是 一个 有限 状态 的 离散 马尔科夫 过程 ， 智能 系统 每步 可 在 有限 动作 集合 中 选取 某一 动作 ， 环境 接受 该 动作 后 状态 发生 转移 ， 同时 给出 评价 r 例如 ， 在 时刻 t 选择 动作 at ， 环境 由 状态 st 转移 到 st ， 给出 评价 rt ， rt 及 st 的 概率分布 取决于 at 及 st 环境 状态 以 如下 概率 变化 到 stprob ［ sssstat ］ P ［ statst ］ 智能 系统 面临 的 任务 是 决定 一个 最优 策略 ， 使得 总 的 折扣 奖励 信号 期望值 最大 在 策略 π 的 作用 下 ， 状态 s 的 值 为 由于 智能 系统 希望 立即 收到 强化 信号 r π x ， 然后 以 概率 移动 到 一个 赋值 为 V π st 的 一个 状态 动态 规划 理论 保证 至少 有 一个 策略 π 使得 　 　 Q 学习 面临 的 任务 是 在 初始条件 未知 的 情况 下来 决定 π Watkins 把 Q 学习 看成 一个 增量 式 动态 规划 ， 用 一步 方式 来 决定 策略 希望 找到 一个 策略 （ 动作 序列 ） 使 评价 总和 得到 最大 如果 环境 模型 （ 即 状态 转移 概率 及 评价 模型 ） 已知 或 由 观测 估计 出来 ， 则 上述 问题 可用 动态 规划 （ DP ） 解决 ， Q 学习 的 思想 的 是 不 去 估计 环境 模型 ， 而是 直接 优化 一个 可 迭代 计算 的 Q 函数 ， Watkins 定义 此 Q 函数 为 在 状态 st 时 执行 动作 at ， 且 此后 按 最优 动作 序列 执行 时 的 折扣 累计 强化 值 ， 即 　 　 在 Q 学习 中 ， 智能 系统 的 经历 包括 一系列 不同 的 阶段 ， 在 每个 阶段 ， 其 学习 步骤 如下 ： 　 　 观察 现在 的 状态 st 　 　 选择 并 执行 一个 动作 at 　 　 观察 下 一个 状态 st 　 　 收到 一个 立即 强化 信号 rt 　 　 调整 Q 值 ： 其中 　 　 在 初始 阶段 学习 中 ， Q 可能 是 不 准确 地 反映 了 它们 所 定义 的 策略 ， 初始值 Qsa 对于 所有 的 状态 和 动作 假定 是 给出 的 Watkins 证明 了 Q 学习 在 一定 条件 下 的 收敛性 收敛 的 条件 是 ： 　 　 环境 是 马尔科夫 过程 ； 　 　 用 lookup 表来 表示 Q 函数 ； 　 　 每个 状态 动作 对 ， 可 无限 次地 重复 试验 ； 　 　 学习 速率 的 正确 选择 　 　 定理 给定 有 界 强化 信号 rt ≤ R ， 学习 率 ≤ α t ≤ 及 则 ： 当 t → ∞ 时 ， Qtsa 以 概率 收敛 于 最优 Qsa 　 　 Q 函数 的 实现 方法 主要 有 两种 方式 ： 一种 是 采用 神经网络 方法 ； 另 一种 是 采用 lookup 表格 方法 ； 采用 lookup 表格 方法 ， 也 就是 利用 表格 来 表示 Q 函数 ， 设 Q （ sa ） （ s ∈ S ， a ∈ A ） 为 一 lookup 表格 ， S ， A 为 有限 集合 Qsa 代表 s 状态 下 执行 动作 a 的 Q 值表 的 大小 等于 S × A 的 笛卡尔 乘积 中 元素 的 个数 当 环境 的 状态 集合 S 、 智能 系统 可能 的 动作 集合 A 较大 时 ， Qsa 需要 占用 大量 的 内存空间 ， 而且 也 不 具有 泛化 能力 　 　 采用 神经网络 实现 Q 学习 时 ， 网络 的 输出 对应 每个 动作 的 Q 值 ， 网络 的 输入 对应 描述 环境 的 状态 采用 神经网络 实现 Q 学习 后 ， 可以 克服 上述 存在 的 问题 　 Q 学习 的 神经网络 实现 　 Q 学习 系统 的 结构 　 　 Q 学习 系统结构 不同于 AHC 算法 的 结构 ， 采用 Q 学习 的 智能 系统 只有 一个 决策 单元 ， 同时 起到 动作 的 评价 及 选择 作用 ， 其 结构 如图所示 图 　 Q 学习 系统结构 　 　 如果 一个 智能 系统 要 想 获取 较 高 的 强化 值 ， 在 每个 状态 ， 智能 系统 不得不 选择 具有 最高 Q 值 的 动作 ， 特别 是 在 学习 的 初始 阶段 ， 对 状态 动作 的 经验 了解 的 比较 少 ， Q 值 不能 准确 地 表示 正确 的 强化 值 通常 ， 选择 最高 Q 值 的 动作 导致 了 智能 系统 总是 沿着 相同 的 路径 而 不 可能 探索 到 较 好值 因此 ， 基于 上述情况 ， 智能 系统 必须 随机 地 选择 动作 ， 根据 当前 的 Q 值 也许 选择 的 动作 不是 最优 的 有 许多 方法 来 随机 地 选择 动作 如 Boltzmann 分布 方法 ： 　 　 其中 T 为 温度 值 ； T 的 大小 代表 了 随机性 的 大小 T 越大 ， 随机性 越大 在 学习 的 初试 阶段 ， T 取较 高 的 值 ， 因为 此时 学习 的 经验 较 小 ， 需要 增加 搜索 能力 ； 在 学习 的 过程 中 ， 逐渐 降低 温度 ， 保证 以前 的 学习效果 不 被 破坏 　 　 Q 学习 可用 各种 神经网络 来 实现 ， 每 一个 网络 的 输出 对应 于 一个 动作 的 Q 值 ， 即 ： Qsai 用 神经网络 实现 Q 学习 的 关键 是 学习 算法 的 确定 根据 Q 函数 的 定义 ： 只有 在 得到 最优 策略 的 前提 下 上式 才 成立 在 学习 阶段 上 式 两边 不 成立 ， 误差 信号 为 其中 ， Qstat 表示 下 一 状态 所 对应 的 Q 值 ， 其中 Δ Q 通过 调整 网络 的 权值 调整 使 误差 尽可能 小 一些 　 综合 神经网络 的 Q 学习 算法 　 　 这里 我们 结合 竞争 神经网络 和 自 组织 映射 网络 的 思想 实现 Q 学习 算法 网络 的 结构 如图所示 ： 图 　 综合 神经网络 　 　 网络 由层 组成 ， 即 输入 层 、 中间层 、 输出 层 竞争 组织 层 其 特点 主要 体现 在 竞争 组织 层上 网络 的 输出 节点 对应 一系列 的 动作 和 状态 的 评 价值 QSaj 这里 对 动作 按 一定 的 顺序排列 ， 即 ai 与 ai 是 相近 的 动作 其 工作 原理 是 这样 的 ： 在 状态 Stss … sn 输入 时 ， 网络 正向 传播 产生 相应 的 输出 QSaj ， 随机 动作 选择 单元 根据 QSaj 值 随机 选取 动作 假设 动作 ai 被 选中 ， 在 Q 学习 中 ， 其中 ， 根据 竞争 思想 令 Qmax 　 　 另外 ， 动作 ai 被 选中 后 ， 其 相邻 的 动作 也 产生 一定 的 权值 的 调整 基于 自 组织 映射 的 思想 ， 对同 一种 输入 应有 多个 动作 反应 ， 只不过 反应 的 程度 不同 而已 ， 这里 我们 采用 正态分布 的 形式 ， 来 确认 相邻 动作 的 反应 强度 ： 　 　 若 网络 输出 节点 对应 一系列 的 相反 动作 ， 如 a 与 am 、 a 与 am 等 都 是 相互 对立 的 动作 ， 则 随机 单元 选取 一 动作 ai 后 ， 其 误差 为 Δ Qmax ， 而 相反 的 动作 ami 的 误差 为 Δ Qmaxami 相邻 的 动作 对应 按下式 修改 ： 　 　 这样 ， 每次 学习 时 就 产生 多个 节点 的 误差值 网络 的 目标 函数 是 　 　 利用 误差 反向 传播 算法 来 进行 网络 的 权值 调整 　 Q 学习 在 智能 机器人 局部 路径 规划 中 的 应用 研究 　 强化 值 的 确定 　 　 强化 信号 的 作用 是 对 学习 系统 性能 的 一种 评价 ， 主要 用于 改善 系统 的 性能 强化 信号 根据 控制 任务 的 不同 ， 其 形式 有所不同 在 机器人 避碰 行为 学习 中 ， 其 目的 是 使 机器人 离 障碍物 越远 越 好 当 机器人 与 障碍物 相碰 时 ， 应该 得到 惩罚 在 我们 研究 的 机器人 中 ， 共 配置 了 个 声纳 用于 避碰 ， 如图所示 应该 综合 考虑 这个 声纳 的 距离 信息 ， 有 的 探测 距离 较 远 ， 有 的 探测 距离 较近 图 　 机器人 与 障碍物 的 关系 　 　 这里 我们 采用 势场 法来 确定 外部 强化 值 首先 计算 机器人 所 受 斥力 的 合力 ： 其中 ki 为 比例 系数 ； ρ i 为 第 i 声纳 的 探测 距离 ； ρ 为 安全 距离 ， ρ mi 为 声纳 的 最大 探测 距离 ； θ i 为 声纳 i 与 环境 坐标系 的 夹角 体现 了 机器人 距 障碍物 的 综合 相对 位置 关系 较大 则 表明 机器人 总体 上离 障碍物 较近 ， 反之 较 远 相邻 时刻 的 受力 之差 为 表明 机器人 运动 的 趋势 Δ Ft ＜ 表明 机器人 远离 障碍物 ， 应该 得到 奖励 ， Δ Ft ＞ 表示 机器人 走近 障碍物 ， 因该 得到 惩罚 故 强化 信号 rt 为 rtg ［ Δ Ft ］ gx 函数 取为 在 实验 中 ， ki ρ ［ i ］ ρ m ［ i ］ 另外 ， 当 机器人 与 障碍物 相碰 时 rt 　 输入 、 输出 的 表示 形式 　 　 在 我们 研究 的 机器人 系统 中 ， 避碰 的 主要 传感器 是 避 碰 声纳 因此 ， 作为 强化 学习 系统 的 输入 信号 就是 这个 声纳 传感器 的 距离 信息 ， 对于 声纳 信息 ， 我们 划分 个 等级 ， 即 Danger ， Near ， Middle ， Far ， 对于 前 方向 声纳 划分 如下 ： 对于 侧向 声纳 ： 把 机器人 的 旋转 角度 划分 为个 离散 动作 ， 即 ° ± ° ± ° ± ° 机器人 根据 传感器 的 信息 ， 经过 学习 后 选择 合适 动作 令 a ° a ° a ° a ° a ° a ° a ° 其中 ： a 与 a ， a 与 a ， a 与 a 互为 对立 动作 为了 使 机器人 尽快 学习 避碰 能力 ， 让 机器人 在 比较复杂 的 环境 中以 漫游 方式 运动 ， 当 机器人 与 障碍物 相碰 时 ， 回到 起点 重新 开始 学习 ， 即 在 上次 学习 结果 的 基础 上 重新 进行 权值 的 调整 我们 用 机器人 漫游 所 经过 的 路径 来 衡量 学习效果 的 好坏 ， 机器人 所 走 的 路径 越长 表明 机器人 避碰 能力 越强 　 基于 Q 学习 的 机器人 避碰 实验 结果 　 　 基于 Q 学习 的 机器人 避碰 动作 学习 系统 采用 层 神经网络 来 实现 网络 的 输入 为个 节点 ， 输出 为个 节点 ， 中间层 选择 个 节点 将 动作 进行 有序 的 排列 ， 而 转角 动作 以零为 中心 ， 均 为 互相 对称 的 动作 每次 学习 时 ， 随机 动作 选择器 根据 Borlzmann 分布 随机 选取 一个 动作 为了 方便 更 清晰 的 观察 机器人 的 学习效果 ， 我们 采用 了 对 多次 试验 的 统计 方法 来 描述 曲线 ， 其 实验 曲线 如图所示 图 　 实验 结果 　 　 综合 Q 学习 算法 （ 实线 ） 的 学习 速度 明显提高 ， 且 平均 路径 长度 远远 大于 标准 的 Q 学习 算法 （ 虚线 ） 从 曲线 上 可以 看出 ， 采用 标准 的 神经网络 Q 学习 方法 ， 其 学习 的 时间 比较 长 ， 而且 没有 达到 稳定 的 效果图 为 该 算法 在 学习 初试 的 学习 轨迹 其 轨迹 比较 杂乱 ， 不 平滑 ， 好像 无头 苍蝇 到处 乱撞 图为 经过 一段时间 学习 后 的 轨迹 ， 其 轨迹 比较 平滑 图 　 机器人 学习 初始 阶段 的 运动 轨迹 图 　 机器人 经过 一段时间 学习 后 的 运动 轨迹 　 　 为了 进一步 检验 学习效果 ， 让 训练 好 的 机器人 在 另 一 环境 中 漫游 （ 不再 学习 ） ， 图为 漫游 的 结果 考虑 到 局部 路径 规划 的 问题 ， 让 机器人 在 杂乱 的 环境 中 到达 指定 的 目标 点 ， 其 仿真 的 结果 如图所示 图 　 机器人 在 迷宫 中 运动 的 仿真 结果 图 　 基于 Q 学习 的 局部 路径 规划 结果 　 　 从 仿真 结果 可以 看出 ， 具有 Q 学习 机制 的 机器人 经过 学习 后 可以 在 比较复杂 的 环境 中 运动 ， 同时 可以 实现 局部 路径 规划 任务 　 结 　 　 论 　 　 本文 提出 了 具有 竞争 思想 和 自 组织 机制 的 Q 学习 神经网络 学习 算法 ， 与 标准 的 Q 学习 方法 相比 ， 学习 速度 大大提高 采用 该 算法 的 机器人 通过 学习 后 具有 较强 的 避 碰 能力 ， 从而 提高 了 机器人 对 环境 的 适应能力 该 算法 同样 可以 应用 到 其它 的 智能 控制系统 中 作者简介 ： 张汝波 ， 男 ， 年月生 ， 副教授 ， 博士 ， 主要 研究 方向 为 机器 学习 、 智能 控制 及 　 　 　 　 　 智能 机器人 　 　 　 　 　 杨广铭 ， 男 ， 年月生 ， 硕士 研究生 ， 主要 研究 方向 为 机器 学习 及 智能 机器人 　 　 　 　 　 顾国昌 ， 男 ， 年月生 ， 博士生 导师 ， 主要 研究 方向 为 智能 机器人 、 机器人 体系 　 　 　 　 　 结构 、 行动 决策 和 控制技术 　 　 　 　 　 张国印 ， 男 ， 年月生 ， 博士 ， 副教授 ， 主要 研究 方向 为 智能 控制 及 智能 机器人 作者 单位 ： 哈尔滨工程大学 计算机系 　 哈尔滨 　 参考文献 　 　 SebastianTMitchellTMLifelongrobotlearningRoboticsandAutonomousSystem ～ 　 　 阎 平凡 再励 学习 — — 原理 、 算法 及其 在 智能 控制 中 的 应用 信息 与 控制 ～ 　 　 （ YanPingfanReinforcementlearning — — principlealgorithmanditsapplicationinintelligentcontrolInformationandControlinChinese ～ ） 　 　 WatkinsJCHDayanPQlearningMachineLearning ， ～ 　 　 Szepesv á riCTheasymptoticconvergencerateofQlearningInProceedingsofNeuralInformationProcessingSystemsCambridgeMAMITPress ～ 　 　 WilliamsRJSimplestatisticalgradientfollowingalgorithmsforconnectionistMachineLearning ： ～ 　 　 BeomHBAsensorbasednavigationforamobilerobotusingfuzzylogicandreinforcementlearningIEEETransonSMC ～ 　 　 WinfriedIKarstenBAlearningarchitecturebasedonadaptivecontrolofthewalkingmachineLAURONRoboticsandAutonomousSystem ， ： ～ 原稿 收到 日期 ： ； 修改稿 收到 日期 ：