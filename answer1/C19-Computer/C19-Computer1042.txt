自动化 学报 ACTAAUTOMATICASINICA 年 第卷 第期 VolNo 一种 用于 函数 学习 的 小波 神经网络 吕柏权 　 李天铎 　 吕 崇德 　 刘兆辉 摘要 　 在 非线性 系统 辨识 中 ， 系统 输入 往往 是 多 变量 的 小波 处理 此类 问题 则 比较复杂 结合 神经网络 形式 和 小波 特点 建立 一种 新型 的 网络 ， 可 简单 有效 地 解决 网络 多 输入 问题 同时 给出 此 网络 可以 逼近 任意 连续函数 的 数学 证明 并 通过 实例 验证 了 此 方法 的 正确性 关键词 　 参数 辨识 ， 小 波 网络 ， BP 神经网络 WAVELETNEURALNETWORKFORFUNCTIONLEARNINGL ü BAIQUAN 　 LITIANDUOL ü CHONGDE 　 LIUZHAOHUIDepartmentofThermalEngineeringTsinghuaUniversityBeijingAbstractInidentificationofnonlinearsystemthenetworkinputsusuallyaremultivariablesTheapproachusingwaveletsalonetoresolvetheproblemiscomplexThereforethepaperestablishesanewnetworkmethodbycombiningneuralnetworkandwaveletcharacteristicstohandlethemultivariableproblemsimplyandeffectivelyThispaperalsogivesthemathematicalverificationthatthenetworkcanapproximateanykindoffunctionsThesimulationresultsofanexamkpleindicatethevalidityofthemethodKeyworksParametersidentificationwaveletnetworkBPneuralnetwork 　 引言 　 　 小 波 分析 是 近年来 迅速 发展 起来 的 新 学科 它 广泛 地 用于 信号 分析 、 故障诊断 等 方面 其 最大 特点 是 它 在 时域 － 频域 同时 具有 良好 的 局部 化 性质 另外 小波 还 能 把 LIR 空间 分成 闭子 空间 WJJ ∈ ZZ 的 直 和 形式 尺度 分析 目前 利用 此 特点 进行 函数 逼近 也 倍受 关注 ［ ］ 但 对 多维 情况 ， 小 波 处理 此 问题 比较复杂 在 实时 单 输入 － 单 输出 的 控制系统 参数 辨识 中 ， 往往 又 要求 网络 输入 为 多 变量 的 为此 本文 利用 神经网络 形式 ， 以小 波函数 做 激励函数 从而 简单 而 有效 地 解决 网络 多 输入 问题 ， 同时 给出 此 网络 可以 逼近 任意 连续函数 的 数学 证明 ， 并 通过 实例 验证 了 此 方法 的 正确性 　 小 波 变换 及 学习 算法 　 　 首先 考虑 一维 的 情况 ， 记 〈 fg 〉 ∫ Rfxgxdx ‖ f ‖ 〈 ff 〉 Ψ abx 其中 Ψ 为 母小波 ， 即 　 　 　 　 定义 函数 fx ∈ LIR 空间 的 连续 小 波 变换 为 Tab 其反 变换 为 当 ajbkbjjk ∈ zz 时 ， 反 变换 离散 化为 　 　 其中 Ψ jk 是 Ψ jk 的 共轭 基 ， Ψ jkxj Ψ jxkb 　 　 为了 能 再现 fx 还 必须 满足 如下 条件 ， 即 框架 Frame 　 且 AB ＜ ∞ 式 可 近似 为 ， 这 可以 用 三层 NN 网络 实现 当维数 为 d 时 判断 Ψ dx 满足 框架 条件 比较 困难 ［ ］ 同样 ， Ψ dx 为 小 波函数 时 ， 可以 用 Ψ dx Ψ d 表示 此 表示法 在 x 的 维数 比较 高时 计算 量 比较 大 本文 用来 近似 fx ， 其中 α TR 为 行向量 ， X 为列 向量 ， 现在 讨论 Ψ x 是 怎样 的 函数 时用 此 近似 fx 的 误差 在 允许 范围 内 　 　 定义 ［ ］ 设 函数 gx ∈ LPLOCR ， 若 线性组合 在 任意 LP ［ ab ］ 中 稠密 ， 则 称 g 是 一个 LP － Tauber － Wiener 函数 ， 简记 g ∈ LPTW 其中 Ci θ i 及 λ i ≠ 均 为 实数 ， i … N 若 g 为 一个 一元函数 连续 的 或 不 连续 的 且 在 任意 C ［ ab ］ 中 稠密 ， 则 称 g 是 一个 C － Tauber － Wiener 函数 简记 g ∈ CTW 　 　 引理 设 K 为 Rn 中任 一个 紧集 ， V 为 LPK 一个 紧集 ， g ∈ LPTWP ∞ 则 对 任意 ε ＞ 存在 不 依赖 f 的 一 正整数 N ， 实 常数 θ i 以及 向量 yi ∈ R ， 和 依赖于 f 的 常数 cifi … N 使 对 一切 f ∈ V 成立 ， 且 所有 cif 都 是 定义 在 V 上 的 连续 泛函 　 　 定理 设 K 为 Rn 中任 一个 紧集 ， V 为 LK 中 的 一个 紧集 ， 对于 框架 函数 Φ ∈ LIR ， 则 对于 任意 ε ＞ 存在 一个 正整数 N ， 实 常数 bi 向量 ai ∈ R ， 均 不 依赖于 f ∈ V 以及 依赖于 f ∈ V 的 常数 cif 使 对 一切 f ∈ V 成立 所有 cifi … ， N 都 是 定义 在 V 上 的 线性 连续 泛函为 φ 的 付里叶 变换 　 　 证由 引理 知道 ， 只要 x 在 任意 LP ［ ab ］ 中 稠密 其中 x ∈ R ， 则 对 任意 fx ∈ VV 为 LPK 一个 紧集 ， K 为 Rn 中任 一个 紧集 ， 都 可以 用 x 逼近 其中 向量 x ∈ R ， 由于 Ψ 是 框架 ， 则 保证 了 x 在 任意 LIR 中 稠密 其中 x ∈ R ， 也就是说 用 x 是 可以 以 任意 精度 逼近 fx 证毕 　 　 定理 设 K 为 rn 中 任一 紧集 ， V 为 CK 中 R 一个 紧集 ， 对于 框架 函数 Φ ∈ LIR 则 对于 任意 ε ＞ 存在 一个 正整数 N ， 实 常数 bi ， 向量 ai ∈ R ， 它们 均 不 依赖于 f ∈ V 以及 依赖于 f ∈ V 的 常数 cif 使 对 一切 f ∈ V 成立 所有 cif ， i … N 都 是 定义 在 V 上 的 线性 连续 泛函为 φ 的 付里叶 变换 　 仿真 结果 　 　 本文 以 y ″ siny ′ y ′ yyu 为例 进行 仿真 仿真 步长 为 hs 每倍 h 作为 一个 学习 点 ， 共点 ， 每点 作为 一个 学习 周期 重新 开始 学习 u ， 其权 初值 为 ， 的 均匀分布 ； x ｛ ynynun ｝ 为 网络 的 输入 ； y 为 网络 的 输出 ； 中间 结点 选 为 激励函数 分别 为 BP 网络 ， 小 波 网络 易 验证 小 波 Ψ x 满足 框架 条件 终止 条件 为 当 激励函数 为 Ψ x 且 学习 个 周期 时 E 才 小于 学习 速率 η 当 激励函数 为 Ψ x 且 学习 个 周期 时 ， E 已 小于 学习 速率 η 可见 效果 明显 其 仿真 结果 如图 和 图 所示 横轴 表示 采样 点 ， 换算 成 时间 秒 × 采样 数图 　 小 波 网络 逼近 结果 图 　 神经网络 逼近 结果 　 　 本文 给出 了 一种 小 波 网络 逼近 任意 函数 方法 的 证明 ， 从而 为用 这种 小 波 网络 逼近 函数 时 提供 理论 根据 并 通过 仿真 验证 此 方法 的 正确性 作者 单位 ： 清华大学 热能 工程系 　 北京 　 参考文献 　 DelyonBJuditskyABenveniseAAccuracyanalysisforwaveletApproximationsIEEETransactionsonNeuralNetworks － 　 JunZhangGilbertGWalterYuboMiaoetalWaveletNeuralNetworksforfunctionlearningIEEETransactionsonSignalProcessing 　 倪 先锋 ， 陈宗基 ， 周绥平 基于 神经网络 的 非线性 学习 控制 研究 自动化 学报 ， ， ： 　 陈天平 神经网络 及其 在 系统 识别 应用 中 的 逼近 问题 中国 科学 学报 A 辑 ， ， ： 　 秦 前清 ， 杨宗凯 实用 小 波 分析 西安 ： 西安电子科技大学 出版社 ， 收稿 日期 　