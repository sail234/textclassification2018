计算机 研究 与 发展 JOURNALOFCOMPUTERRESEARCHANDDEVELOPMENT 年 　 第卷 　 第期 　 Vol 　 No 　 基于 神经网络 结构 学习 的 知识 求精 方法 刘振凯 　 贵忠华 蔡青 　 　 摘 　 要 　 知识 求精 是 知识 获取 中 必不可少 的 步骤 已有 的 用于 知识 求精 的 KBANNknowledgebasedartificialneuralnetwork 方法 ， 主要 局限性 是 训练 时 不能 改变 网络 的 拓扑 结构 文中 提出 了 一种 基于 神经网络 结构 学习 的 知识 求精 方法 ， 首先 将 一组 规则 集 转化 为 初始 神经网络 ， 然后 用 训练样本 和 结构 学习 算法 训练 初始 神经网络 ， 并 提取 求精 的 规则 知识 网络拓扑 结构 的 改变 是 通过 训练 时 采用 基于 动态 增加 隐含 节点 和 网络 删除 的 结构 学习 算法 实现 的 大量 实例 表明 该 方法 是 有效 的 　 　 关键词 　 知识 求精 ， 人工神经网络 ， 结构 学习 ， 规则 提取 　 　 中图法 分类号 　 TPAKNOWLEDGEBASEREFINEMENTMETHODBASEDONSTRUCTURALLEARNINGOFNEURALNETWORKSLIUZhenKaiGUIZhongHuaandCAIQingSchoolofMechanicalEngineeringXianJiaotongUniversityXianNorthwesternUniversityofTechnologyXian 　 　 Abstract 　 KnowledgebaserefinementisnecessaryforknowledgeacquisitioninbuildingexpertsystemsTheKBANNknowledgebasedartificialneuralnetworkforknowledgebaserefinementhasbeenproposedinliteraturesThekeylimitationofKBANNisthatthereisnomechanismforchangingthetopologyofthenetworkInthispaperanovelapproachtoknowledgebaserefinementbaseduponstructurallearningofneuralnetworksispresentedInthisapproachasetofrulesaremappedintoaneuralnetworkietheinitialneuralnetworkandthenthisreformulatedknowledgeisrefinedusingstructurallearningietheinitialneuralnetworkistrainedbystructurallearningalgorithmandasetoftrainingexamplesFinallytherefinedrulesareextractedfromthetrainedneuralnetworkToaccomplishthetopologychangeoftheinitialneuralnetworkstructurallearningalgorithmsbasedondynamicnodecreationandnetworkpruningareusedinthisapproachManysimulationexperimentshaveprovedtheeffectofthisapproach 　 　 Keywords 　 knowledgebaserefinementartificialneuralnetworksstructurallearningruleextraction 　 引 　 言 　 　 知识 获取 是 研制 专家系统 的 瓶颈 问题 对 知识 获取 的 主要 步骤 及其 一般 模式 ， 学者 们 提出 了 种种 见解 ， 现将 其中 比较 著名 的 种 介绍 如下 ： 　 　 Buchanan 等 人 在 构造 专家系统 一文 ［ ］ 中 提出 了 一个 知识 获取 模式 ， 其 主要 步骤 是 ： ① 识别 阶段 ； ② 概念化 阶段 ； ③ 形式化 阶段 ； ④ 实现 阶段 ； ⑤ 测试阶段 ； ⑥ 初始 知识 模型 的 修改 阶段 　 　 Kidd 等 人 根据 心理学 的 理论 提出 了 一个 知识 获取 模式 ［ ］ ， 分为 个 阶段 ： ① 领域 知识 基本 结构 的 识别 ； ② 细节 知识 的 抽取 ； ③ 知识库 的 调试 和 求 精 　 　 Ginsberg 等 人 把 知识 获取 分为 两个 阶段 ［ ］ ： ① 初始 知识库 抽取 阶段 ； ② 初始 知识库 求精 阶段 　 　 综上所述 ， 知识 求精 是 知识 获取 必不可少 的 步骤 一般来说 ， 得到 的 初始 知识库 常常 有些 问题 ， 比如 知识 不 完全 、 知识 之间 不 一致 、 有 的 知识 不 正确 等 ， 因此 需要 调试 、 修改 和 补充 实践证明 ， 初始 知识库 求 精后 可以 显著 地 提高 专家系统 的 运行 性能 ， 比如 利用 知识 求精 系统 SEEK 对 诊断 风湿病 专家系统 EXPERT 的 知识库 求 精后 ， 其 诊断 正确率 提高 了 ［ ］ 因此 知识 求精 受到 专家系统 研制者 的 极大 重视 　 　 近年来 ， 人们 将 人工神经网络 用于 知识 求精 ， 取得 了 一些 进展 年 ， Towell 和 Shavlik 提出 了 用于 知识 求精 的 基于 知识 的 人工神经网络 knowledgebasedartificialneuralnetwork ， 简称 KBANN ［ ］ ， 并 通过 实例 证明 基于 KBANN 的 知识 求精 方法 比纯 符号 求精 系统 要 好 但是 KBANN 也 具有 一些 缺点 ［ ， ］ ， 主要 是 KBANN 在 学习 时 不能 改变 网络 的 拓扑 结构 ， 因而 不能 向 不 完全 的 规则 集 增加 新 的 规则 年 Optiz 和 Shavlik 提出 了 一个 KBANN 改进 学习 算法 ［ ］ ， 在 训练 期间 利用 符号 机制 指导 、 解释 和 说明 网络 中 如何 动态 增加 新 节点 ， 但 究竟 在 何处 增加 新 节点 仍 是 一个 问题 ， 需要 进一步 研究 同年 ， Fu 提出 了 与 KBANN 类似 的 KBCNNknowledgebasedconceptualneuralnetwork 用于 知识 求精 ［ ］ ， 他 指出 当 网络 训练 不 收敛 时 ， 可 人工 在 指定 的 隐含 层上 增加 新 节点 　 　 针对 上述 问题 ， 我们 提出 了 一种 基于 神经网络 结构 学习 的 知识 求精 方法 　 基于 神经网络 结构 学习 的 知识 求精 方法 　 知识 求精 问题 描述 　 　 本文 所指 的 知识 求精 问题 描述 如下 ： 　 　 已知 ： 初始 知识库 指 规则 集 ； 　 　 专家 例证 　 　 求解 ： 用 已知 例证 检查 初始 知识库 ， 并 对 它 进行 修改 、 删除 和 补充 ， 使 加工 后 的 知识库 达到 预期 的 运行 性能 　 　 　 基于 神经网络 结构 学习 的 知识 求精 方法 流程 　 　 与 KBANN 类似 ， 基于 神经网络 结构 学习 的 知识 求精 方法 的 流程图 如图所示 ， 图中 的 初始 规则 集即 初始 知识库 、 训练样本 即 专家 例证 它 由个 步骤 组成 ： 　 　 转化 ： 初始 规则 集 转化 为 初始 神经网络 ； 　 　 求精 ： 用 训练样本 和 学习 算法 训练 初始 神经网络 ； 　 　 提取 求精 的 规则 知识 在 训练 网络 时 ， 不仅 考虑 了 权值 和 阈值 的 学习 对应 于 规则 的 修改 ， 而且 考虑 了 动态 增加 隐含 节点 对应 于 规则 的 补充 和 网络 的 删除 对应 于 规则 的 删除 图 　 基于 神经网络 结构 学习 的 知识 求精 方法 流程图 　 初始 规则 集 转化 为 神经网络 　 　 初始 规则 集 转化 为 神经网络 ， 对应 关系 为 ① 最终 结论 ： 输出 节点 ； ② 支持 的 事实 ： 输入 节点 ； ③ 中间 结论 ： 隐含 节点 ； ④ 依赖 关系 ： 联接 权值 和 阈值 　 　 利用 上述 关系 ， 可以 将待求 精 的 规则 知识 转化 为 初始 神经网络 但是 ， 这些 规则 必须 满足 这样 两个 条件 ： 无 谓词 运算 变量 ； 不 构成 回路 　 　 构造 初始 神经网络 的 方法 有 很多 ， 不同 的 方法 产生 不同 拓扑 结构 、 不同 规模 的 初始 网络 本文 采用 了 一个 与 Towell 等 在 KBANN 中 使用 的 “ 规则 转化 为 网络 算法 ” 相似 的 算法 ， 简介 如下 　 　 第步 规则 重写 使 每个 析取 式 都 表示 为 仅 有 一个 前提 的 规则 如果 有 多条 规则 且 具有 相同 的 结论 ， 则 将 这些 规则 分别 重写 为 两条 规则 ： 其中 一条 以 原来 的 结论 为 结论 ， 以 一个 新建 项为 前提 ； 另 一条 规则 以此 新建 项为 结论 ， 以原 规则 的 前提 为 前提 这样 做 的 目的 是 便于 所 对应 的 网络 分层 图 给出 了 一个 规则 重写 的 实例 ， 设 知识库 中 的 一条 规则 为 B ∧ C ∨ D ∧ E ∧ F → A ， 则 将 此 规则 改写 为 两条 规则 ， 即 B ∧ C → A 和 D ∧ E ∧ F → A ， 然后 将 它们 分别 重写 为 A ′ → AB ∧ C → A ″ 和 A ″ → AD ∧ E ∧ F → A ″ 图 　 规则 重写 实例 　 　 第步 将 规则 集 转化 为 神经网络 用 上述 规则 与 网络 的 映射 关系 ， 将 包含 逻辑关系 AND ， OR 和 NOT 的 规则 转化 到 神经网络 中 ， 建立 一个 与 规则 集 元素 一一对应 的 初始 神经网络 这些 规则 均 为 合取 式 （ 析取 式 经过 第一步 已 被 改写 为 多个 合取 式 ） ， 且 无 回路 、 无变 元规则 前提 与 结论 的 依赖 关系 转化 为 网络 中 对应 的 权值 联接 　 　 第步 增加 中间 节点 为了 使 网络 中 各 节点 处于 正确 的 层次 ， 可能 需要 适当 增加 一些 新 节点 　 　 第步 增加 联接 在 原来 没有 关系 的 上 下层 节点 之间 增加 权值 为 零 的 联接 ， 使 网络 中 的 任何 一个 中间 节点 和 终 节点 与其 每 一个 下层 节点 间 均 存在 联接 　 　 第步 对 权值 和 节点 扰动 ， 即 对 网络 中 的 所有权 值 和 阈值 加上 一个 很小 的 随机数 　 　 按照 上述 步骤 ， 最终 得到 的 是 一个 全 互联 、 多层 神经网络 　 训练 初始 神经网络 　 　 KBANN 和 KBCNN 都 采用 BP 学习 算法 ， 而 BP 算法 只能 对 联接 权值 进行 学习 ， 不能 改变 网络 的 拓扑 结构 但 在 知识 求 精时 ， 由于 初始 规则 集 可能 不 完善 或 含有 一些 错误 的 规则 ， 这样 导致 初始 神经网络 缺少 节点 或 包含 一些 错误 的 联接 因此 必须 使 网络 的 拓扑 结构 进行 合理 变化 ， 包括 增加 节点 、 联接 以及 删除 节点 、 联接 等 只有 通过 结构 学习 才能 实现 网络拓扑 结构 的 改变 　 　 我们 提出 的 结构 学习 算法 包括 动态 增加 隐含 节点 和 网络 删除 两步 　 　 动态 增加 隐含 节点 　 　 从 前面 可知 ， 初始 神经网络 的 拓扑 结构 是 根据 初始 规则 集 构造 的 全 互联 、 多层 网络 一般来说 ， 初始 规则 集 并 不 完善 ， 由此 构造 的 网络 表现 为 缺少 隐含 节点 ， 因此 经过 BP 算法 训练 不 一定 能 收敛 为 解决 这个 问题 ， 我们 提出 在 训练 过程 中 动态 地 增加 隐含 节点 具体地说 ， 就是 用 BP 算法 训练 网络 直到 到达 一 学习 平台 ， 即 网络 的 训练 误差 不再 随 时间 而 减小 ， 此时 在 网络 的 每 一 隐含 层 加入 一定 数量 的 节点 例如 ， 当前 隐含 层 节点 数 的 ， 或 至少 一个 ， 新 加入 的 隐含 节点 与 相邻 层 节点 采用 全 互联 并 具有 较 小 的 随机 权值 ， 因而 其 重要性 较 低 ， 当 网络 继续 学习 时 ， 新 的 隐含 节点 的 重要性 增加 并 开始 影响 网络 输出 上述 过程 重复 进行 直到 网络 达到 规定 允差 　 　 当然 ， 这样 动态 增加 隐含 节点 有 可能 使 网络 增加 了 多余 的 节点 或 连接 ， 所以 有 必要 在 网络 收敛 后 ， 删除 多余 的 节点 和 联接 ， 这样 做 同样 也 可 删除 初始 规则 集中 的 错误 规则 　 　 网络 删除 　 　 目前 已有 许多 神经网络 结构 学习 的 删除 算法 ［ ］ ， 有 的 算法 只 删除 多余 联接 ， 有 的 算法 只 删除 多余 节点 ， 还有 的 算法 则 同时 删除 多余 节点 和 联接 我们 采用 Ishikawa 提出 的 遗忘 结构 学习 算法 ［ ］ structurallearningwithforgetting ， 简称 SLF ， 它 既 删除 多余 节点 也 删除 多余 联接 ， 共 由 部分 组成 ， 即 遗忘 学习 LFlearningwithforgetting 、 隐含 节点 澄清 学习 LHUClearningwithhiddenunitsclarification 和 选择 遗忘 学习 LSFlearningwithselectiveforgettingSLF 的 基本 思想 是 在 神经网络 的 均 方差 MSE 上 增加 惩罚 项 ， 以 实现 LFLHUC 和 LSF 该 算法 由 以下 步 组成 ： 　 　 ① 用 LF 得到 一个 骨架 网络结构 ； 　 　 ② 用 LHUC 使 隐含 节点 的 输出 为 全 激活 接近 或全 抑制 接近 ； 　 　 ③ 同时 用 LSF 和 LHUC 得到 更好 的 学习效果 　 　 SLF 算法 不仅 删除 了 多余 节点 、 多余 联接 ， 而且 使 隐含 节点 的 输出 接近 或 ， 这样 为 下 一步 提取 规则 打好 了 基础 因为 如果 隐含 节点 的 输出 不是 接近 或 ， 这样 对于 提取 规则 是 很 困难 的 ， 故 一些 从 神经网络 提取 规则 的 算法 都 作 了 相应 的 处理 ［ ］ 　 提取 规则 　 　 通过 上述 算法 训练 好 的 神经网络 实际上 是 求 精 的 神经网络 ， 其 知识 分布 表示 于 网络结构 和 权值 中 ， 是 一种 隐式 表达 ， 不 易于 理解 只有 通过 提取 规则 ， 才能 使 之 成为 人们 易于 理解 和 接受 的 形式 　 　 Towell 和 Shavlik 提出 了 一种 从 KBANN 中 提取 求精 规则 的 方法 ［ ］ ， 即 NofM 方法 这种 方法 提取 的 规则 形式 如下 ： 　 　 IfNofthefollowingMantecedentsaretruethen … 　 　 这种 规则 实际上 是 神经网络 隐式 知识 的 另 一种 表示 形式 ， 不能 直接 用于 构造 专家系统 的 知识库 我们 采用 Fu 提出 的 KT 方法 ［ ］ ， 该 方法 能 提取 易于 理解 的 规则 知识 ， 其 算法 简介 如下 ： 　 　 对于 每个 节点 Y ， 将 W ＋ 和 W － 分别 定义 为 其 所有 正 输入 权值 的 集合 和 所有 负 输入 权值 的 集合 　 　 若 其 某 部分 正 输入 权值 之 和 加上 除 某 部分 负 输入 权值 以外 的 任何 权值 之 和 均 大于 其 阈值 ， 则 产生 如下 规则 　 　 若 其 某 部分 负 输入 权值 之 和 加上 除 某 部分 正 输入 权值 以外 的 任何 权值 之 和 均 小于 其 阈值 ， 则 产生 如下 规则 　 　 其中 ， ， 分别 是 通过 权值 ， 与 Y 节点 相联 的 Y 节点 前 一层 的 节点 　 仿真 实验 　 　 利用 本文 提出 的 知识 求精 方法 ， 我们 进行 了 许多 仿真 实验 下面 给出 一个 实例 ， 其 初始 规则 集 为 　 　 　 　 　 　 　 RB ∧ Z → A 　 　 　 　 RE ∧ F ∧ G → B 　 　 　 　 RC ∧ ～ D → B 　 　 　 　 　 　 　 RY ∧ X → Z 　 　 　 　 RH ∧ I → Y 　 　 　 　 　 RG ∧ J → X 　 　 经过 规则 重写 得到 的 网络 如图所示 ， 再 通过 增加 联接 、 扰动 等 步骤 最终 得到 一个 全 互联 、 多层 网络 网络 共有 层 ， 输入 层 有 个 节点 ， 代表 C ， D ， E ， F ， G ， H ， I 和 J ； 两个 隐含 层 分别 有个 和 个 节点 ， 代表 中间 概念 B ， B ， Y 和 X ； 输出 层 只有 一个 节点 代表 最终 结论 A 图 　 规则 转化 为 神经网络 　 　 训练样本 共有 个 ， 初始 规则 集对 其中 个 得出 错误 结论 用 BP 算法 训练 网络 至 一 平台 ， 这时 分别 在 个 隐含 层 各 增加 个 节点 ， 然后 训练 收敛 再用 SLF 算法 训练 网络 ， 删除 了 在 第 隐含 层 增加 的 多余 节点 并 删除 了 全部 多余 连接 ， 最终 得到 的 网络结构 如图所示 最后 用 KT 算法 提取 的 求 精后 的 规则 知识 如下 ： 图 　 求 精后 的 神经网络 　 　 　 　 　 　 　 　 　 RB ∧ Z → A 　 　 　 　 RX ∧ Y → Z 　 　 　 　 RE ∧ F → B 　 　 　 　 　 　 　 　 　 RC ∧ ～ D → B 　 　 　 R ～ C ∧ D → B 　 　 　 RG ∧ H → Y 　 　 　 　 　 　 　 　 　 RI ∧ ～ J → X 　 　 这些 规则 对 全部 个 样本 均 得出 正确 结论 可以 看出 该 方法 不仅 修改 了 一些 错误 的 规则 ， 而且 还 补充 了 一些 遗漏 的 规则 　 结 　 论 　 　 本文 针对 KBANN 的 局限性 提出 了 一种 基于 神经网络 结构 学习 的 知识 求精 方法 该 方法 与 KBANN 的 主要 区别 是 ： ① 训练 初始 神经网络 采用 了 结构 学习 算法 ， 即 动态 增加 隐含 节点 和 网络 删除 ； ② 采用 KT 算法 提取 求精 的 规则 知识 ， 其 形式 易于 理解 和 接受 ， 并 可 直接 用于 构造 专家系统 的 知识库 将 这种 方法 应用 于 工程 实际 问题 是 作者 进一步 的 研究 工作 本 课题 得到 国家自然科学基金 资助 项目编号 作者简介 ： 刘振凯 ， 男 ， 年月生 ， 博士后 ， 研究 方向 为 人工神经网络 、 专家系统 、 智能 CAD 和 反求 工程 　 　 　 　 　 贵忠华 ， 女 ， 年月生 ， 博士 研究生 ， 研究 方向 为 智能 制造 、 FMS 和 CIMS 　 　 　 　 　 蔡青 ， 男 ， 年月生 ， 教授 ， 博士生 导师 ， 研究 方向 为 CADCAM 、 智能 CAD 、 CAGD 和 可视化 作者 单位 ： 西安交通大学 机械 工程学院 　 西安 　 　 　 　 　 　 西北工业大学 十系 　 西安 　 参考文献 　 　 　 BuchananBGetalConstructinganexpertsystemInHayesRothFWatermanDALenatDBedsBuildingExpertSystemsChaptReadingMAAddisonWesley ～ 　 　 　 KiddAWelbankSMKnowledgeacquisitioninexpertsystemsBerkshirePergamonInfortechLtdStateofArtRept ～ 　 　 　 GinsbergAWeissSMPolitakisPAutomaticknowledgebaserefinementforclassificationsystemArtificialIntelligence ～ 　 　 　 TowellGGShavlikJWNoordewierMORefinementofapproximatedomaintheoriesbyknowledgebasedneuralnetworksInProcofAAAIMenloParkAAAIPress ～ 　 　 　 TowellGGShavlikJWKnowledgebasedartificialneuralnetworksArtificialIntelligence ～ 　 　 　 OptizDWShavlikJWHeuristicallyexpandingknowledgebasedneuralnetworkInProcofIJCAISanMateo ～ 　 　 　 FuLKnowledgebasedconnectionismforrevisingdomaintheoriesIEEETransonSystemsManCybernetics ～ 　 　 　 ReedRPruningalgorithm — — AsurveyIEEETransonNeuralNetworks ～ 　 　 　 IshikawaMStructurallearningwithforgettingNeuralNetworks ～ 　 　 　 YoonBLacherRCExtractingrulesbydestructivelearningInProcIEEEICNNNewYorkIEEEPress ～ 　 　 　 TowellGGShavlikJMExtractingrefinedrulesfromknowledgebasedneuralnetworksMach 　 Learn ～ 　 　 　 FuLRulegenerationfromneuralnetworksIEEETransonSystemsManCybernetics ～ 原稿 收到 日期 ： ； 修改稿 收到 日期 ：